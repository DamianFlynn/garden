{
  "garden/content/az-billing-ea_privilege_delegation": {
    "title": "Azure EA - Delegating Privileges",
    "content": "\n\nUnder the Enterprise agreement we have some different Persona's, which have quite different abilities and operations upon which they are permitted to preform. \n\n## Roles and Accounts\n\nBefore we being the process of delegation, It is important to understand this Hierarchy, so we can correctly proceed with the technical work ahead.\n\u003c!--more--\u003e\n### Enterprise Administrator\n\nHas the ability to add additional Enterprise and Department Administrators, Additional this persona can\n* Add Departments \n* Add or Associate Accounts to and Enrollment\n* Can view usage and charges across ALL Accounts and Subscriptions\n* Can view the monetary commitment balance associated to the Enrollment\n\nThere is no limit on the number of Enterprise Administrators that can be associated with an Enrollment, and additionally a notification contact can be assigned to receive all email notifications issued.\n\n### Department Administrator \n\nThe Department Administrator has ability to do the following:\n* Create Department Administrator (Department focus – click on add administrator)\n* View/Edit Department properties such as name or Cost Center (Department focus – click on edit pen icon)\n* Create a new Account Owner on the Department they administer (Switch to Account focus – click on add account)\n* Remove the associated Accounts from the Department they administer (In Account focus – hover over account and then select the x icon to delete)\n* Download usage details of the Department they administer (Switch to Reports panel on left – Select Download Usage focus)\n* View the monthly Usage and Charges associated to their Department if Enterprise Administrator has granted permission to do so. (Switch to Reports panel on left –Select Usage Summary focus)\n* Enable the Account owners to create Non Production Subscriptions \n  * Subscription Offer MS-AZR-0148P for Dev/Test\n  * Subscription Offer MS-AZR-0017P for Production\n\n### Account Owners\n\nThe Account Owner can add Subscriptions to their Accounts. Additionally they have the ability to\n\n* Update the Service Administrator for a Subscription\n* View Usage Data for their Account\n* If enabled by the Enterprise Administrator, can also view Account Charges\n* Enumerate existing subscriptions in their account\n* Create new subscriptions within the scope of their account\n\nThe Account Owner also has the privilege's of delegating their responsibility (Role) to a Service Principal; which is the core of the process we are going to undertaken in this post.\n\n\u003e As of March 2020; this is no longer relevant.\n\u003e ~~This account is REQUIRED to have established at least on subscription manually.~~\n\nIn the [EA Portal](https://ea.azure.com), the _Account_ page for a Department will present a list of established subscription.\n\n![EP Portal](az-billing-ea_privilege_delegation/azure-billing-ea_privilege_delegation-EAPortalAccountsView.png)\n\n\n## Programmatic Subscription Creation\n\nThe objective of the is process is to delegate the EA Account Owner privilege's to a Service Principal, which can be leveraged to programmatically establish subscriptions. While the procedure is not overly complex; we will break the operation in 4 key milestones, which should make this easier to understand.\n\n1. EA Department Account Owner \n2. Service Principal for Delegation\n3. Delegating EA Account Owner Privilege's\n4. Validation \n\n### EA Department Account Owner\n\nUsing the Azure AD account credentials for your Microsoft Tenant, you should be able to authenticate with the [EA Portal](https://ea.azure.com), and view the _Account_ page, as illustrated earlier.\n\nOnce you have confirmed that this works as expected, we will repeat this authentication again, however this time, using either the Azure Powershell Module or the Azure CLI; The guide will present the commands used in either scenario.\n\n#### Authenticate to Azure the EA Account Owner Credentials\n\nWith working credentials validated in the EA Web portal, proceed with a new shell session and authenticate to Azure with your **EA Account Owner** credentials; you may also take advantage of the Azure Cloud Shell which will automate the login experience.\n\n\u003e The credentials **MUST** be hosted in the tenants AAD instance.\n\u003e Microsoft Accounts, or Guest accounts from other AAD Tenants are NOT Supported for the following procedures.\n\n\n```Powershell\nPS\u003e $aadTenantId =      \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\" \nPS\u003e $azSubscriptionId = \"42d96448-0b0a-4b33-9d9a-5653a3f11811\" \nPS\u003e Connect-AzAccount -TenantId $aadTenantId -Subscription $azSubscriptionId \n```\n\n```bash\n$\u003e az login\n```\n\n#### Check the EA Accounts which we have access to\n\nOnce you have authenticated, we will verify that the credentials do indeed have access to the EA environment, but getting a list of the EA Accounts which can be accessed\n\n```Powershell\nPS\u003e $eaAccountList = Get-AzEnrollmentAccount\nPS\u003e $eaAccountList\n\nObjectId                              PrincipalName\n--------                              -------------\n88888888-8888-8888-8888-888888888888  eaaccountowner@org.onmicrosoft.com\n\nPS\u003e $EAAccountId = $eaAccountList.ObjectId\n```\n\n```bash\n$\u003e az billing enrollment-account list\n\n[\n  {\n    \"id\": \"/providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888\",\n    \"name\": \"88888888-8888-8888-8888-888888888888\",\n    \"principalName\": \"eaaccountowner@org.onmicrosoft.com\",\n    \"type\": \"Microsoft.Billing/enrollmentAccounts\"\n  }\n]\n```\n\nAssuming the results of these commands, presents back your account, and a billing enrollment object we are ready to proceed. If this is not the case, you must check that you are using the correct account.\n\nThe important data you have as we complete this stage is\n\n|Variable           | Value|\n|---|---|\n|Azure AD Tenant Id | aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa|\n|EA Account ID      | 88888888-8888-8888-8888-888888888888|\n\n\n### Service Principal for Delegation\n\nOur objective is to delegate the role of creating new Azure Subscriptions to a Service Principal. Before we begin that effort; we first need to know the ObjectID of the (Service Principal) SPN which we are delegating to.\n\n#### Create a new principal\n\nEstablish a new AzureAD Application and a Service Principal in the AD Tenant. If you have an existing application which you would rather utilize; we can skip this step and reference that object in the next stage\n\n```powershell\n$password = \"S3cR3tP@ssw0rd!\"\n$credentials = New-Object Microsoft.Azure.Commands.ActiveDirectory.PSADPasswordCredential -Property @{StartDate=Get-Date; EndDate=Get-Date -Year 2024; Password=$password}\n$spName = \"AZ GOV Subscription Provision\"\n$sp = New-AzAdServicePrincipal -DisplayName $spName -PasswordCredential $credentials\n```\n\n```bash\naz ad sp create-for-rbac --name \"azGOVSubscriptionProvision\"\n{\n  \"appId\": \"eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\",\n  \"displayName\": \"azGOVSubscriptionProvision\",\n  \"name\": \"http://azGOVSubscriptionProvision\",\n  \"password\": \"8ff3e80e-a518-4dfa-ad5f-235bfb1d8bd9\",\n  \"tenant\": \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\"\n}\n```\n\nAssuming no errors with this command, you should be able to locate the new principal in the Azure AD portal, under Enterprise Applications.\n\n![AAD SPN](az-billing-ea_privilege_delegation/azure-billing-ea_privilege_delegation-AADServicePrincipal.png)\n\n#### Get the Application ID \n\nIf you plan on using a newly created Service Principal for the delegation, we can quickly reference it, and store it to a variable\n\n```Powershell\nPS\u003e $appId = $sp.Id\nPS\u003e $appId\neeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\n```\n\nAlternative, we can reference an existing application, and get its Application Id, in this case we simply provide the display name of the service principal we are going to reference\n\n```bash\n$\u003e az ad sp list --display-name azGOVSubscriptionProvision --query \"[].objectId\" -o tsv\neeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\n```\n\n### Delegating EA Account Owner Privilege's\n\nFinally, we are at the heart of the show. We will delegate the Account Owner role to the chosen Service Principal.\n\n#### Add the service principal to your enrollment account \n\nThe following command will allocate the **Owner** role to the EA Account Id which we discovered earlier, to our chosen Service Principal\n\n```Powershell\nPS\u003e New-AzRoleAssignment -ObjectId $appId -RoleDefinitionName \"Owner\" -Scope \"/providers/Microsoft.Billing/enrollmentAccounts/$eaAccountId\"  \n\nRoleAssignmentId   : /providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888/providers/Microsoft.Authorization/roleAssignments/aa516c46-1954-4bac-bddf-618f1f03acf6\nScope              : /providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888\nDisplayName        : AZ GOV Subscription Provision\nSignInName         : \nRoleDefinitionName : Owner\nRoleDefinitionId   : 8e3af657-a8ff-443c-a75c-2fe8c4bcb635\nObjectId           : eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\nObjectType         : ServicePrincipal\nCanDelegate        : False                           \n```\n\n```bash\n$\u003e az role assignment create --role Owner --assignee-object-id eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee --scope /providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888\n\n{\n   \"canDelegate\": null,\n   \"id\": \"/providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888/providers/Microsoft.Authorization/roleAssignments/11341c1d-7785-46a9-a415-147b5c5aec5c\",\n   \"name\": \"11341c1d-7785-46a9-a415-147b5c5aec5c\",\n   \"principalId\": \"eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\",\n   \"roleDefinitionId\": \"/providers/Microsoft.Authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635\",\n   \"scope\": \"/providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888\",\n   \"type\": \"Microsoft.Authorization/roleAssignments\"\n}\n```\n\n#### Azure AD Directory Reader\n\nFinally, To ensure the ability of assigning Onwers to the new subscriptions, we MUST also ensure that the Service Principal has the minimum privilege's of  Azure AD 'Directory Reader' role\n\n```powershell\nPS\u003e Connect-AzureAD -TenantId aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\n\nAccount         Environment TenantId                             TenantDomain             AccountType\n-------         ----------- --------                             ------------             -----------\nuser@my.org     AzureCloud  aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa org.onmicrosoft.com      User\n\nPS\u003e Add-AzureADDirectoryRoleMember -ObjectId (Get-AzureADDirectoryRole | where-object {$_.DisplayName -eq \"Directory Readers\"}).Objectid -RefObjectId $appId\n```\n\nFantastic - now we need to wait for the clock work engine in the back of the cloud to wake up the Noddy. Go have a Coffee, and then we can check the results of our efforts\n\n\u003e These delegations must be allowed to propagate, I have observed this taking up to 1 Hour!\n\n## Validation\n\nThe final stage of this effort is to validate that everything is working as desired. After waiting for delegations to propagate, we will authenticate to Azure as our Service Principal, and then attempt to communicate with the services we have granted access to.  Ready?\n\n### Login as your new Service Principal\n\nWith a new shell (or an existing on if you wish); we will proceed to login to Azure using the Service Principal Credentials.\nI assume the service principal has no Azure subscriptions delegated to it; so we will indicate that also at login time, to prevent warning.\n\n```bash\n$\u003e az login --service-principal -u eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee --password Easter2019 --tenant aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa --allow-no-subscriptions\n\n[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"id\": \"369b7bfe-5c66-41ef-837a-e2df4e9db6dd\",\n    \"isDefault\": true,\n    \"name\": \"Management\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\n    \"user\": {\n      \"name\": \"eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee\",\n      \"type\": \"servicePrincipal\"\n    }\n  }\n]\n```\n\n### Check you have Directory Access\n\nFirst test, lets see if we can read some object from the Azure AD\n\n```bash\n$\u003e az ad user list\n```\n\nAssuming no issues, you should see a list of users scroll by, feel free to terminate that process when you are satisfied that this is working to plan\n\n### Check we have access to the Enterprise Agreement Account\n\nNow, the real meat; Do we have access to the Enterprise Agreement Account?\n\n```bash\n$\u003e az billing enrollment-account list\n[\n  {\n    \"id\": \"/providers/Microsoft.Billing/enrollmentAccounts/88888888-8888-8888-8888-888888888888\",\n    \"name\": \"88888888-8888-8888-8888-888888888888\",\n    \"principalName\": \"eaaccountowner@org.onmicrosoft.com\",\n    \"type\": \"Microsoft.Billing/enrollmentAccounts\"\n  }\n]\n```\n\nAssuming you get a response, take a close look at the *principalName* and you will be presented with the name of the account who have delegated their privilege's to this active service account to work as.",
    "lastmodified": "2022-12-28T23:25:23.441768212Z",
    "tags": null
  },
  "garden/content/az-private_link-overview": {
    "title": "Ignite 2019 - Private Link, Delivering Services Privately",
    "content": "\n\n[MS Ignite 2019](conf-microsoft_ignite-2019) Session: #BRK3168\nPresenters: Narayan Annamalai and Sumeet Mittal\n\n\u003e [!note] Generally available\n\u003e This service has exited [Public Preview](Public Preview), and is now published to all regions as General Availability\n\n~~Additionally today (November 2019) in US West Central, US North, and US East the ability to use with *CosmosDB* has been added to this service~~\n\n## Your Service\n\nScenario, Number of VMs, linked to a Standard Load balancer. With a single click the Front End IP of the SLB, will be implemented as a Private Link, replacing the public IP with its new Private Address\n\n* Create or Convert your existing services in Private Link Services\n* VNET-VNET Connectivity\n* \nTo Connect to this service, we simply create a **Private Endpoint** in the VNET linked to the Private Link Endpoint.\n\n1. Create the Service\n1. Convert to Private Link Service on SLB Frontend IP\n1. Share the Private Link Service ID Alias (ARM Resource ID) to the customers \n1. Create a Private Endpoint in any subnet by specify the private link service URI/Alias\n1. Configure the DNS record to the price ip address\n1. Act on the request o accept or reject the connection\n1. Once approve the Private link is established\n\n\n### Alias\n\nTo hide the hosting subscription id, and resource group id of the service being published, for security mapping, an Alias can be established which will mask the original ID - The name is established using a GUID, and some customer concatenated strings to identify the resource\n\n### Visibility\n\nTo ensure the exposure of the service is limited, the Endpoint can be protected, even if the link name, or alias is determined; using an approval process:\n\n* RBAC\n* Subscription\n* open to anyone with the link\n\n### Auto-Approval\n\nTo support a large scale scenario for approvals, an automation can be used to set the audience which will be auto-approved.\n\n### NAT-IP\n\nThe service provider also has the ability to Allocate a Private IP, which is translated to the Source IP. Logging is linked to the IP Allocated by the Service Provide, which is presented as the SourceIP for inbound packets\n\n### TCP Proxy v2\n\nServer Side Settings, including headers for Source IP of customer and Link ID of private Endpoint. Currently been testing in NGNIX to implement a new custom header for Private Link.",
    "lastmodified": "2022-12-28T23:25:23.437767991Z",
    "tags": null
  },
  "garden/content/az.billing.subscription": {
    "title": "Creating Azure Subscriptions with Terraform",
    "content": "\n\nA little time back, I posted an article on working with the Azure [Enterprise Billing Services, for Privilage Delegation](az-billing-ea_privilege_delegation). That article looked at the organisation structure of the Enterprise Agreement portal and how to establish an **account** which could be used for creating new subscriptions, associating them with the correct billing context.\n\nSince then, the billing architecture has evolved, with more scenarios presented, and more features enabled, including different billing relationships, and new programatic options for establishing subscriptions.\n\n\nIf you’ve ever tried to create Azure Subscriptions with [Terraform](Terraform), [azCLI](azCLI) or via ARM api you’ll know that it was not possible up until recently. Previously you had to open a different UI to choose a billing model and create a new Subscription (even if it was bound to the same AD Tenant as your existing Subscription). Terrform and AZ work in the context of a Subscription so it was assumed you would have one before you started, however sometimes we need to create these automatically. It looks like Microsoft are addressing this issue and converging their services and have introduced an API (preview) for programmatically creating subscriptions on the fly. \n\n## Add Subscription Extension\n\nBefore we begin, let's be sure we have the **Subscriptions** extension added to our AZ CLI instance, by running the following command in your active context:\n\n```\naz extension add --name subscription\n```\n\n## Creating Subscriptions\n\n### Billing ID\n\nBefore we proceed to create any subscriptions, you are going to need to get the relevant Billing details, to which this subscription will be atttached before it will be possible to comission your subscription.\n\nThe following command should present the options available, to you, .\n\n```\naz billing enrollment-account list\n```\n\n::: note\nIf you get an empty reply back, then you need to have the delegation checked first, as we can not proceeed without the billing information\n:::\n\nTypically, the reply we get should look similar to the following\n\n```json\n[\n  {\n    \"id\": \"/providers/Microsoft.Billing/enrollmentAccounts/7760268a-blah-blah-this-secretdf64c9\",\n    \"name\": \"7760268a-blah-blah-this-secretdf64c9\",\n    \"principalName\": \"azure-test@company.com\",\n    \"type\": \"Microsoft.Billing/enrollmentAccounts\"\n  }\n]\n\n```\n\nThere are two main account types Enterprise and Dev/Test (get better prices on your Azure resources in dev!), The following table presents the Codes for the main subscription options available to us\n\n|Code| Type |\n|---|---|\n|MS-AZR-0017P| Production |\n|MS-AZR-0148P | Dev Test |\n\n### Create the subscription using AZ Cli\n\nThe following command will be used for creating a new subscription\n\n```bash\naz account create --offer-type \"MS-AZR-0017P\" --display-name \"p-gov\" --enrollment-account-object-id \"/providers/Microsoft.Billing/enrollmentAccounts/7760268a-blah-blah-this-secretdf64c9\" --owner-object-id \"\u003cuserObjectId\u003e\",\"0882673f-a426-418f-9681-0bd0c54f63b0\"\n```\n\n### Create the Subscription using Terraform\n\n```hcl\nterraform {\n  required_providers {\n    azurerm = \"\u003e= 2.54.0\"\n  }\n}\n\n\nvariable \"billing_account_name\" {\n  description = \"(required)\"\n  type        = string\n}\n\nvariable \"enrollment_account_name\" {\n  description = \"(required)\"\n  type        = string\n}\n\nvariable \"timeouts\" {\n  description = \"nested block: NestingSingle, min items: 0, max items: 0\"\n  type = set(object(\n    {\n      read = string\n    }\n  ))\n  default = []\n}\n\n\ndata \"azurerm_billing_enrollment_account_scope\" \"this\" {\n  # billing_account_name - (required) is a type of string\n  billing_account_name = var.billing_account_name\n  # enrollment_account_name - (required) is a type of string\n  enrollment_account_name = var.enrollment_account_name\n\n  dynamic \"timeouts\" {\n    for_each = var.timeouts\n    content {\n      # read - (optional) is a type of string\n      read = timeouts.value[\"read\"]\n    }\n  }\n\n}\n\noutput \"id\" {\n  description = \"returns a string\"\n  value       = data.azurerm_billing_enrollment_account_scope.this.id\n}\n\noutput \"this\" {\n  value = azurerm_billing_enrollment_account_scope.this\n}\n\n```\n\n\n\n```hcl\ndata \"azurerm_billing_enrollment_account_scope\" \"example\" {\n  billing_account_name    = \"61732179\"\n  enrollment_account_name = \"149836\"\n}\n\nresource \"azurerm_subscription\" \"example\" {\n  subscription_name = \"p-gov\"\n  billing_scope_id  = data.azurerm_billing_enrollment_account_scope.example.id\n}\n```\n\n_**Note: Please make sure that you are using all the correct details which are can be found from EA portal.**_\n\n```hcl\ndata \"azurerm_subscription\" \"this\" {\n  # subscription_id - (optional) is a type of string\n  subscription_id = var.subscription_id\n\n  dynamic \"timeouts\" {\n    for_each = var.timeouts\n    content {\n      # read - (optional) is a type of string\n      read = timeouts.value[\"read\"]\n    }\n  }\n\n}\n\n```",
    "lastmodified": "2022-12-28T23:25:23.441768212Z",
    "tags": null
  },
  "garden/content/conf-microsoft_ignite-2019": {
    "title": "Microsoft Ignite 2019",
    "content": "\nOrland Florida",
    "lastmodified": "2022-12-28T23:25:23.437767991Z",
    "tags": null
  },
  "garden/content/iac-arm-functionapp_keys": {
    "title": "Azure IaC - Function Keys",
    "content": "\n\nTodays conundrum: As I deploy a new [Function Application](iac-arm-functionapp_scaffold), I need a simple methodology to retrieve the Host Keys for the function application so that I validate the deployment has been successful; and potentially pass on the key to related services, for example API Management.\n\nAs before, I am leveraging templates, and will stay cloud native; this time depending on the functions Output ability to present the keys.\n\u003c!--more--\u003e\n## Solution\n\nA not well know ARM resource is going to be our Hero in this journey. The resource is a called `Microsoft.Web/sites/host/functionKeys` and you can gain a little (actually almost none) more details on the Microsoft [reference site](https://docs.microsoft.com/en-us/azure/templates/microsoft.web/2018-02-01/sites/functions/keys). \n\n\u003e The Documentation refer the API release 2018-02-01; But as you will see in the example code; I discovered that a slightly newer version available.\n\nThe Key to this deployment is the following resource definition\n\n```json\n    \"resources\": [\n        {\n            \"comments\": \"~~ Function App Keys  ~~\",\n            \"type\": \"Microsoft.Web/sites/host/functionKeys\",\n            \"apiVersion\": \"2018-11-01\",\n            \"name\": \"[concat(variables('webappName'), '/default/apimanagement')]\",\n            \"dependsOn\": [\n                \"[resourceId('Microsoft.Web/sites/', variables('webappName'))]\"\n            ],\n            \"properties\": {\n                \"name\": \"api-management\"\n            }\n        }\n    ]\n```\n\nLet's inspect this closer\n\n|Lines | Description|\n|---|---|\n| 2 | A simple comment defining the objective of this resource\n| 3 | The Resource type we are deploying\n| 4 | The Current API Version of the resource; Normally extracted from the Documentation.\n| 5 | The Name of the resource, In this case we are defining a sub-resource of a `Microsoft.Web/sites`; therefore the name is defined as {resourcename}/host/{Function Apps, Host Level Key Name}\n|6-8| Ensure that the Function App has been deployed before we attempt to create the Function App Host Key\n|9-11|Set the name for the Host Key we are creating\n\nAfter this resource has been deployed, we then will reference this in our template `output` section\n\n```json\n    \"outputs\": {\n        \"functionKey\": {\n            \"type\": \"string\",\n            \"value\": \"[listkeys(concat(resourceId('Microsoft.Web/sites', variables('webappName')), '/host/default/'),'2016-08-01').functionKeys.apimanagement]\"\n        }\n    }\n```\n\nIn the `value` section of this output, we are getting a reference to the Function Applications resource identity we just deployed, and with this leverage the ARM Function of `listkeys` to return the function key for the resource we identified on Line 5 in the earlier snippet.\n\nWith this output, we can now chain this as the input for additional templates, or reference the value for our scripts.",
    "lastmodified": "2022-12-28T23:25:23.43376777Z",
    "tags": null
  },
  "garden/content/iac-arm-functionapp_scaffold": {
    "title": "Azure Functions Scaffolding",
    "content": "\n\nMost of my time is spent working on Governance in Azure; and while the cloud native plumbing is amzaing, there are still some gaps which need to be filled, and my go-to tool of choice is Azure Functions.\n\nOne of the products which I oversee the devleopment of is called the 'Concierge' and recently maintaining it has been a challange instead of a joy. It was architected with Fuctions v1 principals, and its well time for a look inside the tool box.\n\nThis post will cover the initial template we get with a V3 Function, and I will extend this with some of the principals which are really important to ensure that as the abilities of the app expand; we will still be able to mainaint and support the code.\n\n## Starting Template\n\nThe function runtime is controlled by a file called `host.json`; and this will control simple yet, extremly important featuers such as logging. In its default form we are offered\n\n```json\n{\n  \"version\": \"2.0\",\n  \"logging\": {\n    \"applicationInsights\": {\n      \"samplingExcludedTypes\": \"Request\",\n      \"samplingSettings\": {\n        \"isEnabled\": true\n      }\n    }\n  }\n}\n```\n \nThis is accompanised with a file to define the environment variables `local.settings.json`\n\n```json\n{\n  \"IsEncrypted\": false,\n  \"Values\": {\n    \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n    \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet\"\n  }\n}\n```\n\n\n\n## HTTP Trigger Function\nThe initial template created for a HTTP Trigger function will include the function itself `function1.cs`\n\n```cs\nusing System;\nusing System.IO;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.Extensions.Logging;\nusing Newtonsoft.Json;\n\nnamespace Custodian\n{\n  public static class Function1\n  {\n    [FunctionName(\"Function1\")]\n    public static async Task\u003cIActionResult\u003e Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req,\n        ILogger log)\n    {\n      log.LogInformation(\"C# HTTP trigger function processed a request.\");\n\n      string name = req.Query[\"name\"];\n\n      string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\n      dynamic data = JsonConvert.DeserializeObject(requestBody);\n      name = name ?? data?.name;\n\n      string responseMessage = string.IsNullOrEmpty(name)\n          ? \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\"\n          : $\"Hello, {name}. This HTTP triggered function executed successfully.\";\n\n      return new OkObjectResult(responseMessage);\n    }\n  }\n}\n```\n\n## Introduce Direct Injection\n\nAdd to the CSProject file `Custodian.csproj`\n\n* Update the `Microsoft.NET.Sdk.Functions` to 3.0.7\n* Add `Microsoft.Azure.Functions.Extensions` 1.0.0\n* Add `Microsoft.Extensions.Http` 3.1.4\n\n```xml\n\u003cProject Sdk=\"Microsoft.NET.Sdk\"\u003e\n  \u003cPropertyGroup\u003e\n    \u003cTargetFramework\u003enetcoreapp3.1\u003c/TargetFramework\u003e\n    \u003cAzureFunctionsVersion\u003ev3\u003c/AzureFunctionsVersion\u003e\n  \u003c/PropertyGroup\u003e\n  \u003cItemGroup\u003e\n    \u003cPackageReference Include=\"Microsoft.Azure.Functions.Extensions\" Version=\"1.0.0\" /\u003e\n    \u003cPackageReference Include=\"Microsoft.Extensions.Http\" Version=\"3.1.4\" /\u003e\n    \u003cPackageReference Include=\"Microsoft.NET.Sdk.Functions\" Version=\"3.0.7\" /\u003e\n  \u003c/ItemGroup\u003e\n  \u003cItemGroup\u003e\n    \u003cNone Update=\"host.json\"\u003e\n      \u003cCopyToOutputDirectory\u003ePreserveNewest\u003c/CopyToOutputDirectory\u003e\n    \u003c/None\u003e\n    \u003cNone Update=\"local.settings.json\"\u003e\n      \u003cCopyToOutputDirectory\u003ePreserveNewest\u003c/CopyToOutputDirectory\u003e\n      \u003cCopyToPublishDirectory\u003eNever\u003c/CopyToPublishDirectory\u003e\n    \u003c/None\u003e\n  \u003c/ItemGroup\u003e\n\u003c/Project\u003e\n```\n\nThe function runtime is controlled by a file called `host.json`; and this will control simple yet, extremly important featuers such as logging. In its default form we are offered\n\n```json\n{\n  \"version\": \"2.0\",\n  \"logging\": {\n    \"applicationInsights\": {\n      \"samplingSettings\": {\n        \"isEnabled\": true,\n        \"maxTelemetryItemsPerSecond\": 5\n      }\n    },\n    \"fileLoggingMode\": \"debugOnly\",\n    \"logLevel\": {\n      \"default\": \"Trace\"\n    }\n  }\n}\n```\n \nThis is accompanied with a file to define the environment variables `local.settings.json`\n\n```json\n{\n    \"IsEncrypted\": false,\n    \"Values\": {\n        \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n        \"FUNCTIONS_WORKER_RUNTIME\": \"dotnet\"\n    }\n}\n```\n\nAdd a `startup.cs`\n\n```cs\nusing Microsoft.Azure.Functions.Extensions.DependencyInjection;\nusing Microsoft.Extensions.DependencyInjection;\n\n\n[assembly: FunctionsStartup(typeof(AzureFunctionDependencyInjection.Startup))]\nnamespace AzureFunctionDependencyInjection\n{\n  public class Startup : FunctionsStartup\n  {\n    public override void Configure(IFunctionsHostBuilder builder)\n    {\n      builder.Services.AddHttpClient();\n    }\n  }\n}\n```\n\n\n## HTTP Trigger Function\nThe initial template created for a HTTP Trigger function will include the function itself `function1.cs`\n\n```cs\nusing System.IO;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Mvc;\nusing Microsoft.Azure.WebJobs;\nusing Microsoft.Azure.WebJobs.Extensions.Http;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.Extensions.Logging;\nusing Newtonsoft.Json;\nusing Microsoft.Extensions.Configuration;\n\nnamespace Custodian\n{\n\n  public class Function1\n  {\n    private readonly ILogger\u003cFunction1\u003e _Logger;\n    private readonly IConfiguration _Configuration;\n\n    public Function1(ILogger\u003cFunction1\u003e logger, IConfiguration config)\n    {\n      _Logger = logger;\n      _Configuration = config;\n    }\n\n    [FunctionName(\"Function1\")]\n    public async Task\u003cIActionResult\u003e Run(\n        [HttpTrigger(AuthorizationLevel.Function, \"get\", \"post\", Route = null)] HttpRequest req)\n    {\n      _Logger.LogTrace(\"Trace - C# HTTP trigger function processed a request.\");\n      _Logger.LogDebug(\"Debug - C# HTTP trigger function processed a request.\");\n      _Logger.LogInformation(\"Information - C# HTTP trigger function processed a request.\");\n      _Logger.LogWarning(\"Warning - C# HTTP trigger function processed a request.\");\n      _Logger.LogError(\"Error - C# HTTP trigger function processed a request.\");\n      _Logger.LogCritical(\"Critical - C# HTTP trigger function processed a request.\");\n\n      string name = req.Query[\"name\"];\n\n      string requestBody = await new StreamReader(req.Body).ReadToEndAsync();\n      dynamic data = JsonConvert.DeserializeObject(requestBody);\n      name = name ?? data?.name;\n\n      string responseMessage = string.IsNullOrEmpty(name)\n          ? \"This HTTP triggered function executed successfully. Pass a name in the query string or in the request body for a personalized response.\"\n          : $\"Hello, {name}. This HTTP triggered function executed successfully.\";\n\n      return new OkObjectResult(responseMessage);\n    }\n  }\n}\n```",
    "lastmodified": "2022-12-28T23:25:23.429767549Z",
    "tags": null
  },
  "garden/content/iac-state-azure_tags": {
    "title": "Azure IaC - Tags as Parameters",
    "content": "\n\nIn the post, I am going to introduce a concept which will allow you to greatly up your Infrastructure as Code game, by using Azure as a State Machine!\n\nOne of the typical challenges when deploying [ARM](ARM) templates, is the sheer number of parameters which we find as a requirement to complete a deployment; which as you will appreciate gets considerably harder as we target many environments.\n\nThere are a number of methods to address this, including the use of Parameter files or Continuous deployment variables; each with their own challenges. \n\n## Putting Tags to Work\n\nTags can be applied at both the level of the subscription and resources.\n\nFor the purpose of this post we will use a scenario of the recovery vault. At the subscription level, we will apply 2 tags to identity the vault we will target for the resources, and then on the actual resource we will add a tag to identity the recovery policy we wish to be applied.\n\n|resource | tag | value |\n|---|---|---|\n|subscription | recoveryVault      | p-vault-001\n|subscription | recoveryVaultRG    | p-vault\n|resource     | recoveryPolicy     | DefaultPolicy\n\n### Subscription Level Tags\n\nWith these Tags applied, we will have something similar to the following at the subscription level:\n\n![Subscription Tags](iac-state-azure_tags/iac-state-azure_tags_2021-07-27-12-38-12.png)\n\nIn my lab the subscription is `/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535`\n\n### Resource Tags\n\nAnd the VM called **p-vm001**, which will represent the resource which we are going to monitor the tag on in my lab will be in the same subscription (for permissions to be simplified), hosted in a resource group called **p-vm**\n\nThe full resource ID of this VM in the lab is `/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001`\n\n![VM with its Tags](iac-state-azure_tags/iac-state-azure_tags_2021-07-27-12-43-35.png)\n\n## Azure Resource Manager Magic\n\nNow, we have all the parts of this environment in place, we will create an ARM template, which simply looks up the values of these tags for us, and to illustrate how it works, it will return the values as outputs.\n\nUsing the **Custom Template Deployment** in Azure Portal, paste the sample template below\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"sourceResourceId\": {\n      \"type\": \"String\",\n      \"defaultValue\": \"/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001\",\n      \"metadata\": {\n        \"description\": \"The resource ID of the resource we wish to look up a tag from.\"\n      }\n    }\n  },\n  \"variables\": {\n    \"referenceSubscriptionTagsResourceId\": \"[concat('/subscriptions/', subscription().subscriptionId, '/providers/Microsoft.Resources/tags/default')]\",\n    \"referenceResourceTagsResourceId\": \"[concat(parameters('sourceResourceId'),'/providers/Microsoft.Resources/tags/default')]\",\n     \"referenceTagsApi\": \"2020-06-01\"\n  },\n  \"resources\": [\n  ],\n\n  \"outputs\": {\n    \"recoveryVaultId\": {\n      \"type\": \"String\",\n      \"value\": \"[resourceId(subscription().subscriptionId,  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults',  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault) ]\"\n    },\n    \"recoveryPolicyId\": {\n      \"type\": \"string\",\n      \"value\": \"[concat( resourceId(subscription().subscriptionId,  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults',  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault), '/backupPolicies/', reference(variables('referenceResourceTagsResourceId'), variables('referenceTagsApi')).tags.recoveryPolicy )]\"\n    },\n    \"subscriptionRecoveryVaultRGTag\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG]\"\n    },\n    \"subscriptionRecoveryVaultTag\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault]\"\n    },\n    \"resourceRecoveryPolicyTag\": {\n      \"type\": \"String\",\n      \"value\": \"[reference(variables('referenceResourceTagsResourceId'), variables('referenceTagsApi')).tags.recoveryPolicy]\"\n    }\n  }\n}\n```\nNext, Check your going to deploy to the lab environment subscription, and resource group where we have the test VM which we will check the tag on\n\n![Project Details in Custom Deployment](iac-state-azure_tags/iac-state-azure_tags_2021-07-27-12-52-12.png)\n\nClick on the **Review and Create** Button, allow the validation to complete, and then click again on **Create**\n\nAs this is simply a reference deployment, it will complete instantly, so next we can check the output from the deployment\n\n![Template Output](iac-state-azure_tags/iac-state-azure_tags_2021-07-27-12-54-16.png)\n\nNow, we should see all the tag values which we set in the lab!\n## Behind the Scenes - Explain the Template\n\nSo, How? \n\nThe following ARM JSON uses the `reference` function to lookup the data we care about\n\nWe establish to variables which we use to point at both the tags provider for the current subscription `referenceSubscriptionTagsResourceId` and also, the Resource ID of the lab VM `referenceResourceTagsResourceId`\n\nWe also will need a provider API version, while looking up the values of these tags, so we also can use a variable `referenceTagsApi` to store this.\n\n```json\n\"variables\": {\n    \"referenceSubscriptionTagsResourceId\": \"[concat('/subscriptions/', subscription().subscriptionId, '/providers/Microsoft.Resources/tags/default')]\",\n    \"referenceResourceTagsResourceId\": \"[concat(parameters('sourceResourceId'),'/providers/Microsoft.Resources/tags/default')]\",\n     \"referenceTagsApi\": \"2020-06-01\"\n  }\n```\n\n### Subscription Tags\n\nNow, the magic. In the outputs, we simply reference the resource ID of the Tag provider for the subscription, with its API version, naming the tag as the last parameter, for example `recoveryVaultRG`\n\n`[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG]`\n\nWe do exactly the same for the subscription Recovery Vault tag called `recoveryVault`\n\n`[reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault]`\n\n### Resource tags\n\nIn a very similar manner we can reference the tags on the resource, returning the `recoveryPolicy` tag value\n\n`[reference(variables('referenceResourceTagsResourceId'), variables('referenceTagsApi')).tags.recoveryPolicy]`\n\n### Getting Creative\n\nFinally, with some concatenation, we can now build resource identifiers dynamically so that we an combine the values of the tags to reference recovery vaults, and recovery policies \n\n```json\n    \"recoveryVaultId\": {\n      \"type\": \"String\",\n      \"value\": \"[resourceId(subscription().subscriptionId,  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults',  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault) ]\"\n    },\n    \"recoveryPolicyId\": {\n      \"type\": \"string\",\n      \"value\": \"[concat( resourceId(subscription().subscriptionId,  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVaultRG ,'Microsoft.RecoveryServices/vaults',  reference(variables('referenceSubscriptionTagsResourceId'), variables('referenceTagsApi')).tags.recoveryVault), '/backupPolicies/', reference(variables('referenceResourceTagsResourceId'), variables('referenceTagsApi')).tags.recoveryPolicy )]\"\n    },\n\n```\n\nIn this case our lab produced the following references\n\n|Output Key | Output value |\n|---|---|\n|recoverVaultId|/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vault/providers/Microsoft.RecoveryServices/vaults/p-vault-001\n|recoveryPolicyId|/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vault/providers/Microsoft.RecoveryServices/vaults/p-vault-001/backupPolicies/DefaultPolicy\n\nNow - we have the ability to leverage Azure as a stateful configuration database!",
    "lastmodified": "2022-12-28T23:25:23.429767549Z",
    "tags": null
  },
  "garden/content/iot-audio-usb_streamer": {
    "title": "Streaming Vynil On Sonos",
    "content": "\n\nA little known trivia - I was once a [DJ](DJ), and spent a lot of my youth behind the decks, in clubs around the West Of Ireland. Today, I still am the proud owner of a very large collection of Vynil and CD music, which of course deserves to get a second life with my digital streaming audio system powered by [Sonos](Sonos)\n\n## USB Turntable Streamer\n\nI own a really nice turntable which is modeled on the Legendary Technical SL1200 MK3, which I am so well aquatinted with, including the awesome Citronix DJ Console which was home to 2 of these beauties in so many clubs way back when...\n\nMy [Audio-Technica AT-LP120-USB](http://amzn.to/2drytFC) device is the focus of todays IoT challange, I will be using a [Raspberry PI 3](Raspberry PI), to stream audio from one of these turntables with USB audio codec output. \n\nIf your in the market, these are also workable options for this exercise\n\n- [Audio-Technica AT-LP60-USB](http://amzn.to/2dSVrGz)\n- [ION Audio Classic LP | 3-Speed USB](http://amzn.to/2e3piLt)\n- [Sony PSLX300USB](http://amzn.to/2dW2Xm7)\n\n\n## Enable SSH before booting\n\nBecause we are going to use the Raspberry Pi headless (without a display) and without keyboard attached, we need a way to control the device. Luckily we can enable SSH by adding an *empty file* called `ssh` to the root of the SD card. This will enable **SSH** for us automatically. \n\nIf you are using the Ethernet port on the Raspberry Pi, networking and SSH should work out of the box with DHCP.\n\n## Update to Current OS\n\nUpdate your Raspbian install:\n\n```bash\nsudo apt-get update\n```\n\n\u003e [!note] Notes\n\u003e The Rasbian Version used at the time of writing is Buster 2021-02\n\n## Connect the USB Turntable to the Raspberry Pi\n\nNow, connect the turntable to the Raspberry Pi, using USB. You can use the command `arecord -l` to check if your device has been detected. Mine shows this:\n\n```bash\npi@raspberrypi:~ $ arecord -l\n**** List of CAPTURE Hardware Devices ****\ncard 1: CODEC [USB AUDIO  CODEC], device 0: USB Audio [USB Audio]\n  Subdevices: 1/1\n  Subdevice #0: subdevice #0\n```\n\nMake a note of the card number, *1 in my case*. This is probably the same for you, but if it differs, you may need to remember it and change accordingly in the following steps.\n\n### Fix volume issues\n\nAs most USB turntables do not have hardware volume control, and the input volume is stuck on roughly half of what it should be, we need to add a software volume control. Create the file `/etc/asound.conf` and edit it to add the following contents:\n\n```bash\npcm.dmic_hw {\n    type hw\n    card 1\n    channels 2\n    format S16_LE\n}\npcm.dmic_mm {\n    type mmap_emul\n    slave.pcm dmic_hw\n}\npcm.dmic_sv {\n    type softvol\n    slave.pcm dmic_hw\n    control {\n        name \"Boost Capture Volume\"\n        card 1\n    }\n    min_dB -5.0\n    max_dB 20.0\n}\n```\n\nNext, run this command to refresh the **alsa** state and also show VU Meters to test the input volume:\n\n```bash\narecord -D dmic_sv -r 44100 -f S16_LE -c 2 --vumeter=stereo /dev/null\n```\n\nAs you might notice, the volume is way too low. You can use `alsamixer` to change the volume. Press `F6` to select the USB Turntable device, and press `TAB` until you see the boost slider.\n\nI have it set to **65** on my setup, but you might try out. Make sure you are not turning it up too high, or your sound quality might degrade due to clipping.\n\n\n## Streaming using `icecast2`\n\nNow, to stream we will use the `icecast2` package, which of course needs to be deployed to our Pi.\n\n```bash\nsudo apt-get install icecast2\n```\n\n### Configure icecast2\n\nNext, we will edit `/etc/icecast2/icecast.xml`, which is the casting servers settings, to provide a name for the stream, and some credential's to protect the stream.\n\n```xml\n\u003cicecast\u003e\n\n    \u003clocation\u003eRecord Room\u003c/location\u003e\n    \u003cadmin\u003eicemaster@localhost\u003c/admin\u003e\n\n    \u003climits\u003e\n        \u003cclients\u003e100\u003c/clients\u003e\n        \u003csources\u003e2\u003c/sources\u003e\n        \u003cqueue-size\u003e524288\u003c/queue-size\u003e\n        \u003cclient-timeout\u003e30\u003c/client-timeout\u003e\n        \u003cheader-timeout\u003e15\u003c/header-timeout\u003e\n        \u003csource-timeout\u003e10\u003c/source-timeout\u003e\n        \u003cburst-on-connect\u003e0\u003c/burst-on-connect\u003e\n        \u003cburst-size\u003e65535\u003c/burst-size\u003e\n    \u003c/limits\u003e\n\n    \u003cauthentication\u003e\n        \u003csource-password\u003evynil\u003c/source-password\u003e\n        \u003crelay-password\u003evynil\u003c/relay-password\u003e\n\n        \u003c!-- Admin logs in with the username given below --\u003e\n        \u003cadmin-user\u003eadmin\u003c/admin-user\u003e\n        \u003cadmin-password\u003evynil\u003c/admin-password\u003e\n    \u003c/authentication\u003e\n\n\n    \u003chostname\u003elocalhost\u003c/hostname\u003e\n\n    \u003clisten-socket\u003e\n        \u003cport\u003e80\u003c/port\u003e\n    \u003c/listen-socket\u003e\n\n    \u003chttp-headers\u003e\n        \u003cheader name=\"Access-Control-Allow-Origin\" value=\"*\" /\u003e\n    \u003c/http-headers\u003e\n\n\n    \u003cfileserve\u003e1\u003c/fileserve\u003e\n\n    \u003cpaths\u003e\n        \u003cbasedir\u003e/usr/share/icecast2\u003c/basedir\u003e\n\n        \u003clogdir\u003e/var/log/icecast2\u003c/logdir\u003e\n        \u003cwebroot\u003e/usr/share/icecast2/web\u003c/webroot\u003e\n        \u003cadminroot\u003e/usr/share/icecast2/admin\u003c/adminroot\u003e\n        \u003calias source=\"/\" destination=\"/status.xsl\"/\u003e\n        \u003c!-- The certificate file needs to contain both public and private part.\n             Both should be PEM encoded.\n        \u003cssl-certificate\u003e/usr/share/icecast2/icecast.pem\u003c/ssl-certificate\u003e\n        --\u003e\n    \u003c/paths\u003e\n\n    \u003clogging\u003e\n        \u003caccesslog\u003eaccess.log\u003c/accesslog\u003e\n        \u003cerrorlog\u003eerror.log\u003c/errorlog\u003e\n        \u003cloglevel\u003e3\u003c/loglevel\u003e \u003c!-- 4 Debug, 3 Info, 2 Warn, 1 Error --\u003e\n        \u003clogsize\u003e10000\u003c/logsize\u003e \u003c!-- Max size of a logfile --\u003e\n    \u003c/logging\u003e\n\n    \u003csecurity\u003e\n        \u003cchroot\u003e0\u003c/chroot\u003e\n    \u003c/security\u003e\n\u003c/icecast\u003e\n```\n\n\n### Starting IceCast2\n\nWe are going to start the casting server, but we will bing to TCP 80, which requires that we allow this to happen, but following these simple steps.\n\n```bash\nsudo setcap 'cap_net_bind_service=+ep' `which icecast2`\n\nupdate-rc.d icecast2 defaults\nsystemctl status icecast2.service\n```\n\n### Icecast2 admin\n\nNow, we should have the casting server online, and working, It will be listening at [http:\\\\\\\\Pi](http://pi-ipaddress/) *(replacing PI with the IP or name you assigned to the device)* and is good for checking the status of connected clients, authenticate with the account *admin* and password *vynil*\n\n## Darkice\n\nNow we need to link the USB audio source, to our Icecast server, and to make this work, we will use another excellent package called `darkice`\n\nThen install a bunch of needed packages:\n\n```bash\nsudo apt-get -y install darkice\n```\n\n### Configure Darkice\n\n\nThis time, we will update the DarkIce configuration file located at `/etc/darkice.cfg` so that it is aware of where the USB turntable is connected to our system (remember the pointer earlier), and how to connect with our Icecast server.\n\n\n```bash\n# this section describes general aspects of the live streaming session\n[general]\nduration        = 0         # duration of encoding, in seconds. 0 means forever\nbufferSecs      = 1         # size of internal slip buffer, in seconds\nreconnect       = yes       # reconnect to the server(s) if disconnected\nrealtime        = yes       # run the encoder with POSIX realtime priority\nrtprio          = 3         # scheduling priority for the realtime threads\n\n# this section describes the audio input that will be streamed\n[input]\ndevice          = hw:1,0    # OSS DSP soundcard device for the audio input\nsampleRate      = 44100     # other settings have crackling audo, esp. 44100\nbitsPerSample   = 16        # bits per sample. try 16\nchannel         = 2         # channels. 1 = mono, 2 = stereo\n\n# this section describes a streaming connection to an IceCast2 server\n# there may be up to 8 of these sections, named [icecast2-0] ... [icecast2-7]\n# these can be mixed with [icecast-x] and [shoutcast-x] sections\n[icecast2-0]\nbitrateMode     = cbr\nformat          = mp3\nbitrate         = 320\nserver          = localhost\nport            = 80\npassword        = vynil\nmountPoint      = listen.mp3\nname            = Turntable\ndescription     = Audio-Technica AT-LP120 Turntable\nurl             = http://turntable\ngenre           = vinyl\npublic          = no\nlocalDumpFile   = recording.m4a\n```\n\n### Darkice Init Script\n\nOk, we now need to create a simple script which will autostart this darkice service each time the Pi is rebooted, so that we can set and forget about this little IoT solution.\n\nStart by creating a new *init.d* file called `/etc/init.d/darkice` and then populating the file with the following script\n\n```bash\n#!/bin/sh\n#\n# Copyright (c) 2007 Javier Fernandez-Sanguino \u003cjfs@debian.org\u003e\n# Copyright (c) 2009 Jochen Friedrich \u003cjochen@scram.de\u003e\n#\n# This is free software; you may redistribute it and/or modify\n# it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 2,\n# or (at your option) any later version.\n#\n# This is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License with\n# the Debian operating system, in /usr/share/common-licenses/GPL;  if\n# not, write to the Free Software Foundation, Inc., 59 Temple Place,\n# Suite 330, Boston, MA 02111-1307 USA\n#\n### BEGIN INIT INFO\n# Provides:          darkice\n# Required-Start:    $network $local_fs $remote_fs\n# Required-Stop:     $network $local_fs $remote_fs\n# Should-Start:\n# Should-Stop:\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Live audio streamer\n# Description:       DarkIce is an IceCast, IceCast2 and ShoutCast\n#                    live audio streamer.\n### END INIT INFO\n\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin\n\nNAME=darkice\nDAEMON=/usr/bin/$NAME\nDESC=\"Live audio streamer\"\nLOGDIR=/var/log\nUSER=nobody\nGROUP=nogroup\nLOGFILE=\"$LOGDIR/$NAME.log\"\n\nPIDFILE=/var/run/$NAME.pid\n\ntest -x $DAEMON || exit 0\n\n. /lib/lsb/init-functions\n\n# Default options, these can be overriden by the information\n# at /etc/default/$NAME\nDAEMON_OPTS=\"\"          # Additional options given to the server\n\nDIETIME=2               # Time to wait for the server to die, in seconds\n                        # If this value is set too low you might not\n                        # let some servers to die gracefully and\n                        # 'restart' will not work\n\n# Include defaults if available\nif [ -f /etc/default/$NAME ] ; then\n\t. /etc/default/$NAME\nfi\n\n# Use this if you want the user to explicitly set 'RUN' in\n# /etc/default/\nif [ \"x$RUN\" != \"xyes\" ] ; then\n    exit 0\nfi\n\nset -e\n\nrunning_pid() {\n# Check if a given process pid's cmdline matches a given name\n    pid=$1\n    name=$2\n    [ -z \"$pid\" ] \u0026\u0026 return 1\n    [ ! -d /proc/$pid ] \u0026\u0026  return 1\n    cmd=`cat /proc/$pid/cmdline | tr \"\\000\" \"\\n\"|head -n 1 |cut -d : -f 1`\n    # Is this the expected server\n    [ \"$cmd\" != \"$name\" ] \u0026\u0026  return 1\n    return 0\n}\n\nrunning() {\n# Check if the process is running looking at /proc\n# (works for all users)\n    sleep 1\n    # No pidfile, probably no daemon present\n    [ ! -f \"$PIDFILE\" ] \u0026\u0026 return 1\n    pid=`cat $PIDFILE`\n    running_pid $pid $DAEMON || return 1\n    return 0\n}\n\nstart_server() {\n# Start the process using the wrapper\n\n        start-stop-daemon --start --quiet --make-pidfile --pidfile $PIDFILE \\\n            --background --chuid $USER:$GROUP --no-close \\\n\t    --exec $DAEMON -- $DAEMON_OPTS \u003e\u003e $LOGFILE 2\u003e\u00261\n\n        errcode=$?\n\treturn $errcode\n}\n\nstop_server() {\n# Stop the process using the wrapper\n        start-stop-daemon --stop --quiet --remove-pidfile --pidfile $PIDFILE \\\n            --exec $DAEMON\n        errcode=$?\n\treturn $errcode\n}\n\nforce_stop() {\n# Force the process to die killing it manually\n\t[ ! -e \"$PIDFILE\" ] \u0026\u0026 return\n\tif running ; then\n\t\tkill -15 $pid\n\t# Is it really dead?\n\t\tsleep \"$DIETIME\"s\n\t\tif running ; then\n\t\t\tkill -9 $pid\n\t\t\tsleep \"$DIETIME\"s\n\t\t\tif running ; then\n\t\t\t\techo \"Cannot kill $NAME (pid=$pid)!\"\n\t\t\t\texit 1\n\t\t\tfi\n\t\tfi\n\tfi\n\trm -f $PIDFILE\n}\n\n\ncase \"$1\" in\n  start)\n\tlog_daemon_msg \"Starting $DESC \" \"$NAME\"\n        # Check if it's running first\n        if running ;  then\n            log_progress_msg \"apparently already running\"\n            log_end_msg 0\n            exit 0\n        fi\n        if start_server \u0026\u0026 running ;  then\n            # It's ok, the server started and is running\n            log_end_msg 0\n        else\n            # Either we could not start it or it is not running\n            # after we did\n            # NOTE: Some servers might die some time after they start,\n            # this code does not try to detect this and might give\n            # a false positive (use 'status' for that)\n            log_end_msg 1\n        fi\n\t;;\n  stop)\n        log_daemon_msg \"Stopping $DESC\" \"$NAME\"\n        if running ; then\n            # Only stop the server if we see it running\n            stop_server\n            log_end_msg $?\n        else\n            # If it's not running don't do anything\n            log_progress_msg \"apparently not running\"\n            log_end_msg 0\n            exit 0\n        fi\n        ;;\n  force-stop)\n        # First try to stop gracefully the program\n        $0 stop\n        if running; then\n            # If it's still running try to kill it more forcefully\n            log_daemon_msg \"Stopping (force) $DESC\" \"$NAME\"\n            force_stop\n            log_end_msg $?\n        fi\n\t;;\n  restart|force-reload)\n        log_daemon_msg \"Restarting $DESC\" \"$NAME\"\n        stop_server\n        # Wait some sensible amount, some server need this\n        [ -n \"$DIETIME\" ] \u0026\u0026 sleep $DIETIME\n        start_server\n        running\n        log_end_msg $?\n\t;;\n  status)\n\n        log_daemon_msg \"Checking status of $DESC\" \"$NAME\"\n        if running ;  then\n            log_progress_msg \"running\"\n            log_end_msg 0\n        else\n            log_progress_msg \"apparently not running\"\n            log_end_msg 1\n            exit 1\n        fi\n        ;;\n  reload)\n        log_warning_msg \"Reloading $NAME daemon: not implemented, as the daemon\"\n        log_warning_msg \"cannot re-read the config file (use restart).\"\n        ;;\n\n  *)\n\tN=/etc/init.d/$NAME\n\techo \"Usage: $N {start|stop|force-stop|restart|force-reload|status}\" \u003e\u00262\n\texit 1\n\t;;\nesac\n\nexit 0\n```\n\nSave the file!\n\n### Autostarting `Darkice`\n\nAlmost ready, Now, In `/etc/default/darkice` check that you have\n\n```ini\nRUN=yes\n```\n\nThen restart the service\n\n```bash\nsystemctl daemon-reload\n```\n\nAdd default user nobody to the audio group (in my case, to work with ALSA)\n\n```bash\nadduser nobody audio\n```\n\nFix start sequence so that Darkice is one of the last services to load\n\n```bash \nupdate-rc.d -f darkice remove\nupdate-rc.d darkice defaults 99\n```\n\nNot directly related to the init script, but note that darkice is being run as nobody:nobody. This user can’t set the realtime scheduling priority requests in darkice’s configuration file, so we give the binary that capability:\n\n```bash\nsudo setcap cap_sys_nice=+ep `which darkice`\n```\n\nWrap up with a `reboot`, and we should be ready to stream\n\n## Connecting to the Stream\n\nWe can validate that everything worked as expiected by connect your streaming client to our new stream server, which is waiting for our conenction at the address `http://vinyl/listen.mp3`. For this we can use for example VLC or even your favourite browser client.\n\nPut on a record, sit back, and you should now be able to enjoy the tunes from the turntable and soak in all that nostalgia.\n \n### Sonos / Tunein\n\nOn Sonos, add your streaming turntable URL (http://vinyl/listen.mp3) by Using the Sonos App for iOS or Android:\n\n* From the **Browse** tab, select **Radio by TuneIn**.\n* Tap **My Radio Stations**.\n* Tap the t**hree dots** in the *top right* and tap **Add New Radio Station**.\n* Enter the *Streaming URL* and *Station Name* and tap **OK**.\n\n\n### Pi Musicbox\nOn Pi Musicbox, add the URL to your `/boot/config/radiostations.js` file or use the GUI.\n\n## Acknowledgements\n\nData in this post is a collection from 3 sources, and combined into a single unified flow, tested on a Pi3, running Buster 2021-02\n\n* https://mykter.com/2019/02/02/streaming-vinyl-raspberry-pi\n* https://github.com/basdp/USB-Turntables-to-Sonos-with-RPi\n* https://github.com/coreyk/darkice-libaacplus-rpi-guide/blob/master/README.md",
    "lastmodified": "2022-12-28T23:25:23.425767327Z",
    "tags": null
  },
  "garden/content/iot-cbus-mqtt_gateway": {
    "title": "CBus MQTT Bridge on Raspberry PI",
    "content": "\n\nTurn back to 2007; My wife and I built our home, integrating many smart technologies, including the Clipsal CBus lighting system. This solution is classified as a Prosumer technology, and is designed to integrate into whole house automation systems.\n\nThe CBus system implements however a proprietary technology, and utilizes a communication protocol which is not 'open source'; however, accepting a license agreement will permit access to this protocol for creating an programming interface.\n\nTo simplify (arguable) the process of integrating with the CBus environment Clipsal released a Bridge solution which enables a TCP interface using a special Java application called 'C-GATE'.\n\nUsing a Raspberry Pi, with a USB to RS-232 cable, which is then connected to a Clipsal interface called the *Serial PCI Module*,\n\n## Prerequisites\n\nDeploy the current release of Rasbian for your Pi. \n\n\u003e Update: Jan 2021 - Currently using Rasbian 2021-01-11 \n\n\nOnce the Pi have been configured and added to the network, we can connect via SSH, and begin installing the pre-requisites for our gateway.\n\n```bash\n# Serial 2 Socket Build Tools\nsudo apt-get install git build-essential autotools-dev devscripts libssl-dev \n# Java 8 Runtime for CGate\nsudo apt-get installopenjdk-8-jdk\n# Node and NPM for CGateWeb\nsudo apt-get installnode npm\n```\n\n## TCP to Serial Bridge\n\nConfiguring the CBus system over TCP however will not work with just C-Gate alone, we need to also establish a TCP connection directly to the *Serial PCI Module*. \n\n### Serial To Socket\n\nThis requires that we compile and run a small *C* application (Don't worry, this is painless and fast); It took a lot of searching to find, posted the source to the following GIT repository; the application is called **ser2sock**.\n\nHere’s the steps to get it set up:\n\n```bash\ngit clone https://github.com/nutechsoftware/ser2sock.git  \ncd ser2sock\n\n./configure --without-ssl\ncc -o ser2sock ser2sock.c  \n\nsudo mv ser2sock /usr/local/bin  \ncd /usr/local/bin/ser2sock  \nsudo chown -R pi:pi /usr/local/bin/ser2sock \n```\n\n\n#### Running as a service - System.d\n\nThe source offers us a sample init script which we can use for starting the service. First we will place this in the SystemD folder, and then update it to match our requirements for C-Gate\n\n```bash\nsudo cp ~/ser2sock/init/systemd/ser2sock.service /etc/systemd/system\n```\n\nUpdate the startup script, `/etc/systemd/system/ser2sock.service` to have the daemon auto-start with the required C-Gate port, which is TCP 10001, and the serial interface baud rate set to 9600.\n\n```bash {linenos=table,hl_lines=[7]}\n[Unit]\nDescription=Proxy that allows tcp connections to serial ports\nAfter=syslog.target network.target\n\n[Service]\nType=forking\nExecStart=/usr/local/bin/ser2sock -p 10001 -s /dev/ttyUSB0 -b 9600 -d\nExecReload=/bin/kill -HUP $MAINPID\n\n[Install]\nWantedBy=multi-user.target\n```\n\nThen activate using:\n\n```bash\nsudo systemctl enable ser2sock.service  \nsudo systemctl start ser2sock.service\n```\n\n## Installing C-Gate\n\n[Download](https://updates.clipsal.com/ClipsalSoftwareDownload/mainsite/cis/technical/CGate/cgate-2.11.4_3251.zip) the current software release of C-Gate off the clipsal website and unzipped the files into `/usr/local/bin/cgate.` \n\n```bash\ncd ~\nwget https://updates.clipsal.com/ClipsalSoftwareDownload/mainsite/cis/technical/CGate/cgate-2.11.4_3251.zip  \nunzip cgate-*.zip  \nsudo mv cgate /usr/local/bin\n```\n\n\n### Running C-Gate as Service - System.d\n\nAdding the following 'system.d' startup script, to have the daemon auto-start with the operating system `/etc/systemd/system/cgate.service`\n\n```bash\n[Unit]  \nDescription=Clipsal CBUS Gateway\nAfter=syslog.target network.target\n\n[Service]  \nExecStart=/usr/bin/java -Djava.awt.headless=true -jar -noverify /usr/local/bin/cgate/cgate.jar  \nRestart=always  \nUser=root  \nGroup=root  \nEnvironment=PATH=/usr/bin:/usr/local/bin  \nWorkingDirectory=/usr/local/bin/cgate/\n\n[Install]  \nWantedBy=multi-user.target\n```\n\nThen activate using\n\n```bash\nsudo systemctl enable cgate.service  \nsudo systemctl start cgate.service\n```\n\n### C-Gate Access Control\n\nWe must configure the C-Gate service to allow remote connections from machines on the network by editing the access control file `nano /usr/local/bin/cgate/config/access.txt`; adding a line, providing the ip address of the remote system. In the example I am allowing the network 172.16.0.0/23 or the IP's in the range of 172.16.0.0 to 172.16.255.255\n\n```bash\necho \"remote 172.16.255.255 Program\" \u003e\u003e /usr/local/bin/cgate/config/access.tx\n```\n\nSave, then restart cgate:\n\n```bash\nsudo systemctl restart cgate.service\n```\n\n### C-Gate Auto-Connect\n\nEdit the file `/usr/local/bin/cgate/config/C-gateConfig.txt`. Set the **project.default** and **project.start** lines to the name of the C-Gate Project.\n\nIf you do not know your project name, you can interact with C-Gate over telnet\n\n```bash\ntelnet 127.0.0.1 20023\n\n\u003e project list\n\n```\n\nIn my environment, my project is called `home`\n\n## CGate MQTT\n\nMQTT interface for CBus lighting written in Node.js\n\n```bash\ncd /usr/local/bin\nsudo git clone https://github.com/the1laz/cgateweb.git\ncd cgateweb\nnpm install\nsudo nano settings.js\n```\n\nPut in your settings.\n\n```bash\nsudo cp cgateweb.service /etc/systemd/system\nsudo systemctl enable cgateweb\nsudo systemctl start cgateweb\n```\n\nAt this point, you should start seeing your [MQTT](MQTT) Service update with CBus Lighting application status.",
    "lastmodified": "2022-12-28T23:25:23.445768433Z",
    "tags": null
  },
  "garden/content/personal-reflection-2018": {
    "title": "Hitting Reset",
    "content": "\n\n10 Years, It is hard to believe that I have been posting thoughts here that long. And how so much has changed since I begun?\n\nI started this journey with the encouragement of some amazing people in Microsoft, as an opportunity to spread the news about *Hyper-V* and even more relevant at the time *System Center Virtual Machine Manager* which was still known by its code name!.\n\u003c!--more--\u003e\nMy daily experience with this application, Windows Server, and real-world enterprise issues; positioned me at one of the leading edges of Microsoft Technologies; and fully armed with a true business driver pushing forward. Wounds and pains exposed, I gained a lot of insight to the digital plumbing of these technologies and as a result of a lot of fantastic information to share - sometimes not good news; but never the less - reality.\n\nI have primarily worked in the mindset that when I find an issue to be addressed, before sharing, escalating or attacking - I need to stop and consider solutions; which normally result in a more constructive and progressive approach to unblocking my path. That ethos spans back to my days working as what would be considered today in 2018 as an *IoT* architect; but 20 years ago in a Rubber Molding plant, The Operations Manager always reminded me as I entered his office, \"If you don't have some suggestion for a solution before entering and presenting a problem, leave now, and come back when I am prepared (But don't spend all day - Problems cost money!).\"\n\nIn Hindsight, this approach challenged my limits every day; but I now also realise that he actually had no technical knowledge, and without my suggestions, we were heading the route of the Titanic!\n\n## System Center\n\nWhile the solution itself continues to live on, as clear from the very recent launch of *System Center 2019* at the Ignite Conference in Florida; My own passion and engagement with this technology has ultimately diminished to a point of history., despite co-authoring and technically reviewing a number of books, speaking at so many events, and investing 1000's of hours.\n\n## Personal Redevelopment\n\nAfter almost 20 years I changed Jobs, A decision which was extremely difficult to make; and honestly post that change point; I took at least 6 more months to adjust to the new world order.\n\nI found myself amidst a team of like-minded peers, left to find a niche which I could own. Despite working with fantastic scenarios, these new challenges were amazing; and I was learning new stuff again. But, yet I still felt uninspired.\n\nJust reflect on the number of blog posts I have published in the last 3 years.\n\nMy personal life also took a major change; and today after celebrating 21 years of marriage; I am a super proud father of two amazing girls, with my oldest just after celebrating her 4 birthday and the youngest just turned 2.\n\nWhen I reflect on these massive changes, it is a totally different world from when I stood just 10 years ago.\n\n## Inspiration\n\nLast week I participated in my first **Microsoft Ignite** event; and spent the vast majority of this opportunity meeting with so many old friends who have also evolved into completely new roles within their organizations.\n\nAs an example, Mr Taylor Brown; I had the honour of meeting Taylor for the first time almost 12 years ago. Back then we both were working on Test Scenarios for Hyper-V; in his role, he ran the labs for Microsoft's internal testing; and I was responsible for our internal *Technology Adoption Program (TAP)* Pilot testing. Today, Taylor owns the *Docker (Container)* features in Windows Server. An amazing achievement, from an inspiring person and a good friend. \n\nThere are so many amazing people, with just as amazing stories; and I was so proud to be able to stop, and say hello to these *icons*, and learn how their lives have also changed.\n\n## Governance\n\nAs the adoption, and practices of Cloud become centrally focused for so many organizations the focus shifts left, as Compliance, Control, and Culture changes ignite to enable a completely fresh view of the potential.\n\nWhen I combine past experience, with the foundational tooling which 3rd parties like Terraform offer, and native tooling which Microsoft adding to the core of their offerings, the next challenge is clear.\n\nNow, I see a clear path to assist organizations of any size to evolve from what might have been the chaos of Shadow IT, the central control of IT, or the old practices of Enterprise Architects; and guide them to a culture driven enablement of Cloud; supported with *Governance* and enabling that missing **trust** thought the use of *Safety Guard Rails*\n\n## Finding my Mojo\n\nLooking back at Ignite and the last 12 months of work; I now feel like I have a new rhythm. I have found a new passion - Namely that of enabling organizations on this transformation trough common sense, debate and technology.\n\nAddressing political issues, and provisioning the structures of support required to encourage trust and co-operation, all of which is based on logical technical foundations *(Yes Mr Spock!)*. \n\nEven more rewarding, I am actually observing directly the impact this has on people and processes as they evolve their life's with the culture changes required to start a new fabric of growth in cloud and DevOps practices; while also addressing their relationships with the business owners.\n\nI believe I have found my *Mojo*.\n\n## The Next Step...\n\nOver the last years, I have had the honour of presenting at so many fantastic conferences, delivering workshops, and engagement in meetups. When I reflect on the topics I have focused on these all have contributed to building strong foundational elements to this new way. \n\nTopics ranging from Containerization and Automation with Docker and Kubernetes, Serverless to AI, BOT Frameworks to Python, Git flow to Infrastructure as Code, DevOps to Event Handling; while all feel very disjointed, these technologies combined are core to understanding how the world is evolving, and therefore how the organizations can adopt.\n\nIts time to **Hit Refresh**, and join me on this next wave, as I share, present, document, and offer guidance; both here on the Blog, On stages in various locations and professionally, through a range of mediums from 'hands-on' demonstrations, Technical guides, papers, and talks.\n\nI have my *mojo* charged and ready; Have You?",
    "lastmodified": "2022-12-28T23:25:23.421767106Z",
    "tags": null
  },
  "garden/content/sw-freeradius-getting_started": {
    "title": "Installing FreeRadius",
    "content": "\n\nFreeRADIUS is an open source, high-performance, modular, scalable and feature-rich RADIUS server. It ships with both server and radius client, development libraries and numerous additional RADIUS related utilities, for Linux\n\nFreeRADIUS supports request proxying, with fail-over and load balancing, as well as the ability to access many types of back-end databases.\n\nRADIUS, which stands for ***R**emote **A**uthentication **D**ial-**I**n **U**ser **S**ervice*, is a network protocol used for remote user authentication and accounting. It provides AAA services; namely **A**uthorization, **A**uthentication, and **A**ccounting.\n\n{{\u003c series \"RADIUS with Azure Active Directory\" \u003e}}\n\n## FreeRadius\n\n\u003e [!note] Linux OS\n\u003e The Article is authored using **Ubuntu 18.04** as the target distribution\n \nYou can view versions of FreeRadius available in your distribution:\n\n```bash\n$ sudo apt policy freeradius\nfreeradius:\n  Installed: (none)\n  Candidate: 3.0.16+dfsg-1ubuntu3.1\n  Version table:\n *** 3.0.16+dfsg-1ubuntu3.1 500\n        500 http://azure.archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages\n        500 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages\n        100 /var/lib/dpkg/status\n     3.0.16+dfsg-1ubuntu3 500\n        500 http://azure.archive.ubuntu.com/ubuntu bionic/main amd64 Packages\n```\n\nInstall FreeRadius packages from official Ubuntu APT repository with the commands below:\n\n```bash\nsudo apt -y install freeradius freeradius-mysql freeradius-utils\n```\n\nAmong the packages installed, we have the **freeradius mysql module** and **freeradius utilities** packages.\n\n## FreeRadius 'Network Radius' Repositories\n\nThe official distributions for FreeRadius are published by **Network Radius**, and typically are fresher than the builds included with the stock Linux Distribution\n\n\u003e [!important] Ubuntu 18.04\n\u003e However, the **Ubuntu 18.04** version of **FreeRadius** is *3.016*, and has some issues which will essentially block our progress. We can use the official packages to deploy the current release of the service, at the time of writing this is *3.0.23*\n\u003e \n\u003e Read more here: https://networkradius.com/packages/\n\n### Add the **Network Radius** Repository\nAdd the following line to your apt source list `/etc/apt/sources.list`\n\nFor **Ubuntu Bionic 18.04**\n```bash\necho \"deb http://packages.networkradius.com/releases/ubuntu-bionic bionic main\" | sudo tee /etc/apt/sources.list.d/networkradius.list \u003e /dev/null\n```\n\nFor **Ubuntu Focal 20.04**\n```bash\necho \"deb http://packages.networkradius.com/releases/ubuntu-focal focal main\" | sudo tee /etc/apt/sources.list.d/networkradius.list \u003e /dev/null\n```\n\n### Trust the Repository\nNow, we need to trust this new repository, and then update the applications database, before we finally upgrade FreeRadius\n\n```bash\ncurl -s 'https://packages.networkradius.com/pgp/packages%40networkradius.com' | sudo tee /etc/apt/trusted.gpg.d/packages.networkradius.com.asc \u003e /dev/null\n\nsudo apt-get update\n```\n\n### Install FreeRadius from **Network Radius** Repository\n\nNow, we should be ready to Install FreeRadius packages from official Network Radius APT repository with the commands below:\n\n```bash\nsudo apt -y install freeradius freeradius-mysql freeradius-utils\n```",
    "lastmodified": "2022-12-28T23:25:23.421767106Z",
    "tags": null
  },
  "garden/content/sw-openssl-certificate_issuing": {
    "title": "Create PKI Certificate for Linux Server",
    "content": "\n\nIt is enevitable, that at some point in time, you will need to issue a certificate for your system. There are a couple of relativly simple steps to acheive this, begining with the creation of a certificate request, then submitting this to an online certificate authority, which will process the request and issue you a certificate with both a Public and Private Key.\n\nIn this post, we will use the **[OpenSSL](sw-openssl-install_ubuntu)** utility to create such a request file, and walk trough the steps of issuing a certificate from a Windows PKI Server.\n\n## Creating our Certificate Request\n\nWe will create a Certificate Request template file which defines the requirements or purpose of the certificate, optional data can be encoded into the certificate, for illustrative purposes. I will include [Subject Alternate Name](Subject Alternate Name) in the issued certificate. \n\n\u003e [!note] Computer Certificate\n\u003e In this article, the certificates we are working with are to be used in validating a computers identity. This is a common requirement when authenticating with WiFi or VPN services within the organisation\n\n\nThe request file we create can be named as you wish, I will be using  `~/san.cnf`\n\n```ini\n[ req ]\ndefault_bits = 2048\ndistinguished_name = req_distinguished_name\nreq_extensions = req_ext\n[ req_distinguished_name ]\ncountryName = Country Name (2 letter code)\nstateOrProvinceName = State or Province Name (full name)\nlocalityName = Locality Name (eg, city)\norganizationName = Organization Name (eg, company)\ncommonName = Common Name (e.g. server FQDN or YOUR name)\n[ req_ext ]\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = radius.diginerve.ie\n```\n\nThe above is a working template - the only changes you should make to this file is the **atl_names** at the bottom, where these  should represent the name of the system you wish to have the certificate issued on behalf of; In my example this is *radius.diginerve.ie*\n\n## Create the Private Key\n\nFrom the computer you are wishing to create the certificate for (for example a [Linux FreeRadius server](sw-freeradius-getting_started), we will generate the request for private key using the template as follows.\n\n\u003e [!Note] OpenSSL\n\u003e OpenSSL will ask some additional questions during execution\n\nThe following command parameters instruct [OpenSSL](sw-openssl-install_ubuntu) to generate a certificate request\n* `req`: OpenSSL should generate a Certificate Request\n* `-newkey rsa:2048`: The Request should be encrypted using **RSA 2048** \n* `-nodes`: \n* `-keyout server.key`: The Private Key should be saved as `server.key`. This file should be protected\n* `-out server.csr`: The name of the output file which containes the public shareable, encoded request we will send to the issuing server.\n* `-config san.cnf`: The name of the file which we previosuly saved as the [request template](sw-openssl-certificate_issuing#creating-our-certificate-request)\n\n```bash\nopenssl req -newkey rsa:2048 -nodes -keyout server.key -out server.csr -config san.cnf\n\n\nGenerating a RSA private key\n............................+++++\n........................................................+++++\nwriting new private key to 'server.pem'\n-----\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:ie\nState or Province Name (full name) [Some-State]:Mayo\nLocality Name (eg, city) []:Ballina\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:DigiNerve\nOrganizational Unit Name (eg, section) []:IT\nCommon Name (e.g. server FQDN or YOUR name) []:radius.diginerve.ie\nEmail Address []:support@diginerve.ie\n```\n\n### Validate the Request File\n\nBefore we take the Certificate to the PKI server to issue the certificate for the server, we can double check the request to ensure that the SAN is included as per the Template.\n\n```bash    \nopenssl x509 -in server.crt -text -noout\n```\n\nThis command will read in the new certificate request file, and dump its content in a readable text format so we can validate the request is correct.\n\n## Present the Certificate Request to Windows PKI\n\nWith our Certificate Request file (`.csr`) now created, we will proceed to submit the request to the PKI Infrastructure which we will be requesting to issue us the certificate. \n\n### Windows PKI Infrastructure\n\n\u003e [!important] Windows PKI\n\u003e This portion of the Article focuses on using a Windows PKI deployed to provide a Private Key Infrastructure within an Organisation to process the request and issue the certificate. \n\nAs this article is focusing on the use of a Private Enterprise Certificate to authenticate a trusted computer access the organisations WiFi or VPN environment, we will concentrate on working with this technology to process and issue a certificate fit for purpose from the organisations infrastructure.\n\nBefore we begin, we do need to have a copy of the certicate request we created,  `server.csr` transferred to the system we will be operating on to process the request.\n\n### Check the Windows PKI Server Templates\n\nDespite the fact that we created a template earlier for our certicicate to indicate that we going to concentrate on a *Computer* certificate, the Windows PKI environment premits the System Adminstrators to name the templates which it will issue certificates using to match the [organisations governance](organisations governance) standard.\n\nTherefore, before we present the certificate to the Issuing server to process our request, we should first ask the Issuing Server what are the templates which it is willing to allow us to request, and what name has been associated with the various templates.\n\n\u003e [!warning]\n\u003e Windows PKI services follow an [RBAC](RBAC) architecture, in which the templates that are offered will depend on the context of the request, for example its quite common to find that users are not premitted to request machine or computer certificates. These will typically be resticted to a computer or service identity.\n\nUsers looking to issue a computer or machine certificate, typically are using the certicicates to protect a web site using HTTPS for transport, therefore, it is common that the only suitable template that we can choose maybe a *web server* or *server* template, from the list offered by the Issuing CA\n\n```powershell\ncertutil -CATemplates\n\n\nIPSECIntermediateOffline: IPSec (Offline request) -- Auto-Enroll: Access is denied.\nCEPEncryption: CEP Encryption -- Auto-Enroll: Access is denied.\nEnrollmentAgentOffline: Exchange Enrollment Agent (Offline request) -- Auto-Enroll: Access is denied.\nAdministrator: Administrator -- Auto-Enroll: Access is denied.\nWebServer: Web Server -- Auto-Enroll: Access is denied.\n\n\nCertUtil: -CATemplates command completed successfully.\n```\n\n### Requesting the Certificate\n\n\u003e [!Important] Certificate Templates\n\u003e From the list of offered Certificate templates, we are technially looking tof the template which include's the **Server Authentication** OID. \n\u003e For systems which do not include templates with the **Server Authentication** OID in the available list, refer to the topic [Manually adding the Server Authentication OID](sw-openssl-certificate_issuing#optional---manually-adding-the-oid-to-the-certificate-request)\n\nIn this context I will use **'Web Server'** in the request the certificate\n\n```powershell\ncertreq.exe -attrib \"CertificateTemplate:Web Server\" server.csr\n```\n\nA window will popup asking you to select the [Certificate Authority](Certificate Authority) (CA) where your request is to be submitted to, typically any of the presented servers will work fine, as long as they are approved to issue certificates based on the selected template. If the selected Certificate Authority has a problem with processing the request, try selecting an alternative, or checking within the organisation PKI support team to identify the correct server to utilize.\n\nOn a sucessful presentation, the `certreq` tool will present a dialog asking for where to save the new certificate. Provide a filename (for this simple example, Ill just call it `server`) and complete the wizard. \n\nAt this point we now have our issued certificate with ONLY its public key, stored in a file called `server.cer` which we will next need to transfer back to the system we orginally created the request on.\n\n\u003e [!Warning] Public and Private Keys\n\u003e The Certificate we just received from the PKI server only contains the Public Key, the matching Private Key (the main secret for decrypting) is stored on the machine we created the request from initially, which in this article was called `server.key`\n\nCopy the public key certificate file, 'server' back to the machine we created the request on, in this case, it was our FreeRadius server.\n\n### Optional - Manually Adding the OID to the Certificate Request\n\n\u003e [!Important] Server Authentication OID\n\u003e This following step is only relevant if the issueing server, will not offer a template which includes the property for Server Authentication\n\nWhen generating certificates for use by **FreeRadius EAP-TLS**, This has two requirements so that the service will successfully validate the certificate.\n\n* Include the \"Server Authentication\" (OID 1.3.6.1.5.5.7.3.1)\n* Include a Subject Alternate Name\n\n\u003e **802.1x** \n\u003e\n\u003e When a client uses PEAP-EAP-MS-Challenge Handshake Authentication Protocol (CHAP) version 2 authentication, PEAP with EAP-TLS authentication, or EAP-TLS authentication, Microsoft specifies that certificates must have the \"Enhanced Key Usage\" attribute with the value \"Server Authentication\" (OID 1.3.6.1.5.5.7.3.1). [Ref.: http://support.microsoft.com/kb/814394/en-us ]\n\nIf these extension are not present in your FreeRadius certificate, the auth process will fail, because the client will stop communicating with your server due that it can't validate your cert.\n\nSince the certificate request generated in openssl according to the procedure above does not provide this attribute, it is necessary to add it to the pending request with the Windows CLI command `certutil`.\n\nThe general syntax is `certutil -setextension RequestID ExtensionOID Flags @InFile`\n\n- The *RequestID* is the sequence ID for our requested certificate from the issueing Certificate Authority\n- The *ExtensionOID* for the attribute \"Enhanced Key Usage\" is **2.5.29.37**\n* The *flags* value is set to *0*.\n* For the *@InFile*, we will create an input text file `eku.txt` as follows\n  ```powershell\n  echo 30 0a 06 08 2b 06 01 05  05 07 03 01 \u003e eku.txt\n  ```\n\nNow, with all the information required, we can run the following command, Remember to replace `[RequestID]` with the actual numeric request ID\n\n```powershell\ncertutil -setextension [RequestID] 2.5.29.37 0 @eku.txt\n```\n\nOnce the command has completed, launch the Windows  Certification Authority application\n\n* Open **Pending request**\n\t* Right click on the request we just modified (RequestID)\n\t* Select **All tasks** --\u003e  **Issue**\n* Go to **Issued certificates**\n\t* Locate and double-click on the one you just issued *[RequestID]*.\n\t\t* A window will open displaying cert's info. \n\t\t* Go to the tab **Details** and check that the field **Enhanced Key Usage** is present and its value is **Server Authentication (1.3.6.1.5.5.7.3.1)**.\n\t\t* Click on the button **Copy to file...** and save it as either DER encoded or Base-64 encoded\n\t\t\t* Provide a filename (let's call it `server`) and finish the wizard. \n\t\t\t* This will give you a file `server.cer`.\n\nCopy the public key certificate file, 'server' back to the machine we created the request on, in this case, it was our FreeRadius server.\n\n## Verify the Servers Certificate\n\nBack on our Linux node, with a copy of our new certificate on hand `server.cer`, we can now check that the certificate matches all the requirements we outlined at the beginning of this process\n\n* Include the Servers **Common name**, eg `radius.diginerve.ie`\n* Include the **Server Authentication** (OID 1.3.6.1.5.5.7.3.1)\n* Include a **Subject Alternate Name**\n\n\u003e [!Note] FreeRadius\n\u003e FreeRaidus expects that the public certificates be placed in the folder `/etc/freeradius/certs`\n\nUsing OpenSSL can view the certificate\n\n\n```bash\nroot@p-nps-radius01:/etc/freeradius/certs# openssl x509 -in server.pem -text -noout\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            51:~~~~:33\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: DC = IE, DC = diginerve, DC = ie, CN = MY Domain CA Issuer\n        Validity\n            Not Before: Apr  7 11:34:18 2021 GMT\n            Not After : Feb  1 12:16:46 2022 GMT\n        Subject: C = IE, ST = Mayo, L = Ballina, O = MIS, CN = radius.diginerve.ie\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                RSA Public-Key: (2048 bit)\n                Modulus:\n                    00:d0:6d:0d:14:5b:01:a9:4a:8a:ec:51:84:5b:6c:\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    53:87:86:a5:47:f0:81:e6:85:06:c6:96:10:ed:68:\n                    31:9d\n                Exponent: 65537 (0x10001)\n        X509v3 extensions:\n            X509v3 Subject Alternative Name: \n                DNS:radius.diginerve.ie\n            X509v3 Subject Key Identifier: \n                43:35:46:D4:ED:00:89:83:F2:73:B5:6B:51:15:BB:B6:AE:7D:49:8E\n            X509v3 Authority Key Identifier: \n                keyid:0C:21:7E:6B:1D:D6:93:BB:17:7A:55:53:88:CD:5F:5F:64:A3:83:0E\n\n            X509v3 CRL Distribution Points: \n\n                Full Name:\n                  URI:ldap:///CN=My%20Issuer,CN=p-ie1ca-ca01,CN=CDP,CN=Public%20Key%20Services,CN=Services,CN=Configuration,DC=pki,DC=diginerve,DC=ie?certificateRevocationList?base?objectClass=cRLDistributionPoint\n\n            Authority Information Access: \n                CA Issuers - URI:ldap:///CN=My%20Issuer,CN=AIA,CN=Public%20Key%20Services,CN=Services,CN=Configuration,DC=pki,DC=diginerve,DC=ie?cACertificate?base?objectClass=certificationAuthority\n\n            X509v3 Key Usage: critical\n                Digital Signature, Key Encipherment\n            1.3.6.1.4.1.311.21.7: \n                0..\u0026+.....7.....j...#...........,\u0026...?......d...\n            X509v3 Extended Key Usage: \n                TLS Web Server Authentication\n            1.3.6.1.4.1.311.21.10: \n                0.0\n..+.......\n    Signature Algorithm: sha256WithRSAEncryption\n         ab:7f:d6:12:80:f7:fe:d6:d9:44:f8:1a:fc:fb:91:2d:eb:05:\n         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n         b1:f2:ae:a5\n```\n\nSearch for the following to be present and correct\n\n* **Subject:** and check it,\n* Next, Locate the **X509v3 Subject Alternative Name:** and ensure it exists and is correct,\n* Finally, **X509v3 Extended Key Usage** should contain *TLS Web Server Authentication  1.3.6.1.4.1.311.21.10*\n\n### FreeRadius Compatability\n\n\u003e [!Note] DER Encoded Certificate\n\u003e FreeRadius does not work with **DER Encoded Certificates**, these must be instead in **Base64** format to function\n\nIf necessary, we may have to convert our certifcate from **DER Encoding** to **Base64 Encoding**. This can be completed using the `openssl` tool with the following syntax:\n\n```bash\nopenssl x509 -inform DER -in server.cer -outform PEM -out server.pem\n```",
    "lastmodified": "2022-12-28T23:25:23.43376777Z",
    "tags": null
  },
  "garden/content/sw-openssl-install_ubuntu": {
    "title": "Installing the Latest OpenSSL release",
    "content": "\n\nBeing over 25 years old, OpenSSL can be found on just about any system you work with today; but this does not imply that the version installed is current (or even close).\n\nDuring its life, there have been many instances where OpenSSL has been in the news, where some new vulnerability has being discovered, and quickly after, patched.\n\nThe Swiss Army Knive of SSL Certificate's, this is a tool that everyone should have at least used once in their administrative duties.\n\nTo check if OpenSSL is on your system, and more importantly, the version installed, we can use the following simple command.\n\n```bash\nopenssl version\n```\n\nIn my case “OpenSSL 1.1.1 ” was the result.\n\n## Installing the Current Release\n\nWe will go directly to the [OpenSSL](https://openssl.org) distribution source, and download the current stable version, which at the time of writing is now **openssl-1.1.1k**  and save it into `~/Downloads` directory:\n\n```bash\ncd ~/Downloads\nwget https://www.openssl.org/source/openssl-1.1.1k.tar.gz\n```\n\n### Create Working Folder\n\nWe will install the new version at `/opt/openssl`. To do that we need to create and change directory\n\n```bash\nsudo mkdir /opt/openssl\ncd /opt/openssl\n```\n\nExtract the downloaded compressed file into this directory:\n\n```bash\nsudo tar xfzv ~/Downloads/openssl-1.1.1k.tar.gz --directory /opt/openssl\ncd /opt/openssl\n```\n\n## Configure the Build\n\nBefore we proceed to compile the utility, we first must set some of the configuration settings, which will be used to let OpenSSL know where it is to be installed, and what folder it can use for its working space. \n\nThe is quite trivial, as we use the provide `config` script and pass in the parameters required. We are running this with *sudo* to ensure the script can check the system for dependencies and create any folders necessary.\n\nThe folders I will be using for the installation include\n* `/opt/openssl` as the home directory and \n* `/opt/openssl/ssl` as the directory where OpenSSL will store certificates and private keys.\n\n```bash\nsudo ./config --prefix=/opt/openssl --openssldir=/opt/openssl/ssl \n```\n\n## Building and Installing OpenSSL\n\nWith the configuration complete, we now can use the *Makefile* which has just being customized, to build, and then install the new version of OpenSSL. Depending on the speed of your system this can take some time to complete.\n\n```bash\nsudo make \nsudo make install\n```\n\nCongratulations, OpenSSL current version is installed. \n\n## Multiple Instances\n\nWait - we do have a small issue. Right now there are two installations of OpenSSL on your system:\n\n* The original installation\n* and this New Current release we built from source.\n\n## Swapping the Binaries\n\nI won’t delete the original version, instead I will simply rename it to *openssl.old*, keeping this in the original installation path location.\n\n```bash\nsudo mv /usr/bin/openssl /usr/bin/openssl.old \n```\n\n\u003e [!important] Compatability\n\u003e  In the literature there are references to applications that expect openssl to be at the original directory. To maintain compatibility, and avoiding the need to alter the environment variable PATH, we will create a symbolic link `/usr/bin/openssl` pointing to `/opt/openssl/bin/openssl`\n\nWe simply link the new build to the original folder, as follows:\n```bash\nsudo ln -s /opt/openssl/bin/openssl /usr/bin/openssl\nls -lisah /usr/bin/openssl\n```\n\nThe `ls` command above, just offers up a view to ensure that the symbolic link was established from the original path to our new installation.\n\n## Setting the Module Library\n\n\u003e [!important] OpenSSL\n\u003e OpenSSL is a dependency for a lot of different applications, therefore we should complete the installation, by updating the module library configuration to ensure any application we deploy will find and use this release.\n\nModify the module loader configuration in `/etc/ld.so.conf.d/openssl.conf`, open the file and edit it using `vi /etc/ld.so.conf.d/openssl.conf` so that it reads as follows\n\n```ini\n/opt/openssl/lib\n```\n\nNow, reload the modules\n\n```bash\nsudo ldconfig \n```\n\n### Sanity Check\n\nAnd finally, we can verify that everything is correct \n\n```bash\nwhich openssl\nopenssl version\nopenssl\n```\n\n## Clean up\n\nReboot your system to make things permanent and execute the last three commands again, targeting, obviously, the same outcome.\n\nBy now you have OpenSSL new version installed and working correctly. But if you try to download any of the previous files, for instance `openssl-1.1.1k.tar.gz`, you will get the following error:\n\n```error\nThe Certificate Authority “Let’s Encrypt Authority X3” that issued the server certificate is not in OpenSSL certificate and private key directory */opt/openssl/ssl*.\n```\n\nIf this is the desired behavior skip what follows and you have **OpenSSL 1.1.1k** completely installed.\n\nIf this is not your desired behavior, you can copy all certificates in `/etc/ssl/certs/` to `/opt/openssl/ssl/certs`. However I prefer a slightly different approach, simply linking by making `/opt/openssl/ssl/certs` a symbolic link pointing `/etc/ssl/certs/` files.\n\nTo Implement the preferred approach, issue the following command\n\n```bash\nsudo ln -s /etc/ssl/certs/. /opt/openssl/ssl/certs/\n```",
    "lastmodified": "2022-12-28T23:25:23.425767327Z",
    "tags": null
  },
  "garden/content/sw-openssl-split_certificates_and_private_keys": {
    "title": "Extracting Certificates from a PFX",
    "content": "\n\nUsing **OpenSSL** we can extract the private key, and the certificate into independent file's, which is required for most networking devices, and linux services.\n\nYou will need to install the [OpenSSL](sw-openssl-install_ubuntu), package, either on your system.\n\n## Export the private key from the PFX file\n\nWe begin, by passing in the PFX and requesting the Private key to be placed into its own file. This process will require that you provide the password which is used to protect the PFX file, and also a new password to protect the new private key file (this is not optional).\n\n```bash\nopenssl pkcs12 -in filename.pfx -nocerts -out PrivateKeyWithPassword.pem\n```\n```output\nEnter Import Password: [Input the export password of the PFX File]\nMAC verified OK\nEnter PEM pass phrase: [Min 5 Char New Temp Password]\n```\n\n### Remove the passphrase from the private key\n\nThe previous step created a new password protected file, which we called `PrivateKeyWithPassword.pem` that contains only the certificates private key. In many cases, we may need to use this file, without the password protection, so the following step will generate a new private key file `key.pem`\n\n```bash\nopenssl rsa -in PrivateKeyWithPassword.pem -out Private.key\n```\n```output\nEnter pass phrase for c:\\temp\\SSL\\key.pem: [Min 5 Char New Temp Password]\nwriting RSA key\n```\n\n## Export the public certificate from the PFX file\n\nThe final requirement in this typical configuration is to export the public certificate as an independent file from the PFX without the private key.\n\nSimilar to the previous steps, we will provide the name of the PFX and the new certificate file, for example `cert.pem`. Again as the PFX contains the private key, you need to provide the export password to allow this process to complete.\n\n```bash\nopenssl pkcs12 -in filename.pfx -clcerts -nokeys -out cert.pem\n```\n```output\nEnter Import Password: [Input the export password of the PFX File]\nMAC verified OK\n```\n\n### Exporting all cerificates in the chain\n\nWhen your PFX contains all the certificates in the chain, you can also export all these to the certificate file, `cert.pem` using the following command syntax. \n\n```bash\nopenssl.exe pkcs12 -in filename.pfx -out cert.pem -nodes\n```\n```output\nEnter Import Password: [Input the export password of the PFX File]\nMAC verified OK\n```",
    "lastmodified": "2022-12-28T23:25:23.421767106Z",
    "tags": null
  },
  "garden/content/sw-oxidized-deploy": {
    "title": "Change Detection using Oxidized",
    "content": "\n\n\nThe process is very simple:\n\n1. Login to each device in the router list `router.db`,\n2. Run Commands to get the information that will be saved\n3. Clean the output \n4.  Commit the Changes to GIT Repository\n\nThe tool is coded in *Ruby*, and implements a Domain Specific Language (DSL) for interaction.\n\nFinally, there is a Web based User experience included in the solution so we can get a fast overview of the world.\n\n## Docker Container\n\nAll of the configuration for my container is hosted at the file system location `/opt/appdata/oxidized`\n\nI will also select to execute the Web Interface for Oxidized using its default port with is `tcp:8888`\n\nUsing the follow command, we will grab the latest container version from Docker Hub, and call the container *oxidized* locally. Additionally, if the container should stop, I am providing the flag to instruct docker to always restart the service again.\n\n```bash\nsudo docker run --restart always -v /opt/appdata/oxidized:/root/.config/oxidized -p 8888:8888/tcp -t oxidized/oxidized:latest oxidized\n```\n\n## Configuration\n\nWe need a configuration file to guide Oxidized running process\n\n`vi config`\n\nThe following is the configuration sample that I am running with \n\n```yaml {linenos=table,hl_lines=[8,\"47-51\"]}\n---\nusername: admin\npassword: P@ssw0rd!\nmodel: junos\nresolve_dns: true\ninterval: 3600\nuse_syslog: false\ndebug: false\nthreads: 30\ntimeout: 20\nretries: 3\nprompt: !ruby/regexp /^([\\w.@-]+[#\u003e]\\s?)$/\nrest: 0.0.0.0:8888\nnext_adds_job: false\nvars: {}\ngroups: {}\nmodels: {}\npid: \"/root/.config/oxidized/pid\"\ncrash:\n  directory: \"/root/.config/oxidized/crashes\"\n  hostnames: false\nstats:\n  history_size: 10\ninput:\n  default: ssh, telnet\n  debug: false\n  ssh:\n    secure: false\n  ftp:\n    passive: true\n  utf8_encoded: true\noutput:\n  default: git\n  file:\n    directory: \"/root/.config/oxidized/configs\"\n  git:\n    single_repo: true\n    user: Oxidized\n    email: oxidized@email.target\n    repo: \"~/.config/oxidized/oxidized.git\"\nsource:\n  default: csv\n  csv:\n    file: ~/.config/oxidized/router.db\n    delimiter: !ruby/regexp /:/\n    map:\n      name: 0\n      ip: 1\n      model: 2\n      username: 3\n      password: 4\nmodel_map:\n  cisco: ios\n  juniper: junos\n  unifiap: airos\n  edgeos: edgeos\n```\n\n\n### Device list\n\nThe table based on the configuration we just defined, will be formatted as follows\n\n|Name | IP | Model | Username | Password|\n|---|---|---|---|---|\n|Device Name | 172.16.1.x | unifiap |sysadmin | P@ssw0rd!\n\nTo populate the table, we can open the editor `vi router.db`, and then inset the following sample entries\n\n```ini\nBedroom1_ap:172.16.1.114:unifiap:sysadmin:P@ssw0rd!\nKitchen_ap:172.16.1.121:unifiap:sysadmin:P@ssw0rd!\nCinema_ap:172.16.1.160:unifiap:sysadmin:P@ssw0rd!\nServerRoom_ap:172.16.1.115:unifiap:sysadmin:P@ssw0rd!\nFirewall:172.16.1.1:edgeos:ubnt:Sc0rp10n!\n```\n\nNow, we are ready, we have the configuration all set for this installation\n\n### Web Interface\n\nLaunching our browser to the oxidized site hosted on `TCP 8888` renders the current status\n\n![Oxidized Web UX](sw-oxidized-deploy/sw-oxidized-deploy-webux.png)\n\nFrom here we can see all the version changes for the devices configuration\n\n![Change History Log](sw-oxidized-deploy/sw-oxidized-deploy-history.png)\n\nAnd even select any one of these change sets, and view the changes which were applied to the configuration\n\n![Actual Changes](sw-oxidized-deploy/sw-oxidized-deploy-diff.png)\n\n\u003e **Closing Thoughts**\n\u003e\n\u003e Now, I wonder if we could integrate this with Azure?...",
    "lastmodified": "2022-12-28T23:25:23.43376777Z",
    "tags": null
  },
  "garden/content/sw-ssg-jekyll_with_azure_devops": {
    "title": "Constructing a new Home with Jekyll and Azure DevOps",
    "content": "\n\nOne of the unspoken truths behind the lack of posts on ths [blog](blog) in recent history was due to a few bugs, which in the end resulted in an experience where from home it appeared that any new content was published and working; but outside this fortress in the real world, there was a large silence echoing.\n\nI really only discovered this issue in May of this year (2018), and was, to say the least, a little agitated with the situation and decided then to change the approach to how I save my notes and share my thoughts.\n\n## [Jekyll](Jekyll)\n\nAfter a lot of hours hacking at CSS and JS, neither of which are my strongest points; combined with a whole lot of *liquid* scripting, which is based on the [Python](Python) Jinja library; I chose to leverage the open source Jekyll project.\n\nThis is not to say, that I might not reconsider this again as I am pretty intrigued also with [Hugo](Hugo); but one point is for sure... My days struggling with *Wordpress* are history. \n\nDon't get me wrong, Wordpress is great, even fantastic, but when it breaks, or its hacked (and boy have I been hacked), or when the comments system becomes a spam target; then its a total nightmare to have to deal with.\n\nI want something that is easy to use, a lot less prone to hacking, and painless to host; so my choice was clear from the start - I was going to use a Static Site Generator\n\n## Building\n\nLeveraging [GIT](GIT) for my version control, I have a simple pipeline which rebuilds a new version of the site each time a new commit is made to the repository. I do like to tweak and have actually no less than two approaches to the effort\n\n```mermaid\ngraph LR\n    A(Blog Repository)\n    B(Build Pipeline)\n    C(Docker Based Build)\n    D(Native Build)\n    E(Publish Built Site)\n\n    A -.-\u003e|Git Push Trigger| B\n    B --\u003e C\n    B --\u003e D\n    C --\u003e E\n    D --\u003e E\n```\n\nAs I spend the majority of my time focused on Microsoft Technology stack, I am leveraging [Azure DevOps](Azure DevOps) to run my build process; however, if you prefer other tools, for example, Jenkins, CircleCI, etc; then the concepts should be easily transportable, as there is nothing truly complex happening at this point.\n\n### Docker Build Pipeline\n\nThis version of the pipeline is my favourite, as I can use the same commands on my workstation to run a local web server to watch in realtime what my edits are going to look like when I finally commit, with 100% confidence that there will be no drift, as I use the exact same container for both roles, development and deployment\n\nThe pipeline I am sharing is in YAML format, which we are going to see a whole lot most of over time, and by sharing this you can easily recreate your own build pipeline with nothing more than a good paste!\n\nThe build is running on a hosted Ubuntu 16.04 instance, but this could be easily replaced with a dedicated build node; however for the amount of time I will use for the building, I should fall well inside the free monthly allocation offered in Azure DevOps; so, for now, this is perfect.\n\nThe pipeline has only 3 steps\n\n```mermaid\ngraph TD\n    A[Retrieve the relevant commit from Git Repo]\n    B(Run Docker Image to Build Site)\n    C(Move Generated HTML Site to Staging Area)\n    D(Publish Built Site)\n\n    A -.-\u003e B\n    B --\u003e C\n    C --\u003e D\n```\n\nThe *YAML* representation of the flow is as follows; you can also choose to add the steps in the UX and provide the data below into the relevant fields, as there is a 1:1 relationship between the UX and the YAML Infrastructure as Code\n\n```yaml\nresources:\n- repo: self\nqueue:\n  name: Hosted Ubuntu 1604\nsteps:\n- task: Docker@1\n  displayName: 'Run an image'\n  inputs:\n    containerregistrytype: 'Container Registry'\n\n    command: 'Run an image'\n\n    imageName: 'jekyll/builder:latest'\n\n    qualifyImageName: false\n\n    volumes: |\n     $(Build.SourcesDirectory):/srv/jekyll\n     $(Build.BinariesDirectory):/srv/jekyll/_site\n     \n\n    workingDirectory: '$(Build.SourcesDirectory):/srv/jekyll'\n\n    containerCommand: 'jekyll build --future'\n\n    runInBackground: false\n\n- task: CopyFiles@2\n  displayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'\n  inputs:\n    SourceFolder: '$(Build.BinariesDirectory)'\n\n    TargetFolder: '$(Build.ArtifactStagingDirectory)'\n\n\n- task: PublishBuildArtifacts@1\n  displayName: 'Publish Artifact: _site'\n  inputs:\n    ArtifactName: '_site'\n```\n\n### Native Build Pipeline\n\nThe Native approach does not offer a whole lot of immediate advantages over the docker version of the pipeline; I honestly created this to prove to myself that I could. \n\nHowever, after creating this, I do see an advantage. If I should choose to create a dedicated Build Server; I would be able to have the *Ruby bundler* and all the *Jekyll gems* pre-staged on the node; which would remove almost 3 minutes from the build pipeline, as these steps would not need to be repeated every time I executed a new build.\n\nNow, I would have expected the Docker approach to have this as an advantage with the Gems pre-installed in the container, but that's not the case with the official container I have used in the other pipeline; As a result, both pipelines take 3.5 minutes to prepare, build and publish my site artefacts currently.  Clearly, I have a lot of room to make this better.\n\nThis pipeline is a little more verbose with 5 steps currently\n\n\nThe pipeline has only 3 steps\n\n```mermaid\ngraph TD\n    A[Retrieve the relevant commit from Git Repo]\n    B(Use a Current Release of Ruby)\n    C(Install the Ruby Bundler toolchain)\n    D(Use Bundler to install the Jekyll dependencies)\n    E(Build the Jekyll site to the Staging Area)\n    F[Publish Built Site]\n\n    A -.-\u003e B\n    B --\u003e C\n    C --\u003e D\n    D --\u003e E\n    E --\u003e F\n```\n\nThe *YAML* representation of the flow is very similar to the previous sample, this time however you are going to really just been looking at some shell commands, which run essentially on any platform we can host ruby on.\n\n\n```yaml\nresources:\n- repo: self\nqueue:\n  name: Hosted Ubuntu 1604\nsteps:\n\n- task: UseRubyVersion@0\n  displayName: 'Use Ruby \u003e= 2.4'\n\n\n- script: 'gem install bundler' \n  displayName: 'Install bundler'\n\n- script: 'bundle install' \n  displayName: 'Install Jekyll and Dependencies'\n\n- script: 'bundle exec jekyll build -d $(Build.ArtifactStagingDirectory)' \n  displayName: 'Build Jekyll Static Site'\n\n- task: PublishBuildArtifacts@1\n  displayName: 'Publish Artifact: _site'\n  inputs:\n    ArtifactName: '_site'\n```\n\n## Next Steps\n\nWith a built site, published; what we really have is a *.ZIP* file which contains all the generated HTML which we can drop onto our web server to publish to the world. \n\nThere are many choices on the web hosting platform to use, keep tuned, and I will share with you the solution I have elected to use for this site",
    "lastmodified": "2022-12-28T23:25:23.421767106Z",
    "tags": null
  },
  "garden/content/sw-windows-exporting_certificate_as_pfx": {
    "title": "Creating a PFX from a certificate in Windows",
    "content": "\n\nA `.pfx` file is in essence an archive which can contain multiple objects, and can also be password protected; The format of this file is known as *PKCS#12*\n\nTypically, a `.pfx` usually contains one or more certificate, typically the chain of upstream authorities, and the corresponding private key. The most common usage of a PFX file is simplify certificate distribution to alternate systems or deployed to services.\n\n## Logical stores\n\nWithin Windows, all certificates exist in logical storage locations referred to as **certificate stores**.\n\nLogical stores are virtual locations that map to certificate physical paths. Powershell uses the **Cert** PSDrive to map certificates to the physical stores.\n\nJust to keep things nice and simple, Microsoft Style *(yes I am being sarcastic)* The **Certificates** Microsoft Management Console (MMC) logical store labeling is different from the **Cert** PSDrive store labeling. The table below shows the comparison between both:\n\n|PS CERT: Drive|\tCertificates MMC|\n|---|---|\n|My\t              |Personal\n|Remote Desktop\t  |Remote Desktop\n|Root\t            |Trusted Root Certification Authorities\n|CA\t              |Intermediate Certification Authorities\n|AuthRoot\t        |Third-Party Root Certification Authorities\n|TrustedPublisher\t|Trusted Publishers\n|Trust\t          |Enterprise Trust\n|UserDS\t          |Active Directory User Object\n\n\n## Exporting the Certificates\n\nThis article will guide the relatively simple steps required to extract a certificate from the logical store, and create a PFX that we can use to transport the certificate to other systems. PFX support the containment of Private Keys, and should typically be password protected to assist in securing the certificate\n\n### Windows Certificates MMC\n\nUsing the *Certificates* Snap-In the following steps will guide to exporting the certificate to a PFX file, which we can later process with the OpenSSL tools.\n\n* Start the Microsoft Management Console \u003e Run `mmc.exe`\n* Click the **Console** menu and then click **Add/Remove Snap-in**.\n\t* Click the **Add** button and then choose the **certificates** snap-in and click on **Add**.\n\t* Select **Certificates** and click **Add**.\n\t* Select **Computer Account** then click **Next**.\n\t* Select **Local Computer** and then click **OK**.\n\t* Click **Close** and then click **OK**.\n\nWith the Snap in now loaded and focused on the *Local Computer Certificate Store*, we can locate a target *Certificate* to export.\n\n* Expand the menu for **Certificates** and click on the **Personal** folder.\n* Right click on the certificate that you want to export and select **All tasks** \u003e **Export**.\n* A wizard will appear - **Export Certificate**\n  * Make sure you **check** the box to include the **private key**\n  * You will be asked to password protect the PFX for importing\n  * Continue through with this wizard until you have a `.PFX` file.\n\nRather simple right. You now have a `.pfx` file, which is password protected, containing your Certificate and its private key\n\n### Powershell\n\nThe Powershell `PKI` module can be used to export or import PFX files. The following are some examples of the process\n\n#### Export-PfxCertificate\n\nThe `Export-PfxCertificate` cmdlet exports a certificate or to a Personal Information Exchange `.pfx` file. By default, extended properties and the entire chain are exported.\n\nTo export a `.pfx` certificate a password is needed for encrypting the private key.\n\nIn the example below, we use the **Subject** property to find the certificate to be exported by selecting the certificate whose Subject value equals *myCertificate*\n\n```powershell\n$certificate = Get-ChildItem -Path Cert:\\LocalMachine\\My\\ | Where-Object {$_.Subject -match \"myCertificate\"}\n```\n\nAfter selecting the certificate it can be exported from its store location to a folder with the command below:\n\n```powershell\n$password= \"P@ssw0rd\" | ConvertTo-SecureString -AsPlainText -Force\nExport-PfxCertificate -Cert $certificate -FilePath $env:USERPROFILE\\Documents\\myCertificate.pfx -Password $password\n```\n\n#### Importing a Certificate\n\nThe Powershell Cmdlet `Import-PfxCertificate` is used to install a `.pfx` certificate.\n\nTo install a PFX certificate to the current user's personal store, use the command below:\n\n```powershell\nImport-PfxCertificate -FilePath ./PFXCertificate.pfx -CertStoreLocation Cert:\\CurrentUser\\My -Password P@ssw0rd \n```\n\nTo install into the system personal location change the store location in the command above from `Cert:\\CurrentUser\\My` to `Cert:\\LocalMachine\\My`.",
    "lastmodified": "2022-12-28T23:25:23.441768212Z",
    "tags": null
  }
}