{
  "/posts/2018-10-20-static-foundations-in-azure": {
    "title": "Laying Foundations in Azure",
    "content": "\nHosting my site on Wordpress was not super complex; I leveraged the Azure PaaS Services for Web Apps, and orginally the 3rd party support for hosted MySQL database's. Once I was up and running I quickly realised that all media hosted on the site were landing on the webserver, so a plugin from its marketplace offered the ability to relocate the media to an Azure Blob; offloading some of the challanges.\n\nHosting this was not free, while I could have leveraged the Free Webserver option it did not take a lot of load for this to be causing some unacceptable performace issues; which when combined with a hosted MySQL service which also was not going to be super fast and was limited in the database size; which became event more of a problem when an attach on the site would result in 1000's of useless comments filling the database to capacity and taking the site down as a direct result.\n\n## Static Site Hosting\n\nNot a lot has to change as we move to the static side model; The web server is still needed to host the side; but the database component is no longer a concern with this approach.\n\nHowever, The cloud is a beautiful thing, and there are so many more ways to reach you goal, and while we are focused, we can complete a lot more for a lot less!\n\nFor this site I have chosen to run a lean cost model, while providing a super responsive experience to you, my readers and subscribers.\n\n### Blob Storage Foundations\n\nUsing the standard **Azure Blob Storage** offering, I simply copy over the generated HTML site to the account. Assuming the blob is exposed to the public its content is available as a HTTPS endpoint; which simply put is a website.\n\nBut, alone this is not enough; why? because how would I address requested for pages which do not exist for example; I would not want an ugly HTTP 404 error page to be rendered, but instead a nice response to offer a search, navigation, or other more professional experience.\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"#{GITVERSION_FullSemVer}#\",\n  \"parameters\": {\n    \"name\": {\n      \"type\": \"String\"\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"[resourceGroup().location]\"\n    },\n    \"contactEmail\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"info@damianflynn.com\"\n    },\n    \"projectName\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"Static Site Hosting\"\n    },\n    \"environment\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Production\",\n        \"Test\",\n        \"Developer\",\n        \"Proof of concept\"\n      ],\n      \"defaultValue\": \"Developer\",\n      \"metadata\": {\n        \"description\": \"Type of environment\"\n      }\n    }\n  },\n\n  \"variables\": {\n    \"storageName\": \"[tolower(take(concat('blob',parameters('name'),uniqueString(resourceGroup().id)),23))]\"\n  },\n\n  \"resources\": [\n\n    {\n      \"comments\": \"Storage Account for Static Site Content\",\n      \"name\": \"[variables('storageName')]\",\n      \"apiVersion\": \"2018-07-01\",\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"sku\": {\n        \"name\": \"Standard_LRS\",\n        \"tier\": \"Standard\"\n      },\n      \"kind\": \"StorageV2\",\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"properties\": {\n        \"networkAcls\": {\n          \"bypass\": \"AzureServices\",\n          \"virtualNetworkRules\": [],\n          \"ipRules\": [],\n          \"defaultAction\": \"Allow\"\n        },\n        \"supportsHttpsTrafficOnly\": true,\n        \"encryption\": {\n          \"services\": {\n            \"file\": {\n              \"enabled\": true\n            },\n            \"blob\": {\n              \"enabled\": true\n            }\n          },\n          \"keySource\": \"Microsoft.Storage\"\n        },\n        \"accessTier\": \"Hot\"\n      },\n      \"dependsOn\": []\n    }\n\n  ]\n}\n```\n\nTo achieve this I need some HTTP routing options\n\n### House Warming, Inviting Guests\n\nIt is not hard to find a list of potential solutions in Azure to fill this role; We could use a WebApp, API Managment, or event Azure Functions, or the Azure Functions proxy features. However, we do not actually need to be very creative; as Microsoft have listened to uservoice, MVPs and customers all calling for some better web publishing support for blob storage, especially given AWS have features in thier S3 offer that work simply and effectively.\n\nTo this end we will take a look at a feature which at the time of writing is still in preview called **Static Website** (You would have never guessed right!)\n\nAll we need do, is set this feature as *Enabled* and define where visitors should be routed to for the *Index Document* and the *Error Document*, and apply the changes. This simple feature makes our site behave just as we would like; and it is *free!*.\n\n\u003e Note: Due to Preview Status, we currently have no ARM Template settings for enabling and configuring this setting. Therefore for *Infrasructure as Code* we will fall back to *Azure CLI*\n\n```bash\naz extension add --name storage-preview\n\naz storage blob service-properties update '\n   --account-name \u003cACCOUNT_NAME\u003e\n   --static-website \n   --404-document \u003cERROR_DOCUMENT_NAME\u003e \n   --index-document \u003cINDEX_DOCUMENT_NAME\u003e\n\naz storage account show -n \u003cACCOUNT_NAME\u003e -g \u003cRESOURCE_GROUP\u003e --query \"primaryEndpoints.web\" --output tsv\n```\n\n## Foundation for Global Scale\n\nBut why stop here, we can go a little further; I want this site to be responsive for you, regardless of where you are located on the planet; afer all we are all embracing the cloud and need to learn and share; so how better to acomplish this, simply leverage another feature from the Azure arsnal of services.\n\nCalling out the *Azure Content Delivery Network* (Azure CDN) we can take advantage, or any one of three platforms \n\n* Verizon's Global Network\n* Akamai CDN Platform\n* Microsofts Azure's Footprint\n\nThe choice you make here really will come down to what you want in the list of features and the budget you have in mind; but for the purpose of this blog; I have for now chosen to run with the option of **Premium Verizon**!\n\nWhy? Well It is the most feature rich of the offerings currently, and also the most expensive; which still should not cost me more then 3 or 4 euro a month; which is a lot less than I was paying for my Wordpress site. Give me a month or two and I will share thee exact costs of this solution so you can appreciate the value offered by Azure PaaS offerings.\n\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"#{GITVERSION_FullSemVer}#\",\n  \"parameters\": {\n    \"name\": {\n      \"type\": \"String\"\n    },\n    \"cdnEndpointTargetUrl\": {\n      \"type\": \"String\"\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"[resourceGroup().location]\"\n    },\n    \"contactEmail\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"info@damianflynn.com\"\n    },\n    \"projectName\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"Static Site Hosting\"\n    },\n    \"environment\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Production\",\n        \"Test\",\n        \"Developer\",\n        \"Proof of concept\"\n      ],\n      \"defaultValue\": \"Developer\",\n      \"metadata\": {\n        \"description\": \"Type of environment\"\n      }\n    }\n  },\n\n  \"variables\": {\n    \"cdnName\": \"[concat(parameters('name'),'-cdn-',parameters('environment'))]\",\n    \"cdnEndpoint\": \"[concat(parameters('name'),'-cde-',parameters('environment'))]\",\n    \"cdnEndpointName\": \"[concat(variables('cdnName'), '/',   variables('cdnEndpoint'))]\",\n    \"cdnEndpointOriginName\": \"[concat(variables('cdnEndPointName'), '/','Origin')]\"\n  },\n\n  \"resources\": [\n    \n    {\n      \"comments\": \"Azure CDN Service.\",\n      \"name\": \"[variables('cdnName')]\",\n      \"apiVersion\": \"2016-04-02\",\n      \"type\": \"Microsoft.Cdn/profiles\",\n      \"sku\": {\n        \"name\": \"Premium_Verizon\"\n      },\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"dependsOn\": []\n    },\n\n    {\n      \"comments\": \"Azure CDN Endpoint\",\n      \"name\": \"[variables('cdnEndpointName')]\",\n      \"apiVersion\": \"2016-04-02\",\n      \"type\": \"Microsoft.Cdn/profiles/endpoints\",\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"properties\": {\n        \"originHostHeader\": \"[parameters('cdnEndpointTargetURL')]\",\n        \"isHttpAllowed\": true,\n        \"isHttpsAllowed\": true,\n        \"queryStringCachingBehavior\": \"NotSet\",\n        \"originPath\": null,\n        \"origins\": [\n          {\n            \"name\": \"[variables('cdnEndpoint')]\",\n            \"properties\": {\n              \"hostName\": \"[parameters('cdnEndpointTargetURL')]\",\n              \"httpPort\": 80,\n              \"httpsPort\": 443\n            }\n          }\n        ],\n        \"contentTypesToCompress\": [],\n        \"isCompressionEnabled\": false\n      },\n      \"dependsOn\": [\n        \"[resourceId('Microsoft.Cdn/profiles', variables('cdnName'))]\"\n      ]\n    }\n\n  ]\n}\n```\n\nThe template will setup out CDN environment, all we need to provide is a name for the CDN service, and the URI to the Static Site which we established in the previous stage, for example this may appear as `blobwebaddress.z00.web.core.windows.net` \n\n## Adding Content\n\nWith very little effort, and cost, we now have established a foundation which can serve content efficiently globally, Our next challage will be to deploy our site to this foundation in a easy manner",
    "lastmodified": "2023-04-13T22:40:03.136696404Z",
    "tags": [
      "ARM",
      "Azure CDN",
      "Azure Storage",
      "Azure Static Website"
    ]
  },
  "/posts/2018-10-21-Original-Content-archive": {
    "title": "Original Content Archive",
    "content": "\nA quick snoop around this new site will expose a little hole which I am working on addressing in batches.\n\nAll the original content and images which were hosted on the previous incarnation of this site have been converted to markdown ready to republish. \n\nHowever, Prior to this happening, I have decided to update the taxonomy of the site, to recategorise and retag all the original content to fall into a more logical structure.\n\nExpect to see all the original content reposted over before the end of the year, and if there is specific content you would need sooner, ping me on twitter!",
    "lastmodified": "2023-04-13T22:40:03.124696259Z",
    "tags": [
      "General"
    ]
  },
  "/posts/2018-10-24-adding-content-flow": {
    "title": "Adding Content Flow",
    "content": "\nWith the heavy lifting done in creating the site building mechanics and a solid foundation to build and share upon; our final objective is to automate the process of connecting these two stages.\n\n## Release Pipeline\n\nTechnically the goal we are speaking about is the **Release Pipeline** which will take the artefact *(our site .ZIP file)* that we created in the **Build Pipeline** in our previous topic [Constructing a new Home with Jekyll and Azure DevOps](/Building-The-Site/); and publish this to our storage account.\n\nIn simple terms, we are going to Unzip the archive to the storage, which is configured to expose the content as a website, and to increase performance we are leveraging a content delivery network.\n\n### Copying Content\n\nAs with our *Build Pipeline* there are a few different options on how to accomplish the work. \n\n\u003e Currently Azure DevOps is not exposing the release pipelines in an *Infrastructure as Code* configuration so we will complete this in the portal.\n\nWe begin with a new *Release Pipeline*, add add the task **Azure File Copy**, and set the settings similar to the following:\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Azure Blob File Copy\n|Source                | $(System.DefaultWorkingDirectory)/_Jekyll Builder/_site\n|Azure Connection Type | Azure Resource Manager\n|Azure Subscription    | My Azure Subscription\n|Destination Type      | Azure Blob\n|Storage Account       | Name of the Storage Account you created, e.g. My Blog\n|Container Name        | $web\n\n\nDone!, Seriously; pretty easy right!\n\n### Replace Content\n\nOne small issue with the previous approach is that it merely overwrites the content in the blob with the latest version, but it failed to remove any content which might have been removed from the site.\n\nCurrently, there is no equivalent to the magic *XCOPY* command that will sync the blob removing the data that is no longer required; that's a project for another day.\n\nThe quick and dirty fix for this scenario is first to delete the existing content and then deploy the latest version. This would typically result in a potential outage as the content vanishes for a little time; however, we do not have that concern, because we have decided to use a Content Delivery Network which caches our site.\n\n#### Task 1\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Delete Old Files\n|Azure Subscription    | My Azure Subscription\n|Script Location       | Inline\u003cbr\u003e`az storage blob delete-batch --source $web --account-name $(storageAccount) --output table`\n\n#### Task 2\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Upload Fresh Content\n|Azure Subscription    | My Azure Subscription\n|Script Location       | Inline\u003cbr\u003e`az storage blob upload-batch --source _site --destination $web --account-name $(storageAccount) --output table --no-progress`\n\n### Tiggers Ready\n\nThe last point to consider is *When* should this deployment happen, and that's also pretty obvious when we think about it.\n\nEvery time we have a good build of the site, we should check to see which branch just completed the process, and update the relevant site based on this information.\n\n![Release Pipeline Flow](2018-10-24-adding-content-flow/opps-missing-image.png)\n\n## Summary\n\nWhich flow you choose to implement are entirely at your discretion; As I am currently doing much work on the taxonomy of the site; I have implemented the second option of delete and redeploy; but I do in the plan to come back and optimize this by adding a sync process which will be far more efficient.",
    "lastmodified": "2023-04-13T22:40:03.136696404Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2018-10-30-static-comments": {
    "title": "Static Comments",
    "content": "\nAt this point we are almost ready to go live with our site, however, one of the cornerstones to growing and sharing is communication. \n\n## Wordpress\n\nIn the world of Wordpress, this was a standard core feature, which leveraged the fact that the pages were rendered on demand from a backend database. In this scenario, the same approach is offered to maintain a commenting platform. \n\nHowever, as I noted earlier; given that Wordpress powers a very large portion of the blogging surface of the internet; it is an obvious target for hacking, just refer to the CVS database for a glimpse of what this looks like in reality. \n\nI have had more than over of these exceptions result in defacement or excessive spam in the comment system. The real objective, however, is to bloat the database which will then result in the site going offline as the database reaches its maximum limit based on your host or plan. \n\nRecovering from this mess is slow and painful, and you must also not ignore the fact that you now should also update the runtime; a process we try to ignore as this exercise generally results in breaking extensions and taking the site offline for a little time. \n\n## Static Sites\n\nSo, if we do not have the luxury of a database to host our comments in the static site configuration (recall all we have is HTML and client-side JavaScript); how on earth do we implement this critical feature. \n\nOf course, we can use the cloud! There are many SaaS offerings which are designed to integrate into our site but offload all the storage and processing to the service\n\nAdditionally, many of these are free to use, if you agree to let the service display a couple of advertisements. Previous I have used services from Disqus on my site to offload the challenges of hosting and keeping updated my own. \n\n### Disqus Out!\n\nAs I coded the liquid for this side I also implemented Disqus as the commentary service. However, immediately after turning this on for my posts the page load time was almost 3 times slower!  \n\nAdding insult to injury the adverts have evolved to be click bate and not relevant to my content what so ever. \n\nDisqus does offer a *Not Free* option which addressed the Advertising a bit better, but that does not explain why the massive performance hit?\n\nTracing my site loading time with Chromes F12 development tools expose the shocking truth. \n\n\u003e Adding Disqus to the site results in over **50** treads to tracking and other undesirable sites \n\nTherefore I immediately deleted the liquid code and stopped any further integration of this service. It’s gone and good ridden\n\n### Utterance.es\n\nWatching how Microsoft recently replaced their commentary service on the *docs.microsoft.com* sites to leverage GitHub, I decided that this might be a really good solution for this site also.\n\nAfter a little research, I found a lovely match called **utteranc.es** which requires that you log in with your Github account, and will create a new issue per post in my site, that can be tracked and managed as normal issues within github which is pretty awesome. \n\n(I assume based on my content and audience that this should not be a problem - let me know on Twitter if I am wrong about this)\n\n#### Implementing Utteranc.es\n\nAdding this feature is trivial, Really trivial!\n\n1. We require a public Github Repo\n2. Authorise the [Utteranc.es Bot][https://github.com/apps/utterances] access to the selected Repo\n3. Add the following javascript code to our page, updating the paramater `repo=\"[ENTER REPO HERE]\"` to match the Repo name; for example `repo=\"[damianflynn/damianflynn.github.io\"`\n\n```javascript\n\u003cscript src=\"https://utteranc.es/client.js\"\n        repo=\"[ENTER REPO HERE]\"\n        issue-term=\"pathname\"\n        theme=\"github-light\"\n        crossorigin=\"anonymous\"\n        async\u003e\n\u003c/script\u003e\n```\n\nI have added a little extra logic to determine which pages to offer comments; for example; I do not need this feature on the main landing page.\n\n## Summary\n\nNow, I really want you to tell me what your thoughts about this for a solution? \n\nGo On, Leave a comment, even if its just a thumbs up or down!",
    "lastmodified": "2023-04-13T22:40:03.136696404Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2018-11-07-book-wavemaker": {
    "title": "Untitled Page",
    "content": "---\ntitle: Reading material: WaveMaker\ntype: article \nlayout: post \ndescription: Careers are what we all invest our energy and emotions in, either positively or negatively\ndate: 2018-11-07 10:30:30\ncategories: ['Career', 'Books']\ntags: ['General']\nauthors: ['damian'] \ndraft: false \nimage: /images/2018/11/07/banner.jpg\ntoc: false \nfeatured: false \ncomments: True\n---\n\nCareers are what we all invest our energy and emotions in, either positively or negatively. Positive being the belief that we can make significant progress in this or another organisation and that it will give us the wherewithal to have a happy and productive life. Negative in that we can feel that everyone is out to get us, and the slippery pole has been freshly greased to scupper us. That it’s a “not what you know but who you know” world, and, unfortunately, you don’t know anyone. That life’s a bitch and then you die!\n\n\u003ca href=\"https://www.amazon.co.uk/WaveMaker-career-secrets-outstanding-performers/dp/1724091999/ref=as_li_ss_il?ie=UTF8\u0026qid=1541630591\u0026sr=8-1\u0026keywords=wavemaker+robin+farmer\u0026linkCode=li3\u0026tag=damiflyn-21\u0026linkId=c4bc82abd3c35cd6379fdee7313bf426\u0026language=en_GB\" target=\"_blank\"\u003e\u003cimg border=\"0\" src=\"//ws-eu.amazon-adsystem.com/widgets/q?_encoding=UTF8\u0026ASIN=1724091999\u0026Format=_SL250_\u0026ID=AsinImage\u0026MarketPlace=GB\u0026ServiceVersion=20070822\u0026WS=1\u0026tag=damiflyn-21\u0026language=en_GB\" \u003e\u003c/a\u003e\u003cimg src=\"https://ir-uk.amazon-adsystem.com/e/ir?t=damiflyn-21\u0026language=en_GB\u0026l=li3\u0026o=2\u0026a=1724091999\" width=\"1\" height=\"1\" border=\"0\" alt=\"\" style=\"border:none !important; margin:0px !important;\" /\u003e",
    "lastmodified": "2023-04-13T22:40:03.132696355Z",
    "tags": []
  },
  "/posts/2018-11-18-NodeMCU": {
    "title": "NodeMCU Pinouts",
    "content": "\nOver the last number of years I have deployed various Raspberry PI's around my home, to add more features to how we interact in the house. However, as wonderful as the PI is, there are alternative options available which are more appropriate to the various tasks which I need to address.\n\n## NodeMCU\n\nFor the past year, I have been using a tiny board, known as the **NodeMCU** which is essentially a developer board for a module know as the **ESP8266**. The **NodeMCU** is formed by an **ESP12E**, which still has an *ESP8266EX* inside it.\n\n![NodeMCU ESP12 Exploded View](2018-11-18-NodeMCU/opps-missing-image.png)\n\nThis device is really nice to work with, it is supplied preconfigured with a Micro USB input, for both programming and power.\n\n\u003e The term NodeMCU usually refers to the firmware, while the board is called Devkit. NodeMCU Devkit 1.0 consists of an ESP-12E on a board, along with a voltage regulator, a USB interface.\n\n### ESP-12E\n\nThe board created by AI-THINKER, which consists of an *Espressif ESP8266EX* inside the metal cover. The microchip has a low-power consumption profile, integrated WiFi and the *RISC Tensilica L106 32bit Processor* has a maximum clock of 160 MHz\n\n![ESP-12](2018-11-18-NodeMCU/opps-missing-image.png)\n\nThe following illustrates the pinouts on this board\n\n![ESP-12E_Pinout](2018-11-18-NodeMCU/opps-missing-image.png)\n\n\n## IDE \n\nI have so far being developing on this board using VS Code, and its integrations with the Arduino IDE. While this works well, I am currently considering alternative approach's; as the debugging experience is far from optimal in my opinion; but that is work for another day.\n\nThe power of this board however, is understanding how to connect with outside world, and in this case what are the correct pin identifiers, trough reference of the NodeMCU datasheet and how the board boot's.\n\n### NodeMCU Pinout\n\nThe Devkit board which we are leveraging maps the pinouts from the ESP-12E module as follows: \n\n![NodeMCU_ESP12E_Pinout](2018-11-18-NodeMCU/opps-missing-image.png)\n\nTranslating this however to the code we are going to develop in the Arduino sketch's; we need to reference the pins with thier respective names; which is illustrated in this following image\n\n![NodeMCU_ESP12E_Arduino](2018-11-18-NodeMCU/opps-missing-image.png)\n\nUse the number that is in front of the GPIO or the constants as follows\n\n|Constant | IO\n|---|---|\n|D0\u003cbr\u003eLED_BUILTIN | GPIO16\n|D1 | GPIO5\n|D2 | GPIO4\n|D3 | GPIO0\n|D4 | GPIO2\n|D5 | GPIO14\n|D6 | GPIO12\n|D7 | GPIO13\n|D8 | GPIO15\n|D9 | GPIO3\n|D10 | GPIO1\n|A0 | ADC\n\n\n### Pin IO Functions\n\nWhen performing INPUT and OUTPUT tests on the pins, we obtained the following results:\n\n|Function   | GPIO\n|digitalWrite| Working: 0, 1, 2, 3, 4, 5, 9, 10, 12, 13, 14, 15\u003cbr\u003eNot Working: 6, 7, 8, 11, ADC\n|digitalRead | Working: 0, 2, 4, 5, 9, 10, 12, 13, 14, 15\u003cbr\u003eNot Working: 1, 3, 6, 7, 8, 11, ADC\n|analogWrite | Software PWM: 0, 1, 2, 3, 5, 9, 10, 13\u003cbr\u003eHardware PWM: 4, 12, 14, 15\u003cbr\u003eNot Working: 6, 7, 8, 11, ADC\u003cbr\u003e\n|analogRead  | Working: ADC\n\n\n## Flashing Sketch\n\nThe following simple sketch should flash an LED connected directly to the NodeMCU\n\n```arduino\n//Connect a testing LED to GPIO14 which is pin D5\n#define LED D5\n \nvoid setup() {\n  pinMode(LED, OUTPUT);\n}\n \nvoid loop() {\n  digitalWrite(LED, HIGH);    // Turn on the LED\n  delay(1000);                // Wait 1 Second\n  digitalWrite(LED, LOW);     // Turn off the LED\n  delay(1000);                // Wait 1 Second\n}\n```\n\nUsing a very simple hook up example.\n\n![LED Connection](2018-11-18-NodeMCU/opps-missing-image.png)\n\nAlternatively, without any external LED, we can set the LED to use the onboard LED with the mapping to `D0` or the constant `LED_BUILTIN`\n\n\u003e Important: Please note that there are lots of generic ESP8266 boards and there is the possibility that some of them are sold under the name of NodeMCU and have different pin mappings. Besides that, there are different NodeMCU versions.",
    "lastmodified": "2023-04-13T22:40:03.128696307Z",
    "tags": [
      "Internet of Things"
    ]
  },
  "/posts/2018-11-19-Defining-Azure-Policy-as-code": {
    "title": "Defining Policy as Code with ARM Templates",
    "content": "\nMy colleagues and friends [Tao Yang](https://blog.tyang.org/2018/06/06/using-arm-templates-to-deploying-azure-policy-definitions-that-requires-input-parameters/) , and [Stanislav Zhelyazkov](https://cloudadministrator.net/2018/07/17/defining-input-parameters-for-policy-definitions-in-arm-template/) have both recently posts interesting topics on how to implement your Azure Policy as Code which I strongly recommend you take a few moments to review \n* [Using ARM Templates to deploy azure policy definitions that require input parameters](https://blog.tyang.org/2018/06/06/using-arm-templates-to-deploying-azure-policy-definitions-that-requires-input-parameters/)\n* [Defining input parameters for policy definitions in ARM Templates](https://cloudadministrator.net/2018/07/17/defining-input-parameters-for-policy-definitions-in-arm-template/)\n\n## Improving Readability\n\nBoth of these topics address the core of the challenges we face when approaching policy as an Infrastructure as Code problem. However, one of the things that is lost in the translation is the readability of the templates which they are  deploying. \n\nNot to reinvent the wheel, I am going to use the same template which Stan presented in his post, and make a small tweak to the process which he has employed to deal with the `'` *single quote* problem!\n\nARM follows most of the standard JSON escape sequences, therefore the following examples are quite useful\n\n### Escaping a Single quote\n\nAzure ARM behaves nicely with a simply doubling the single quote characters; just as we apply in Visual Basic.\n\n`[concat('This is a ''quoted'' word.')]` which then provides the output of `This is a 'quoted' word.`\n\n### Escaping a Double quote\n\nFor the Double quotes, we use the normal escape character `/`. \n\n`[concat('''single'' and \\\"double\\\" quotes.')]` will render the output as follows `'single' and \"double\" quotes.`\n\n## The Solution\n\nWith this simple trick, we can replace the *variables* definition which the guys used with the following snippet:\n\n```json\n  \"variables\": {\n      \"filterVNetId\": \"[ concat( '[concat(parameters(''virtualNetworkId''),''*'')]' ) ]\"\n  }\n```\n\nI am sure that this is a lot easier to read, and therefor debug; So the full template would look as follows\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n      \"vNetId\": {\n          \"type\": \"string\",\n          \"defaultValue\": \"ScreenConnect\"\n      }\n  },\n  \"variables\": {\n      \"filterVNetId\": \"[ concat( '[concat(parameters(''virtualNetworkId''),''*'')]' ) ]\"\n  },\n  \"resources\": [\n      {\n          \"name\": \"vm-creation-in-approved-vnet-definition\",\n          \"type\": \"Microsoft.Authorization/policyDefinitions\",\n          \"apiVersion\": \"2018-03-01\",\n          \"properties\": {\n              \"displayName\" : \"Use approved vNet for VM network interfaces\",\n              \"policyType\": \"Custom\",\n              \"mode\": \"All\",\n              \"description\" : \"Use approved vNet for VM network interfaces\",\n              \"metadata\": {\n                  \"category\": \"IaaS\"\n              },\n              \"parameters\": {\n                  \"virtualNetworkId\": {\n                      \"type\": \"string\",\n                      \"metadata\": {\n                          \"description\": \"Resource Id for the vNet\",\n                          \"displayName\": \"vNet Id\"\n                      }\n                  }\n              },\n               \"policyRule\": {\n                  \"if\": {\n                      \"allOf\": [\n                          {\n                              \"field\": \"type\",\n                              \"equals\": \"Microsoft.Network/networkInterfaces\"\n                          },\n                          {\n                              \"not\": {\n                                  \"field\": \"Microsoft.Network/networkInterfaces/ipconfigurations[*].subnet.id\",\n                                  \"like\": \"[variables('filterVNetId')]\"\n                              }\n                          }\n                      ]\n                  },\n                  \"then\": {\n                      \"effect\": \"deny\"\n                  }\n              }\n          }\n      },\n      {\n          \"name\": \"vm-creation-in-approved-vnet-assignment\",\n          \"type\": \"Microsoft.Authorization/policyAssignments\",\n          \"apiVersion\": \"2018-03-01\",\n          \"dependsOn\": [\n              \"[resourceId('Microsoft.Authorization/policyDefinitions/', 'vm-creation-in-approved-vnet-definition')]\"\n          ],\n          \"properties\": {\n              \"displayName\" : \"Use approved vNet for VM network interfaces\",\n              \"description\" : \"Use approved vNet for VM network interfaces\",\n              \"metadata\" : {\n                  \"assignedBy\" : \"Admin\"\n              },\n              \"scope\": \"[subscription().id]\",\n              \"policyDefinitionId\": \"[resourceId('Microsoft.Authorization/policyDefinitions', 'vm-creation-in-approved-vnet-definition')]\",\n              \"parameters\" : {\n                  \"virtualNetworkId\" : {\n                      \"value\": \"[parameters('vNetId')]\"\n                  }\n              }\n          }\n      }\n  ],\n  \"outputs\": {\n  }\n}\n```\n\n## Summary\n\nUsing native escape sequences in ARM assist in the overall readability of the final code.\n\nThank you Tao and Stan for the inspiration",
    "lastmodified": "2023-04-13T22:40:03.132696355Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2018-12-04-IoT_Retrospective": {
    "title": "My Journey to the Internet of Things",
    "content": "\nIn a previous post, I referred to an embedded device which is called a **NodeMCU**. This device is a developer kit, designed to make it easy to develop and test programs for the embedded *ESP8266 System on a Chip*. Many manufacturers are offering both developer and production kits which leverage this SoC; including the Wemos D1, Lolin, and in my case the NodeMCU. \n\nThe function of these developer kits is to add some supporting features, for example, a USB to serial converter which makes programming a lot easier, in addition to a couple of buttons, and maybe an LED or two, for testing some simple scenarios with.\n\n\u003cdiv class=\"mermaid\"\u003e\ngraph LR\n    A[ESP8266]\n    B[ESP12]\n    C[NodeMCU]\n    C--\u003e|Hosts the\u003cbr\u003emodule| B\n    B--\u003e|Encapsulates\u003cbr\u003ethe SoC| A\n\u003c/div\u003e\n\nOnce your development efforts are at a stage ready for production, we can implement the *ESP12* module on custom circuit boards, designed for the specific scenarios you might be addressing. The cost of the models is meager and can be significantly reduced further based on the order size. For a tiny quantity of 10 unit's, we will have change out of €15, after Postage and Packing.\n\nThe developer board, is a little more expensive, for example you can grab these from many online suppliers including my link to Amazon, which offers two developer boards for €8, seriously not going to burst any banks here.\n\n## Some History\n\nSo why am I playing with these devices? I have been working on embedded technologies for over 30 years; starting with my first industrial deploying in a rubber injection moulding company, using a board from an American company called Tern, Inc; developing the code in *C*, and running my own communications protocol across an RS485 network! This was way back in 1994! *Now, I am showing my age!* \n\nThe scenario at the time was to monitor the state of the Moulding press, ensure the safety guards were down while injection was happening, reporting temperature which has been managed by a Siemens PLC, and then using a digital scale, counting the number of rubber parts which were just created and placed in a plastic box.\n\n30 years later, not a lot has changed, we still use conveyor belts, PLCs, digital scales, thermocouples, and so on; what has changed however...\n\n1. The computers have shrunk considerably\n2. Protocols are now published and supported with free open-source libraries\n3. Projects are shared so development can be accelerated, and new patterns and practices discovered and learnt\n4. Costs have decreased, while reliability has increased.\n5. Data is preferred to be stored in large pool's; exposing some amazing value attributes.\n\n### Learning\n\nMy original deployment failed after a few weeks in live production, for reasons which are now very obvious, but at the time were not so transparent. Considering the environment, all the heavy 3-Phase machinery introduced a lot of noise, Not noise as in you should wear ear protection, but electrical noise, Spikes, and drops, which on a network of 2 twisted wires with no error handling; and a rudimentary communications protocol with nothing more than a CRC code for data blocks, resulted in some false data getting stored, or worse lost.\n\nThe fix at the time was costly. Firstly, the network required to be insulated; trough both software (firmware) updates to harden the protocol, and better error handling; along with physical shielding from the noise base; In the worst areas of the factory, the copper RS485 network had to be redeployed with optical cables, which transmitted the signal using couplers and decouplers. Very cool stuff back then.\n\n\u003cdiv class=\"mermaid\"\u003e\ngraph LR\n    S1[Thermocouple]\n    S2[Light Curtain]\n    S3[Ram Position]\n    S4[Injector Pressure]\n    P1[PLC]\n    C1[Datalogger]\n    N1[RS485 Network]\n    O1[Optic\u003cbr\u003eTx and Rx]\n    O2[Optic\u003cbr\u003eTx and Rx]\n    P2[Data Aggregator]\n    B1[Business\u003cbr\u003ePresentation]\n    E1[Enterprise\u003cbr\u003eResource Planner]\n    subgraph Moulding Press\n        S1 --\u003e P1\n        S2 --\u003e P1\n        S3 --\u003e P1\n        S4 --\u003e P1\n        subgraph Datalogger\n        P1 --\u003e C1\n        S1 -.- C1\n        end\n    end\n    \n    subgraph RS485 Network\n    C1 --\u003e N1\n        subgraph Optic Overlay\n        C1 -.- O1\n        O1 -.- N1\n        N1 -.- O2\n        O2 -.- P2\n        end\n    N1 --\u003e P2\n    end\n    \n    subgraph Resource Planning\n    P2 --\u003e B1\n    B1 --\u003e E1\n    end\n\u003c/div\u003e\n\nAfter some painful weeks, the solution was stabilized and happily ran for many years, and well after I had left the company.\n\n### Retrospective Thought's\n\nWhile this solution completely changed the efficiency of the production line, and provided end to end visibility of work at all its stages in the manufacturing pipeline; the super important business data was locked tightly behind the code, and only leveraged to illuminate the insights from data which was determined to be of business value in a fixed set of scenarios.\n\nUpdates were tedious; considering every data collector had to be individually re-flashed with a new firmware in-situ. Considering that every update was guaranteed to change the data payload, as new sensors were introduced or retired; then the only option was to shutdown the complete solution for the duration of the update; followed by a number of test cycles, before finally bringing everything up; which of course had to be in the correct sequence. Oh, and the Plant ran 3 Shifts, each 8 hours, for 5 days a week, and some weekends on a Saturday depending on demand pressures. This of course translated to your truly working a lot of Sunday's, and Early Monday's for the first shift to commence\n\nIn today's world the architecture would be much more modular, with the data been deposited into a large pool; and interrogated by subject matter experts to gain insights which are relevant to a particular business scenario, or process. However when a new perspective on a situation might be required; then with open access to all the data points, that subject matter expert can dip into the pool, and evaluate the data to expose the results specific to their case.\n\nAttempting to achieve this at the time would, well, likely have cost more than one of those moulding presses; and require that all the operators had degrees in computer science related fields.\n\n### The IoT Approach\n\nIf we were challenged with this sample problem 30 years on, how would we approach the project? \n\n* I certainly promise, I will not be writing my own communication Protocols again; there is no requirement to reinvent the wheel.\n* Data collectors have not changed a lot. Of course, they will be faster and more accurate, but a thermocouple is still a thermocouple.\n* New collectors types with much richer data, cameras for example; including heat, night vision, spectral, high speed, high resolution and so on.\n* Interacting with the collectors, is essentially reduced to a single line of code, thanks to the ecosystem of libraries.\n* Data would be all pooled securely, enabling the correct people, services and technologies access, to build new business models.\n* Data models packaged into light payloads, JSON, MQTT, etc.\n* No more nightmares of In-Situ updates; Over the Air, self applying firmwares\n* Secure Data communications, shielding and transports\n* Staggered updates; with no system wide downtime, or sensitive startup dependencies\n* Scale, Scale, Scale\n* Fail Fast\n* Change Management and DevOps\n* And so on...\n\nThe evolution is real, yet so much is still very familiar.\n\nBusiness logic Encapsulated as separate and distinct micro-services, ensuring a platform that can scale, while leveraging the correct tools at any point in time to address a scenario; enabling new and old approaches to be conducted in parallel, risk-free\n\n\n\u003cdiv class=\"mermaid\"\u003e\ngraph LR\n    S1[Thermocouple]\n    S2[Light Curtain]\n    S3[Ram Position]\n    S4[Injector Pressure]\n    S5[Cameara]\n    P1[PLC]\n    IOT1[IoT Device]\n    IOT2[IoT Edge\u003cbr\u003eDevice]\n    N1[Network\u003cbr\u003eCloud]\n    P2[Data Aggregator\u003cbr\u003eStream Hub]\n    B1[Stream\u003cbr\u003eAnalytics]\n    B2[Event\u003cbr\u003eHubs]\n    B3[Data Lake]\n    E1[Enterprise\u003cbr\u003eResource Planner]\n    subgraph Moulding Press\n    S1 --\u003e IOT1\n    S2 --\u003e IOT1\n    S3 --\u003e IOT1\n    S4 --\u003e IOT2\n    S5 --\u003e IOT2\n    P1 --\u003e IOT2\n    end\n    \n    subgraph Network\n    IOT1 --\u003e|IoT\u003cbr\u003ecommuncations| N1\n    IOT2 --\u003e|Edge\u003cbr\u003eprocessing| N1\n    N1 --\u003e|Data Stream\u003cbr\u003eMQTT Hub| P2\n    end\n    \n    subgraph Business Logic\n    P2 --\u003e B1\n    B1 -.- E1\n    P2 --\u003e B2\n    P2 --\u003e B3\n    end\n\u003c/div\u003e\n\n## Tooling Up\n\nTaking a leave from my beloved memories of *Borland's Turbo C* IDE and compiler; *(Which, I still have in my display cabinet,)* accompanied with its collection of 2 3.5\" floppy disks, and the now almost defunct RS232 serial cable behind, how do we approach these new developer kits for embedded IoT devices?\n\nPretty simple actually. Regardless of your OS platform, you can now leverage the rich ecosystems which these kits have established; you simply need to decide on what environment you wish to leverage and get started.\n\nBut be warned, when I started with the *Tern, Inc.* boards, there was only __one__ choice, the SDK was licensed for use with the specific board; today, however, this is not the case, and before you start on your first line of code, you can easily be overwhelmed.\n\n### Hardware\n\nOver the years a number of different standards have developed and evolved. One of the richest and possibly best know is that of **Arduino**; which is a family of boards with an amazing eco-system of supporting libraries; shapes, sizes, and purposes.\n\nHowever, Prior to spending time with this family, I actually went *100% Nerd*, and choose to develop on the **MicroChip PIC** series CPUs; which were 100% chips, and required a special programmer to flash the chip, in my real early days these were based on EPROMS (which could only be reset, by playing them in a special UV drawer), and later the cooler EEPROMS which had an Electronic Erase option on the Programable Read Only Memory. Debugging was a different story; in this case I had a RICE Device, which was an expensive 'thing' that enabled step by step debugging of the code been flashed; however most of the time, this was totally without any of the real inputs which would ultimately be connected to the circuit.\n\nThere are other boards, which I will take a closer look at on this journey; including the *Microsoft IoT SDK board*, which is known as the **MX Chip**, essentially also a developer board, but with a boot load of additional inputs and outputs.\n\nAnd of course, the board which I am still waiting patiently to arrive, the *Microsoft Azure Sphere board*.\n\n### Development Environments\n\nWhen I first started playing with these **Arduino** based systems, I of course made the default choice of development tools, and embraced the cumbersome Arduino IDE; which in many ways is still far less flexible than my fond memories of Borland's IDE.\n\nI quickly focused on VS Code, to use its extension for the Arduino IDE; which in all due respect is a really fantastic effort, and a revolutionary step forward, but due to its total dependency on the Arduino IDE been still installed, there are still a lot of irritable behaviors.\n\n### PlatformIO\n\nHowever, I finally saw the light, and one day in pure anger, uninstalled the Ardunio Extension and IDE from my system, and added PlatformIO as an alternative. At that time, I also had a copy of Atom installed, as I had embraced it long before VS Code; and what a change this was.  \n\nToday however, I am working primary in VS Code for all my work; and adding PlatformIO extensions turns development for these boards into a total dream.\n\n## Summary\n\nSo, after a long story, lets simply this; If you are working on, or considering to get hands dirty on your first IoT project; then allow me to suggest that you can save a ton of time and confusion by simply taking the following path:\n\n* IDE: VS Code with PlatformIO Extension\n* SDK: Start with Arduino as its an enormous community\n* Board: NodeMCU 1.0, Wemos 1D Mini, Lolin v3\n\nAnd all you need to plan for, is what project your going to build. It's Christmas time, and I love lights; so join me on next posts, as I dress the house for the holidays!",
    "lastmodified": "2023-04-13T22:40:03.116696162Z",
    "tags": [
      "Internet of Things"
    ]
  },
  "/posts/2018-12-18-managing-calendars": {
    "title": "Assistants, Family's, Work and Calendars = Chaos",
    "content": "\nAssumingly I am not alone, when we sit down as a family and talk about our day, and the plan for the next days or the weekend; only to realize that we have some real scheduling issues; because you totally forgot that you would take the children to an event; while your partner had a long-standing appointment with the hairdresser. \n\nLetting this happen once or twice, is forgivable, but happing on a regular basis; is the recipe for a lousy dispute; that we do not need.\n\n## Synchronizing Schedules\n\nThe solution, of course, is simple, We need to share some visibility of our schedules, and of course be consistent in making sure that we record these events in the calendar in the first place.\n\nThe problem is, however, what calendar do we use, and NO; a whiteboard stuck to the fridge or some other silly place is not an option. \n\nThis is a digital era, and I need a digital solution; as I can not predict where I might be when I agree with that business trip, or customer call which stomps all over that crucial other thing that I have now entirely forgotten about, and its not possible to run from Oslo to my Fridge door to check!\n\n## Environment Survey\n\nFirst, I need to determine what are the artefacts I am dealing with here;\n\n### The Actors\n\n* My Wife\n    * Her Schedule\n* Myself\n    * My Work Schedule, including Trips, Workshops and Meetings\n    * My MVP Schedule, including Product Calls, Meetups, Conferences, Community Time\n* 2 Children\n    * Pre-School, Appointments, etc.\n* Family Unit\n    * Outings, Appointments, Events, Games, etc.\n\n### The Actions\n\nNow, Let's consider the unconscious actions we take\n\n* My Wife\n    * iPhone to update her Calender\n* Myself \n    * Outlook primarily to manage both Work and MVP schedules\n* Google/Alexa/Siri\n    * Shout at these devices to update the Family calendar\n\n## The Objective\n\nThe vision is simple:\n\n|Situation| Requirements|\n|||\n|For Work | Ensure my calendar, offers a true representation of when I am available. Assist my colleagues and customers to not inadvertently stomp on time which I planned to use for other activities; for example, that dentist appointment.\n|Personal | Using the digital assistants, phone, watch, etc.; Maintain an up-to-date view of what my day or week truly looks like, including Personal, Family, Work and MVP appointments\n|Wife     | Requires a view of when I might be otherwise busy; so she can plan around me, when and if necessary\n|Family   | Easily setup events, while being aware of any potential conflicts which might arise, and address appropriately.\n\n## The approach\n\n### Family Calander\nUsing the shared Family calendar features offered by many of the consumer-focused cloud offerings; leverage these services, so that shared appointments are visible; while managing out own independent personal scheduled.\n\nWhile both my wife and I are currently iPhone users; there is no guarantee that this will always be the case; especially looking at the inflation on the newest models. Accessing the iCloud Calendars outside the Apple ecosystem is not a fantastic experience. \n\nCombined with the desire to leverage Siri, Alexa and Google Assistant; my current conclusion is that the best-supported Calander for families, for free is *Google Calander*. \n\nTherefore with existing Gmail accounts, we established a Family relationship and gained the shared family calendar feature.\n\nAdditionally, we Invited each other to our *personal calendars* so we can see the potential conflicts which may arise\n\n### My Work Calendars\n\nOthers essentially manage my Work and MVP Calendars; as they set meetings, appointments and so on, which I usually am obliged to join. I have kept these two environments independent; mainly due to the NDA's which I have signed which results in lots of sensitive emails flowing which I am not comfortable being managed by other mail administrators *(I wore that hat long enough to understand the potential access available)*\n\nUsing Microsoft Flow, I created three flows to synchronise these schedules\n\n1. Sync Work calendar to my Personal Google Calander\n2. Sync MVP calendar to my Personal Google Calander\n3. Sync Family Google Calander appointments to Work Calander *[as Time Blockers]*\n4. Sync MVP calendar to my Work Calander *[as Time Blockers]*\n\n### The Big Picture\n\nNow with the boundaries defined, and the flows described, let's visualise this challenge\n\n\n\u003cdiv class=\"mermaid\"\u003e\ngraph LR\n\n    subgraph Google Calanders\n    C[\u003cb\u003eMy Schedule\u003c/b\u003e\u003cbr\u003e\u003ci\u003eMy Personal\u003cbr\u003eGoogle Calander\u003c/i\u003e]\n    G[\u003cb\u003eWifes Schedule\u003c/b\u003e\u003cbr\u003e\u003ci\u003eHer Personal\u003cbr\u003eGoogle Calander\u003c/i\u003e]\n    D(\u003cb\u003eFamily\u003c/b\u003e\u003cbr\u003e\u003ci\u003eShared Family\u003cbr\u003eGoogle Calander\u003c/i\u003e)\n    end\n\n\n    subgraph My Calanders\n    A[\u003cb\u003eMVP Calander\u003c/b\u003e\u003cbr\u003e\u003ci\u003eOffice 365\u003c/i\u003e]\n    B[\u003cb\u003eWork Calander\u003c/b\u003e\u003cbr\u003e\u003ci\u003eOffice 365\u003c/i\u003e]\n    end\n\n    E(\u003cb\u003eMy Assistants Profile\u003c/b\u003e\u003cbr\u003e\u003ci\u003eGoogle/Alexa/Siri\u003c/i\u003e)\n    F(\u003cb\u003eWife Assistants Profile\u003c/b\u003e\u003cbr\u003e\u003ci\u003eGoogle/Alexa/Siri\u003c/i\u003e)\n\n    A -.-\u003e|Flow\u003cbr\u003e\u003cb\u003eMVP Blockers to Work\u003c/b\u003e|B\n    A --\u003e|Flow\u003cbr\u003e\u003cb\u003eMVP to My Schedule\u003c/b\u003e|C\n    B --\u003e|Flow\u003cbr\u003e\u003cb\u003eWork to My Schedule\u003c/b\u003e|C\n    \n    C -.- D\n    D --\u003e|Flow\u003cbr\u003e\u003cb\u003eFamily Blockers to Work\u003c/b\u003e| B\n    E --\u003e|New Bookings\u003cbr\u003eFamily Calander|D\n    C --\u003e|Presented Schedule\u003cbr\u003eInc Family Calander| E\n\n    G -.- D\n    F --\u003e|New Bookings\u003cbr\u003eFamily Calander|D\n    G --\u003e|Presented Schedule\u003cbr\u003eInc Family Calander| F\n\n\u003c/div\u003e\n\n## Setting Up\n\n### Google Calander\nUsing the existing Google accounts which you may have, establish a *Family* Relationship\n\nNext, Invite each party in the family to have read access at least to each other's calendar.\n\nTurn off notifications on these calendars if they are not your primary calendar. In my scenario I already get notifications from Work and MVP; so do not need duplicate notifications from Google also. My Wife, on the other hand, is using Google as her primary, so does still want notifications.\n\n### Configuring iPhone\n\nUsing the IOS Settings App, the following is how I set up both our iPhones.\n\n* Settings \n    * Passwords \u0026 Accounts \n        * Add Account\n            * Google \n                * Authenticate\n        * Select Gmail\n            * Enable Calander\n            * Enable Contacts\n    * Calander\n        * Default Calendar\n            * Family\n    * Contacts\n        * Default Account\n            * Gmail\n\nFor my phone, I use the Outlook App as the owner of both MVP and Work Mail and Calendars; As Siri has no access in here, only the Google Calendars are linked at the OS level, exposing the data to the native Calander app, and therefore Siri.\n\n### Google Assistant / Alexa\n\nSet up personal profiles, and then Link to your Personal Gmail Account; this should then expose the shared calendars also.",
    "lastmodified": "2023-04-13T22:40:03.124696259Z",
    "tags": [
      "Internet of Things"
    ]
  },
  "/posts/2019-01-02-Happy-New-Year": {
    "title": "Untitled Page",
    "content": "---\ntitle: 2019: Happy New Year\ntype: article \nlayout: post \ndescription: Welcome to 2019\ndate: 2019-01-02 08:20:30\ncategories: ['Career', 'MVP']\ntags: ['General']\nauthors: ['damian'] \ndraft: false \nimage: /images/2019/01/02/banner.jpg\ntoc: false \nfeatured: false \ncomments: True\n---\n\nFirst, allow me to thank all of you that are continuing to read this blog; despite the drought which I am predicting has now passed. I have a long list of topics which I wish to address in detail throughout the year; focused on the areas which I am most passionate.\n\n## Speaking\n\nLast year I had the opportunity to meet a lot of you in various parts of the globe, primarily speaking at some significant events; including the 'Nordic NIC Conference', 'Cloud and Datacenter Conference', 'Tech Camp' and 'Experts Live'.\n\nI am somewhat disappointed to have learned that after 3 years presenting at the NIC Conference, this year I wont be returning, due to concerns of competition *(community spirit?)* as I am involved with the management of the Azure Track for the new **Experts Live Norway** event; happening on *May 29th in Oslo*!\n\nI have been accepted to present at some different conferences throughout 2019; most specifics a little later.\n\n## Azure\n\nWorking with Azure Infrastructure on a daily basis; I have much to share related to many exciting technologies including *Terraform*, *Azure Resource Manager*, *DevOps Pipelines*, *Policies*, and *Governance*\n\n## IoT\n\nAfter much fun during Christmas, I also plan to share some insights into Home Automation and related technologies; Some of these are foundational, and leverage open source offers including *OpenHAB* and *Home-Assistant*; others integrate directly with Azure to leverage the IoT hub and AI services which it has to offer trough standards like MQTT.\n\nI also anticipate covering some more device-specific technologies; including a standard from my past *DMX512*, which has evolved to a new cool standard *e1.31*. Hint - From Dj Lights to Christmas Lights; but don't worry; I promise to decrypt is jargon on the way!\n\n## Roll On 2019\n\nIf you have any topics or questions which you believe I should be able to help explain, or should be considered as topics for some posts, presentations or webinars; then please do let me know; either via mail, twitter or comments on this blog.\n\nSlainte!",
    "lastmodified": "2023-04-13T22:40:03.120696211Z",
    "tags": []
  },
  "/posts/2019-01-04-VSCode-Azure-Cloudshell": {
    "title": "Configure VS Code with Azure Cloud Shell",
    "content": "\nAfter years living in tools like Visual Studio, and PowerShell; Currently my primary landing ground is Visual Studio Code. With my target audience firmly defined as Azure; In this post I am going to share my notes on how to get these two tools working harmonisly; and to make the experience a little richer, we will also mount the underlying Cloud Drive File Share of the Azure Cloud Shell on our local computer as a PowerShell Drive (PSDrive).\n\n\n \n## Azure Cloud Shell in Visual Studio Code\n\nIn VSCode we will naviagte to the **Extension** icon and search for **Azure Account**, then install it. \n\nThe Azure Account extension provides a single Azure sign-in and subscription filtering experience for all other Azure extensions. This extension also exposes the *Azure’s Cloud Shell* service in VS Code’s integrated terminal.\n\nOnce the extension in installed you will see the button to **Reload** your Visual Studio Code instance, which you must complete before we can proceed.\n\n### Sign In to Azure\n\nNow, Open the VS Code command palette (using the key sequence *CTRL, SHIFT + P* or *F1*) and select the option to Sign-in to your Azure Subscription; by typing `Azure: Sign In`\n\nOnce you click on Sign-in, Visual Studio Code will launch your browser and you will be naviagted to login to your azure account. Select the appropiate account and complete the authentication process. Once authenticated, the browser window will confirm, and tell you to return to VS Code.\n\nAs a confirmation of your autentication state, in the status bar of VS Code a new element which is prefixed with the word **Azure:** and postfixed with the account you just authenticated with will be presented.\n\n### Cloud Shell in VS Code\n\nNow that we are signed in, launch the VSCode command palette again (CTRL, SHIFT+P or F1), this time typing `Azure: Open PowerShell in Cloud Shell` or `Azure: Open Bash in Cloud Shell`\n\n\u003e Note: This extension requires *Node.JS*; If not found on your system, you will be prompted to remediate.\n\nVSCode will update the status bar to indicate it is activating the extenstion, If your account is determined to have access to mulitple Azure AD Directories, you will be prompted to select the directory you wish to work in for this session from the drop down list. \n\nAfter a few moments you will then observe the **Terminal** window launch, and the session will be be entitled as *PowerShell in Cloud Shell*. \n\n```powershell\nSelect directory...\nRequesting a Cloud Shell...\nConnecting terminal...\nWelcome to Azure Cloud Shell\n\nType \"az\" to use Azure CLI 2.0\nType \"help\" to learn about Cloud Shell\n\nVERBOSE: Authenticating to Azure ...\nVERBOSE: Building your Azure drive ...\nAzure:/\nPS Azure:\\\u003e\n```\n\n## Mount Azure Cloud Shell drive Locally\n\nFor the best flexability you will regularly have the requirement of interacting with the files in the Cloud Drive, directly from your local working environment.\n\nLaunch your browser, and authenticate to https://portal.azure.com\n\n1. Locate the *Resource Group* which contains the *Cloud Shell Storage*\n1. Open the *Storage Account* and select the *Files Service*\n1. Inspect the names of the presented File Shares, and select the name which contains your chosen authentication account, eg **cs-technology-damianflynn-com-1001100110011001**\n1. Finally, On the *File share* blade, you will select the 'Connect' button, which will present a new blade to the right of the window.\n1. In the blade, choose a **Drive letter** and then copy the auto-generated PowerShell Commands to map your Cloud Drive.\n\nIn a powershell session on your local machine, paste the copied commands; which will automatically mount the cloud drive to your chosen drive letter on your local computer.\n\nFrom here you can now access and interact with the cloud drive, making changes which will appear real time in the Cloud Shell.\n\nEnjoy!",
    "lastmodified": "2023-04-13T22:40:03.132696355Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2019-01-14-waking-daemons": {
    "title": "Waking Deamons",
    "content": "\nWith a multitude of Raspberry PI's deployed around the house, each taking a dedicated duty in ensuring that services run transparently; It is not uncommon for me to discover the initialization scripts designed to have these services auto start at boot is not working.\n\nThe content of this post is a reference for different methods which can be employed to resolve these stubborn daemons; which always are to fond of reappearing after an unplanned outage; or what is more commonly referred to as a Power Failure!\n\n## rc.local\n\nTo start a program on your Linux distribution *(I am focusing on Raspbian running on a Raspberry Pi)* at start-up, before other services are started, we will use the file `rc.local`.\n\n### Editing rc.local\n\nOn your Pi, using *nano* or *vi* which are installed by default, using elevated permissions trough *sudo*, we will edit the file `/etc/rc.local`:\n\n```bash\nsudo nano /etc/rc.local\n```\n\nAdd commands to execute the program, using absolute path references of the file location. \n\nThe final command in the file should be `exit 0` to indicate to the OS that we are terminating without error, then save the file and exit.\n\n```bash\n## Start our Node Application\nsudo node /usr/local/bin/cgateweb/index.js\nexit\n```\n\nProgram which are not expected to terminate, *(runs continuously in an infinite loop)* should be stated as a forked process by adding an ampersand `\u0026` to the end of the command. Failure to address this scenario will prevent the OS from completing its boot process. \n\nThe ampersand allows the command to run in a separate process and continue booting with the main process running.\n\n```bash\nsudo node /usr/local/bin/cgateweb/index.js \u0026\n```\n\n\u003e Note: A script added to  `/etc/rc.local` is added to the OS boot sequence. A bug here will prevent the OS boot sequence progressing. Recommend that the script’s output and error messages are directed to a text file for debugging.\n\n```bash \nsudo node /usr/local/bin/cgateweb/index.js \u0026 \u003e /var/log/myservice.log 2\u003e\u00261\n```\n\n\n## .bashrc\n\nThe `.bashrc` file executes on boot and *also* every time when a new terminal is opened, or when a new SSH connection is made.\n\nNormally, we would spawn our program, by placing the command at the bottom of `/home/pi/.bashrc` file. The program can be aborted with ‘ctrl-c’ while it is running!\n\n```bash\nsudo nano /home/pi/.bashrc\n```\n\nAdd commands to execute the program, using absolute path references of the file location. \n\n```bash\necho Running at boot \nsudo node /usr/local/bin/cgateweb/index.js \u0026\n```\n\nThe echo statement above is used to show that the commands in `.bashrc` file are executed on bootup as well as connecting to bash console.\n\n## init.d directory\n\nThe `/etc/init.d` directory contains the scripts which are started during the boot process, and also during the shutdown or reboot process.\n\n\nCreate a new file in the `/etc/init.d` directory\n\n```bash\ncd /etc/init.d\nsudo nano sample.py\n```\n\nWith the following sample content, we define a *Linux Standard Base (LSB)* (A standard for software system structure, including the filesystem hierarchy used in the Linux operating system) *init* script.\n\n```bash\n### BEGIN INIT INFO\n# Provides:          sample.py\n# Required-Start:    $remote_fs $syslog\n# Required-Stop:     $remote_fs $syslog\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Start daemon at boot time\n# Description:       Enable service provided by daemon.\n### END INIT INFO\n```\n\n\n*init.d* scripts require the above runtime dependencies to be documented so that it is possible to verify the current boot order, the order the boot using these dependencies, and run boot scripts in parallel to speed up the boot process.  \n\n\u003e [LSB *Init* Scripts guide](https://wiki.debian.org/LSBInitScripts).\n\nFinally, the script in the `/etc/init.d` directory should be executable, and added to the init database with the following commands:\n\n```bash\nsudo chmod +x sample.py\nsudo update-rc.d sample.py defaults\n```\n\n## SystemD\n\n`systemd` provides a standard process for controlling what programs run when a Linux system boots up. \n\nA sample *unit* file is provided by default in the OS, located at `/lib/systemd/system/sample.service`\n\n```bash\nsudo cp /lib/systemd/system/sample.service /lib/systemd/system/my.service\nsudo nano /lib/systemd/system/my.service\n```\n\nWith a copy of the sample file, we define a new service called *Sample Service* and we are requesting that it is launched once the multi-user environment is available. \n\n* **ExecStart** parameter specifies the command we want to run. * **Type**  set to `idle` to ensure that the *ExecStart* command is run only when everything else has loaded. \n\n\u003e Note: Paths are absolute and define the complete location of the runtime and any input files\n\n```bash\n[Unit]\nDescription=My Sample Service\nAfter=multi-user.target\n\n[Service]\nType=idle\nExecStart=/usr/bin/node /usr/local/bin/cgateweb/index.js\n\n[Install]\nWantedBy=multi-user.target\n```\n\nIn order to store the output in a log file you can change the *ExecStart* as follows:\n\n```bash\nExecStart=/usr/bin/node /usr/local/bin/cgateweb/index.js \u003e /var/log/myservice.log 2\u003e\u00261\n```\n\nThe permission on the unit file needs to be set to 644, and then we can tell *systemd* to start it during the boot sequence.\n\n```bash\nsudo chmod 644 /lib/systemd/system/my.service\nsudo systemctl daemon-reload\nsudo systemctl enable my.service\n```\n\n## crontab\n\nCrontab is a table used by `cron` which is a daemon used to run specific commands at a particular time.  Crontab is very flexible and can also run a program at boot or to repeat a task or program at specific times.\n\nCreate a script to bootstrap our program\n\n```bash\nsudo nano /home/pi/.scripts/myProgram.sh\n```\n\nand then add the command you wish to execute\n\n```bash\n/usr/bin/node /usr/local/bin/cgateweb/index.js \u003e /var/log/myservice.log 2\u003e\u00261\n```\n\nNow open crontab. You most likely will be required to open crontab with elevated permissions.\n\n```bash\nsudo crontab -e\n```\n\nAdd a new entry at the very bottom with `@reboot` to specify that you want to run the command at boot, followed by the command. Here we want to run our bootstrap script\n\n```crontab\n@reboot sudo /home/pi/.scripts/myProgram.sh\n```\n\nNow save the file and exit.\n\nWhen you restart the pi, the command will be run and we will get the output log file.\n\nBe a bit careful with the permissions and making sure that your program runs properly before you put it on boot: you can waste a lot of time trying to figure out what went wrong!\n\n```bash\ngrep cron /var/log/syslog\n```",
    "lastmodified": "2023-04-13T22:40:03.124696259Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2019-01-29-Windows10-Pester": {
    "title": "Updating Pester on Windows 10",
    "content": "\nI spend the majority of my time working on my Windows machines, and for many scenarios, I find it difficult to complain. However, when Windows decides to dig the boot in and not co-operative; usually is when I grab my Mac Book and get the work done.\n\nHowever, running aware from the problem rarely is a good fix for the issue; My latest battle has been Pester. The testing framework builds on Powershell, and by the grace of God, now shipped as part of the Windows 10 operating system.\n\nThe issue is that the included version is 3.4 and at the time of writing the current release is 4.6. Typically, this is a non-issue a directly issuing an `Update-Module` command addresses the issue and allow the product to continue.\n\nIn the odd case we might need to revert to a push and include the `-Force` switch; but for various reasons, this sometimes also fails; which is the case in this Pester Module scenario\n\n## The Hard Way\n\nA little research identifies that over time, a few things have evolved with this module. In our case, the certificate used for signing the modules has changed, and for obvious reasons of security, the normal processes are failing.\n\nTo accomplish the objective in this case, I have reverted to removing the pre-installed version of pester, and then once a memory; I can proceed to deploy the newest release of this product.\n\nUsing an administrative PowerShell session, I proceed to issue the following commands; which define where the module in question is residing on my system and then assume ownership of the associated files, which I then delete. After all, Powershell modules are discovered, based on the search folder they have been installed within.\n\n```powershell\n$module = \"c:\\Program Files\\WindowsPowerShell\\Modules\\Pester\"\ntakeown /F $module /A /R\nicacls $module /reset\nicacls $module /grant Administrators:'F' /inheritance:d /T\nRemove-Item -Path $Module -Recurse -Force -Confirm:$false\n``` \nNow, we can put the hammer away, and proceed to deploy the module we originally required, this time without challange.\n\n```powershell\nInstall-Module -name pester -MinimumVersion 4.3.0\n```\n\nHappy Testing",
    "lastmodified": "2023-04-13T22:40:03.132696355Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/2019-08-17-USG-Wireguard": {
    "title": "Configure Wireguard on UniFi USG",
    "content": "\n## Install the Wireguard Package\n\nSSH directly to your USG, and run the following commands:\n\n```bash\ncurl -L https://github.com/Lochnair/vyatta-wireguard/releases/download/0.0.20190123/wireguard-ugw3-0.0.20190702-1.deb -o /tmp/wireguard.deb\n\ndpkg -i /tmp/wireguard.deb\n```\n\n### Create the Tunnel Secrets\n\nTo keep stuff private, we will encrypt the traffic using a long password, known as a 'Key'. To make sure this is unique, we will use a tool provided by Wireguard to make a random key for us.\n\n```bash\ncd /config/auth\numask 077\nmkdir wireguard\ncd wireguard\nwg genkey \u003e wg_private.key\nwg pubkey \u003c wg_private.key \u003e wg_public.key\n```\n\n### Configure  the Tunnels\n\nWhile still connected to the USG, we will now create the Interface which will be our end of the tunnel. If we consider this as a Bridge, then as we configure this interface, we will provide the address for our side and also the address of the far side.\n\nThe far side is protected from just anyone connecting to it by using another long password (key) which we need to know before we can complete this process.\n\nIn this example 192.168.33.1 is assumed to be your network, you should change these to match your network space.\n\n```bash\nconfigure\n\n# We start, by creating a new Network space for our side of the VPN\nset interfaces wireguard wg0 address 10.192.10.2/32 \n\n# Configure the Port Wireguard will be listening with\nset interfaces wireguard wg0 listen-port 51820 \n\n# Allow this interface to forward the traffic over our tunnel\nset interfaces wireguard wg0 route-allowed-ips true\n\n# Now, we need to tell the interface the address of the far side of the bridge\n# And also the password to allow us connect\nset interfaces wireguard wg0 peer \u003cInsert-Public-Key-Of-Peer-Here\u003e endpoint 14.28.207.179:51820\n\n# Now, we will tell the far side of the bridge about our network  \n# This is to ensure that the far side lets our network get out of the tunnel\n# The sample only allows the IPs 192.168.33.101 to 192.168.33.106 to cross over\n# you can choose to let everything by using the address 0.0.0.0/0\nset interfaces wireguard wg0 peer \u003cInsert-Public-Key-Of-Peer-Here\u003e allowed-ips 10.192.10.0/32 \n\n# Lets tell the interface where to find our long password we created earlier\nset interfaces wireguard wg0 private-key /config/auth/wireguard/wg_private.key\n\n# Make the changes active, save them and exit configuration mode\ncommit\nsave\nexit\n```\n\n### Firewalls block traffic\n\nAnd our tunnel is no exception, so we need to allow our new Tunnel Interface to be permitted to let the traffic flow. In this case we need to let the far side of the bridge connect back to us; after all there is no point sending traffic over if nothing can come back !\n\n```bash\nconfigure\n\n# Configure the firewall\nset firewall name WAN_LOCAL rule 20 action accept\nset firewall name WAN_LOCAL rule 20 protocol udp\nset firewall name WAN_LOCAL rule 20 description 'WireGuard'\nset firewall name WAN_LOCAL rule 20 destination port 51820\n\n# Make the changes active, save them and exit configuration mode\ncommit\nsave\nexit\n```\n\nAfterwards dump the `config.gateway.json` and put it in the controller so it do not get overwritten\n\n### Reconfigure after an Update\n\nCopy your backed up `config.gateway.json` to `/var/lib/unifi/data/sites/default` on the system running the Controller (which might also be a Cloud Key).\n\nThen through the Controller Web UI navigate to **Devices**, click on the **USG** row and then in the **Properties** window navigate to **Config \u003e Manage Device** and click **Provision**.",
    "lastmodified": "2023-04-13T22:40:03.116696162Z",
    "tags": [
      "Networking"
    ]
  },
  "/posts/2019-09-11-ESP32-FastLED": {
    "title": "Running FastLED on the Dual-Core ESP32",
    "content": "\n## FastLED Package\n\nThere are many projects posted over the web which implement the excellent FastLED library on the ESP12 processor; however locating a project which implements this on the more powerful sibling is a lot more difficult.\n\nSo, with a few failed attempts and a lot of patching samples together; I have a stable running implementation which you can clone or fork to get up and running quickly with your own projects.\n\nThe sample includes 2 different sequences, a simple moving dot; and a more colorful Cylon effect.\n\nThe code is complied within Visual Studio Code; with the Platform.IO environment; and includes a working settings file while will automatically install the required libraries, ready to compile and flash to your device.\n\nCheck out the repo on GitHub @ [https://github.com/DamianFlynn/ESP32FastLED](https://github.com/DamianFlynn/ESP32FastLED); and if you have issues please use the tracker.\n\nEnjoy",
    "lastmodified": "2023-04-13T22:40:03.136696404Z",
    "tags": [
      "Internet of Things"
    ]
  },
  "/posts/2019-10-01-installing-docker-and-docker-compose-on-raspbian-buster": {
    "title": "Installing Docker and Compose on Raspbian Buster",
    "content": "\nQuickly update a new Raspberry Pi, which has an install of Raspbian Buster with Docker and Docker-compose.\n\n## Docker\n\nThis is simple, as the Docker team have done all the work\n\n```bash\ncurl -fsSL get.docker.com -o get-docker.sh\nsh get-docker.sh\n```\n\nAnd, we can add our user to the Docker group so we do not need the `sudo` every time. I am using the environment variable `$USER`; which indicates who is logged in currently. In my case this is the user *pi*.\n\n```bash\nsudo usermod -aG docker $USER\n```\n\n\nRight, that was painful. now reboot the Pi and we are solid.\n\n## Docker-Compose\n\nThis is actually a Python script. Raspbian Buster is shipped with Python 3.6; so we just need to add `PIP3` to install the python packages from *pypy*\n\n```bash\nsudo apt-get install -y python3 python3-pip\nsudo pip3 install docker-compose\n```\n\nWow, that was a struggle, lets check we are good\n\n```bash\ndocker-compose --version\n```",
    "lastmodified": "2023-04-13T22:40:03.128696307Z",
    "tags": [
      "Az CLI",
      "PowerShell",
      "Enterprise Agreement",
      "Automation",
      "SPN"
    ]
  },
  "/posts/2019-11-02-Guacamole": {
    "title": "Guacamole Azure Appliance",
    "content": "\nApache Guacamole is a free and open source web application which lets you access your dashboard from anywhere using a modern web browser. It is a clientless remote desktop gateway which only requires Guacamole installed on a server and a web browser supporting HTML5.\n\nGuacamole is the best way to keep multiple instances accessible over the internet. Once you add an instance to Guacamole, you don’t need to remember the password as it can securely store the credentials. It also lets you share the desktops among other users in a group. Guacamole supports multiple connection methods such as SSH, Telnet, VNC, and RDP.\n\nIn this tutorial, we will install Apache Guacamole on a Azure with an Ubuntu 16.04 instance.\n\n## Guacamole Server\n\nGuacamole server consists of the native server-side libraries required to connect to the server and the **guacd** tool. **guacd** is the Guacamole proxy daemon which accepts the user’s connections and connects to the remote desktop on their behalf. Given below is the architecture of Guacamole System.\n\n[Architecture](.\\image1.png)\n\n\u003e Note: It is required to compile and install the Guacamole server on the host machine, installing the binary is not possible for Guacamole server\n\n### Server Install\n\nDownload the Guacamole server source code files into the temporary directory.\n\n```bash\ncd /tmp\nwget \"http://apache.org/dyn/closer.cgi?action=download\u0026filename=guacamole/0.9.14/source/guacamole-server-0.9.14.tar.gz\" -O guacamole-server-0.9.14.tar.gz\n```\n\nExtract the source code archive.\n\n```bash\ntar xf guacamole-server-0.9.*.tar.gz\ncd guacamole-server-0.9.*\n```\n\nCompile and install the source code.\n\n```bash\n./configure --with-init-dir=/etc/init.d\nmake\nmake install\n```\n\nThe installation will also set up an `init` script which can be used to manage the `guacd` daemon. Create the necessary links and cache for the shared libraries.\n\n```bash\nldconfig\n```\n\nGuacamole server is now installed on your instance. Start the Guacamole proxy daemon and enable it to automatically start at boot time using the following commands.\n\n```bash\nsystemctl enable guacd\nsystemctl start guacd\n```\n\nYou can check the status of the service by running.\n\n```bash\nsystemctl status guacd\n```\n\n## Guacamole Client\n\nGuacamole client is Java based web application which contains all the Java and JavaScript code required for running the user interface of Guacamole. It ultimately creates a web application which connects to the `guacd` daemon running in the background using Guacamole protocol. In the foreground, it renders the remote desktop interface using HTML5 on the web browser to the authorized users.\n\n\u003e Unlike Guacamole server, Guacamole client is not required to be compiled and install from source. Cross-platform Guacamole client binary is available to download and install.\n\n### Apache Tomcat Install\n\nGuacamole binary requires a Java web server to run. In this tutorial, we will install Apache Tomcat 7 or 8 to run the Guacamole binary file.\n\nInstall Java 8 runtime on your server, installing JDK is not required since we do not need to compile any Java code.\n\n```bash\nyum -y install java-1.8.0-openjdk.x86_64\n```\n\nCreate a new group and user for Tomcat installation. Running Tomcat server with an unprivileged user is recommended for security reasons.\n\n```bash\ngroupadd tomcat\nuseradd -M -s /bin/nologin -g tomcat -d /opt/tomcat tomcat\n```\n\nDownload latest Tomcat server of version 8.5 from Apache mirror.\n\n```bash\nwget http://www-us.apache.org/dist/tomcat/tomcat-8/v8.5.28/bin/apache-tomcat-8.5.28.tar.gz\n```\n\nExtract the archive into /opt/tomcat directory.\n\n```bash\nmkdir /opt/tomcat\ntar xvf apache-tomcat-8*.tar.gz -C /opt/tomcat --strip-components=1\n```\n\nProvide appropriate permissions and ownership to Tomcat server files.\n\n```bash\ncd /opt/tomcat\nchgrp -R tomcat /opt/tomcat\nchmod -R g+r conf\nchmod g+x conf\nchown -R tomcat webapps/ work/ temp/ logs/\n```\n\nCreate a new systemd service file for managing Tomcat server.\n\n```bash\nnano /etc/systemd/system/tomcat.service\n```\n\nPopulate the file with the following configuration.\n\n```ini\n[Unit]\nDescription=Apache Tomcat Web Application Container\nAfter=syslog.target network.target\n[Service]\nType=forking\nEnvironment=JAVA_HOME=/usr/lib/jvm/jre\nEnvironment=CATALINA_PID=/opt/tomcat/temp/tomcat.pid\nEnvironment=CATALINA_HOME=/opt/tomcat\nEnvironment=CATALINA_BASE=/opt/tomcat\nEnvironment='CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC'\nEnvironment='JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom'\nExecStart=/opt/tomcat/bin/startup.sh\nExecStop=/bin/kill -15 $MAINPID\nUser=tomcat\nGroup=tomcat\nUMask=0007\nRestartSec=10\nRestart=always\n[Install]\nWantedBy=multi-user.target\n```\n\nStart the Tomcat server and enable it to automatically start at boot time.\n\n```bash\nsystemctl start tomcat\nsystemctl enable tomcat\n```\n\nYou can check if Tomcat is running by going to http://your-server-ip:8080 using your favourite web browser. You should see the default Tomcat page. If you are getting some error, then make sure that port “8080” is allowed in Security group rules.\n\n### Install Guacamole Client\n\nSince we have installed the Tomcat server, download the Guacamole client binary file using the following command.\n\n```bash\nwget \"http://apache.org/dyn/closer.cgi?action=download\u0026filename=guacamole/0.9.14/binary/guacamole-0.9.14.war\" -O guacamole-0.9.14.war\n```\n\nMove the Guacamole client file to the Tomcat’s webapps directory.\n\n```bash\nmv guacamole-0.9.14.war /opt/tomcat/webapps/guacamole.war\n```\n\nRestart the Tomcat server.\n\n```bash\nsystemctl restart tomcat\n```\n\nGuacamole client is now installed on your server, you can check if Guacamole client is working by going to http://your-server-ip:8080/guacamole using your favourite browser. You should see Guacamole login interface. You will not be able to log in yet as we have not configured authentication yet.\n\n\n## Setting Up Authentication\n\nGuacamole client supports multiple authentication mechanisms such as file-based auth, database auth, OAuth, LDAP etc. In this section of the tutorial, we will configure database based authentication using MySQL database server.\n\n### Local Authentication\n\nMySQL database will be used to store the authentication and other data. Since we do not require high performance and scalability which ApasaraDB provides, we will install MySQL server on the same instance.\n\nInstall MariaDB server which is an open source fork of MySQL.\n\n```bash\nyum -y install mariadb mariadb-server\n```\n\nStart the MariaDB server and enable it to automatically start at boot time.\n\n```bash\nsystemctl start mariadb\nsystemctl enable mariadb\n```\n\nSet a password for the MySQL root user and secure the server instance by removing the test database and user.\n\n```bash\nmysql_secure_installation\n```\n\nNow login to your MySQL shell using the root user and the password you just created.\n\n```bash\nmysql -u root -p\n```\n\nRun the following queries to create a new database named guacdb along with guacdb-user having full access to the database. Please change StrongPassword to a very strong password.\n\n```sql\nCREATE DATABASE guacdb CHARACTER SET utf8 COLLATE utf8_general_ci;\nCREATE USER 'guacdb-user'@'localhost' IDENTIFIED BY 'StrongPassword';\nGRANT ALL PRIVILEGES ON guacdb.* TO 'guacdb-user'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n```\n\n### Configure Guacamole\n\nNow that our database server is running, we need to install the MySQL connector and Guacamole JDBC auth plugin. Create the new directories to store the plugins.\n\n```bash\nmkdir -p /etc/guacamole/{extensions,lib}\n```\n\nDownload the MySQL connector extension from MySQL site.\n\n```bash\ncd /tmp\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.45.tar.gz\n```\n\nExtract and move the MySQL connector into `/etc/guacamole/lib.`\n\n```bash\ntar xf mysql-connector-java-5.1.45.tar.gz\nmv mysql-connector-java-5.*/mysql-connector-java-5.*.jar /etc/guacamole/lib/\n```\n\nDownload the Guacamole JDBC authentication extension from Apache Guacamole site.\n\n```bash\ncd /tmp\nwget \"http://apache.org/dyn/closer.cgi?action=download\u0026filename=guacamole/0.9.14/binary/guacamole-auth-jdbc-0.9.14.tar.gz\" -O guacamole-auth-jdbc-0.9.14.tar.gz\n```\n\nExtract the archive and move the extension to `/etc/guacamole/extensions` directory.\n\n```bash\ntar xf guacamole-auth-jdbc-0.9.14.tar.gz\nmv guacamole-auth-jdbc-0.9*/mysql/guacamole-auth-jdbc-mysql-0.9*.jar /etc/guacamole/extensions/\n```\n\nSince we have already created the database and database user, we can proceed to create the database schema and import the initial data. The schema is shipped along with the JDBC extension.\n\nImport the SQL schema and initial data into the `guacdb` database using the following command. Provide the password of the MySQL root user when prompted.\n\n```bash\ncd guacamole-auth-jdbc-0.9*/mysql/schema\ncat *.sql | mysql -u root -p guacdb\n```\n\nCreate a new configuration file for Apache Guacamole so it can override the default configuration.\n\n```bash\nnano /etc/guacamole/guacamole.properties\n```\n\nPopulate the file with the following configuration. Make sure to edit the StrongPassword with the actual password of guacdb-user.\n\n```ini\nmysql-hostname: localhost\nmysql-port: 3306\nmysql-database: guacdb\nmysql-username: guacdb-user\nmysql-password: StrongPassword\nmysql-default-max-connections-per-user: 0\nmysql-default-max-group-connections-per-user: 0\n```\n\nSet `GUACAMOLE_HOME` environment variable so that the Guacamole Server can read the configuration file and the extensions.\n\n```bash\necho \"export GUACAMOLE_HOME=/etc/guacamole\" \u003e\u003e ~/.bash_profile\nsource ~/.bash_profile\n```\n\nDisable SELinux as it causes errors when running Guacamole.\n\n```bash\nsed -i 's/enforcing/disabled/g' /etc/selinux/config\nsetenforce 0\n```\n\nRestart Guacamole proxy daemon and Tomcat server so that the new configuration can take effect.\n\n```bash\nsystemctl restart guacd\nsystemctl restart tomcat\n```\n\nGuacamole Client authentication is now configured on your server. You can check if you can log in by going to http://your-server-ip:8080/guacamole using your favourite browser. Log in using the default administrator user “guacadmin” and password “guacadmin”.\n\n## Setting up Nginx Reverse Proxy\n\nSetting up a reverse proxy secured with SSL is recommended to encrypt the data exchanged between the browser and the Guacamole server. This will also map a domain name to your server so you won’t need to remember the IP address of the server.\n\nInstall Nginx web server.\n\n```bash\nyum -y install nginx\n```\n\nStart the Nginx web server and enable it to automatically start at boot time.\n\n```bash\nsystemctl start nginx\nsystemctl enable nginx\n```\n\n### SSL Certificate\n\nIn this tutorial, we will use the certificates generated with Let’s Encrypt certificate authority. If you wish to use more production friendly certificates, you can purchase commercial certificates from Alibaba Cloud.\n\nDownload and install Certbot. Certbot is an official client application for Let’s Encrypt SSL generation.\n\n```bash\nwget https://dl.eff.org/certbot-auto -O /usr/bin/certbot\nchmod a+x /usr/bin/certbot\n```\n\n\u003e Note: Before requesting SSL certificates, make sure that the domain you are using is pointed towards the IP address of the instance. If not, make an “A” type record in DNS management panel and point the domain or subdomain to the public IP address of your instance and wait for the DNS to propagate.\n\nGenerate Let’s Encrypt SSL certificates for your domain.\n\n```bash\ncertbot certonly --webroot -w /usr/share/nginx/html -d guac.example.com\n```\n\nReplace all occurrences of *guac.example.com* with your actual domain name. The above command will ask you for your email to send you renewal notices. If the certificates are generated successfully, you should get following output.\n\n```bash\nIMPORTANT NOTES:\n - Congratulations! Your certificate and chain have been saved at:\n   /etc/letsencrypt/live/guac.example.com/fullchain.pem\n   Your key file has been saved at:\n   /etc/letsencrypt/live/guac.example.com/privkey.pem\n   Your cert will expire on 2018-06-05. To obtain a new or tweaked\n   version of this certificate in the future, simply run certbot\n   again. To non-interactively renew *all* of your certificates, run\n   \"certbot renew\"\n  ...\n```\n\n#### Auto-Renew Certificate\n\nCreate a cron job to renew the certificates as Let’s Encrypt certificates are expired in every three months.\n\n```bash\n{ crontab -l; echo '36 2 * * * /usr/bin/certbot renew --post-hook \"systemctl reload nginx\"'; } | crontab -\n```\n\nThe above command will run the renewal command every day at 2.36 AM. If the certificates are due for expiry it will automatically renew them.\n\nCreate a new server block configuration file for Guacamole web application reverse proxy.\n\n```bash\nnano /etc/nginx/conf.d/guacamole.conf\n```\n\nPopulate the file with the following configuration. Replace the example domain name with the actual one. Also, make sure that the path to the Let’s Encrypt SSL certificate and the private key is correct.\n\n```config\nserver {\n    listen 80;\n    server_name guac.example.com;\n    return 301 https://$host$request_uri;\n}\nserver {\n    listen 443 ssl http2;\n    server_name guac.example.com;\n    root html;\n    index index.html index.htm;\n    ssl on;\n    ssl_certificate         /etc/letsencrypt/live/guac.example.com/fullchain.pem;\n    ssl_certificate_key     /etc/letsencrypt/live/guac.example.com/privkey.pem;\n    ssl_session_cache    shared:SSL:10m;\n    ssl_session_timeout    1440m;\n    ssl_protocols  TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers HIGH:!aNULL:!eNULL:!EXPORT:!CAMELLIA:!DES:!MD5:!PSK:!RC4;\n    ssl_prefer_server_ciphers on;\n    add_header Strict-Transport-Security \"max-age=31536000; includeSubdomains;\";\n            \n    access_log  /var/log/nginx/guacamole.access.log;\n    location / {\n    proxy_pass http://localhost:8080/guacamole/;\n    proxy_buffering off;\n    proxy_http_version 1.1;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Connection $http_connection;\n    proxy_cookie_path /guacamole/ /;\n    }\n}\n```\n\nCheck the Nginx configuration for errors.\n\n```bash\nnginx -t\n```\n\nYou should see the following output if the configuration is error free.\n\n```bash\n[root@guacamole ~]# nginx -t\nnginx: the configuration file /etc/nginx/nginx.conf syntax is ok\nnginx: configuration file /etc/nginx/nginx.conf test is successful\n```\n\nRestart Nginx web server to implement the changes in the configuration.\n\n```bash\nsystemctl restart nginx\n```\n\nNow you can go to https://guac.example.com to access the Guacamole dashboard. The connections to the server and the browser are also secured with SSL.\n\n## Connecting First Client\n\nGuacamole server is now ready and working. You can add as many remote servers as you want. It can connect to the remote clients using SSH, Telnet, RDP, and VNC. To verify if it can connect to the remote server, let’s add our first SSH based connection.\n\n### Change Default Password\n\nBefore proceeding further, let’s change the password of the default “guacamole” user. Login with default administrator user “guacadmin” and password “guacadmin” and go to the “Preferences” tab. Change the default password from this tab.\n\n![](2019-11-02-Guacamole/opps-missing-image.png)\n\n### Add SSH Connection\n\nTo add a new connection, go to “Connections” tab and click on “Add new Connection” button. Provide a name for the connection and choose the protocol from drop down. Since I am connecting to the Guacamole server via SSH, I am selecting “SSH”.\n\n![](2019-11-02-Guacamole/opps-missing-image.png)\n\nIn “Parameters” provide the hostname of the target server and port. You also use “localhost” for connecting the same server. Provide the username and password, if connecting through private key than provide the contents of the private key. You can also configure the display, such as color scheme and fonts etc. Once you are done, click on “Save” button.\n\n![](2019-11-02-Guacamole/opps-missing-image.png)\n\n### Test Connection\n\nTo connect to the SSH server you just added, go to the dashboard and it will automatically try to connect to the SSH when there is only a single connection is available. Once you are connected, you should see the following interface.\n\n![](2019-11-02-Guacamole/opps-missing-image.png)\n\nSimilarly, you can add more SSH clients and graphical dashboards using various connection methods. The remote connections you want to add are not required to have either of Guacamole Server or Client, you can directly add them. Once you add the remote servers in Guacamole, you will only need a web browser to access them from anywhere in the world.",
    "lastmodified": "2023-04-13T22:40:03.140696452Z",
    "tags": [
      "Az CLI",
      "PowerShell",
      "Enterprise Agreement",
      "Automation",
      "SPN"
    ]
  },
  "/posts/2019-11-07-Network-WAF": {
    "title": "Ignite 2019 - Web Application Gateway and Firewall",
    "content": "\nIgnite Session: BRK3169\nPresenter: Amit Srivastava\n\nMission Critical HTTP Applications, there are many things to consider\n\nPersonalized, Micro-Services, Rich Context.... To support this MS have a number of services i the Suite - Azure Frontdoor, Application Gateway, Azure CDN, Web Application Firewall, Azure Load Balancer, and Azure Traffic Manager\n\n\nRegional Gateway as a service\n\n| Feature | Description\n|---|---|\n|Platform managed | Built in high availability and scalability)\n|Layer 7 balancing |URL Path, Host based, round robin, session affinity, redirection\n|Security and SSL management |WAF, SSL Offload, SSL Re-Encryption, SSL Policy\n|Public or ILB | Public, Internal or Both\n|Flexible backends |VMs, VMSS, AKS, Public IP, Cloud Services, ALB.ILB/ On-Premises\n|Rich Diagnostics |Azure Monitor, Log analytics, Network Watched, RHC, Azure Security Center\n\nStandard V2 SKU in GA, Currently Available in 26 regions, Builtin Zone Redundancy, Static VIP, HTTP Header/cookies insertion/modification\n\n* Increased scale limits 20 -\u003e 100 Listeners\n* Key Vault integration and auto-renewal of SSL Certs\n* AKS ingress Controller\n\n# Autoscaling and Performance Improvements\n\n* Grow and shrink based on app traffic requirements\n* 5X better SSL offloads performance\n* 500-50,000 connections/sec with RSA 2048 bit certs\n* 30,000-3,000,000 persistent connections\n* 2,500-250,000 reqs/sec\n\n# Announcing General Availability:\n\n* Frontend TLS cert integration with Azure Key Vault\n* Utilized user-assigned managed identity access control for key vault\n* User Certificates or secrets on key vault\n* Polls every 4 hours to enable automatic cert renewal\n* manual override of specific certificate version retrieval\n* Manipulate Request and Response headers \u0026 cookies\n  - Strip port from X-Forwarded-for header\n  - Add security headers like HSTS and X-XSS-Protection\n  - Common header manipulation ex HOST, SERVER\n\n## AKS Ingress Control using Application Gateways\n\n* Deployed using Helm\n* Utilizes Pod-AAD for ARM authentication\n* Tighter integration with AKS add on support coming\n* Support URI path based, host based, SSL termination, SSL re-encryption, redirection, custom health probes, draining, cookie affinity\n* Support for Lets Encrypt provide TLS certificates\n* WAF fully supported with custom listener policy\n* Support for multiple AKS as backend\n* Support for  mixed mode - both AKS and other backend types on the same Application Gateway\n\nhttp://aka.ms/appgwaks\n\n## Wild Card Listener\n\n* Support for Wildcard characters in the listener host name\n* Support for * and ? Characters in host name\n* Associated wildcard or SAN certificates the service HTTPS enabled domains\n* Send traffic to multiple tenant end points\n\n## Diagnostics and logs enhancements\n\n* TLS Protocol\n* TLS Cipher\n* Backend target server\n* backend response code\n* backend latency\n\n## Metrics\n\n* Backend response status code\n* RPS healthy nodes\n* End to End Latency\n* Backend Latency\n* Backend connect, first byte and last byte latency\n\n# App Monitor Insights for Application Gateway\n\nSingle health and metic console for your entire cloud network\nNo agent/configuration required\n\n# Azure WAF - Cloud Native WEB Application Firewall\n\nUnified WAF offering to protect your apps at network edge or region uniformly\n\n## Public preview announced\n\nMicrosoft threats intelligence\n\n* Protect agains automatic attacks\n  - Managed good and bad bots with Azure BotManager Rule Set\n  - Data is refreshed daily\n  - Easy to configure in WAF policy\n  - Helps increase your applications performance, by stopping aggressive crawlers.\n\n* Site and URI path specific WAF Policies\n  - Customized WAF police at the region WAF\n  - Assign different Policies to different sites\n  - Site specific polices implies you can tune the WAF to suit the needs of each site independently\n\n* Geo filtering on regional WAF\n  - Allow or Block a list of countries,\n  - Support log mode\n\n* Rule Set for CRS 3.1 added (to be the default soon)\n* Integration with Azure Sentinel\n* Performance and concurrency enhancements",
    "lastmodified": "2023-04-13T22:40:03.120696211Z",
    "tags": [
      "Custom Providers",
      "Managed Applications",
      "Service Catalog"
    ]
  },
  "/posts/2020-04-01-ARM-Function-Union": {
    "title": "Azure IaC - Appending Tags",
    "content": "\nTodays conundrum: As I am leveraging templates, there will always be some standard tags I require to implement within the template, but I also require to provide additional tags as a parameter to be appended with the deployment.\n\nMy objective is to set up tags within an ARM template in accordance with good governance and the Cloud adoption framework.\n\n## Solution\n\nARM Template functions to the rescue. Todays salvation is called `union`, which you can learn more about on the actual [reference site][https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-template-functions-array#union] \n\nThis is the existing implementation\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"configVersion\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"1.0.0.0\",\n      \"metadata\": {\n        \"description\": \"The version of the parameters file that holds the configuration.\"\n      }\n    },\n    \"purpose\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"diag\",\n        \"log\",\n        \"audit\",\n        \"data\"\n      ],\n      \"defaultValue\": \"data\",\n      \"metadata\": {\n        \"description\": \"The designated purpose of the storage account. 'diag' for diagnostics, 'log' for logging, 'audit' for auditing, 'data' for data storage\"\n      }\n    },\n    \"resilience\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Standard_LRS\",\n        \"Standard_ZRS\",\n        \"Standard_GRS\",\n        \"Standard_RAGRS\"\n      ],\n      \"defaultValue\": \"Standard_LRS\",\n      \"metadata\": {\n        \"description\": \"Choose a level of resilience and tier suitable for the purpose and region\"\n      }\n    },\n    \"tier\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Standard\",\n        \"Premium\"\n      ],\n      \"defaultValue\": \"Standard\",\n      \"metadata\": {\n        \"description\": \"Choose tier, Standard or Premium.\"\n      }\n    },\n    \"kind\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Storage\",\n        \"StorageV2\",\n        \"BlobStorage\",\n        \"FileStorage\",\n        \"BlockBlobStorage\"\n      ],\n      \"defaultValue\": \"StorageV2\",\n      \"metadata\": {\n        \"description\": \"Choose a kind of storage account\"\n      }\n    }\n  },\n  \"variables\": {\n    \"iacVersion\": \"undefined\",\n    \"rgName\": \"[resourceGroup().name]\",\n    \"rgLocation\": \"[resourceGroup().location]\",\n    \"uniqueString\": \"[uniqueString(subscription().id, resourceGroup().id)]\",\n    \"storageAccountAffix\": \"[concat(replace(variables('rgName'), '-', ''), parameters('purpose'))]\",\n    \"storageAccountName\": \"[toLower(substring(replace(concat(variables('storageAccountAffix'), variables('uniqueString')), '-', ''), 0, 23) )]\"\n  },\n  \"resources\": [\n    {\n      \"comments\": \"~~ Storage Account  ~~\",\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"apiVersion\": \"2018-07-01\",\n      \"name\": \"[variables('storageAccountName')]\",\n      \"sku\": {\n        \"name\": \"[parameters('resilience')]\",\n        \"tier\": \"[parameters('tier')]\"\n      },\n      \"kind\": \"[parameters('kind')]\",\n      \"location\": \"[variables('rgLocation')]\",\n      \"tags\": {\n        \"IaCVersion\": \"[variables('iacVersion')]\",\n        \"ConfigVersion\": \"[parameters('configVersion')]\"\n      },\n      \"scale\": null,\n      \"properties\": {\n        \"supportsHttpsTrafficOnly\": true,\n        \"encryption\": {\n          \"services\": {\n            \"file\": {\n              \"keyType\": \"Account\",\n              \"enabled\": true\n            },\n            \"blob\": {\n              \"keyType\": \"Account\",\n              \"enabled\": true\n            }\n          },\n          \"keySource\": \"Microsoft.Storage\"\n        },\n        \"accessTier\": \"[if(equals(parameters('kind'), 'Storage'), json('null'),'Hot')]\"\n      },\n      \"dependsOn\": [\n      ]\n    }\n  ],\n  \"outputs\": {\n    \"storageId\": {\n      \"type\": \"string\",\n      \"value\": \"[resourceId('Microsoft.Storage/storageAccounts', variables('storageAccountName'))]\"\n    },\n    \"storageAccountName\": {\n      \"type\": \"string\",\n      \"value\": \"[variables('storageAccountName')]\"\n    }\n  }\n}\n```\n\n###  The Change Set\n\nAnd using our new `union` we can resolve the puzzle with 3 simple changes\n\nTo the `parameter` we add a new object\n\n```json\n    \"tagValues\": {\n      \"type\": \"object\",\n      \"defaultValue\": {\n        \"Dept\": \"Undefined\",\n        \"Environment\": \"Development\"\n      }\n    },\n```\n\nIn the `variables` we can define our default tags\n\n```json\n    \"defaultTag\": {\n      \"IaCVersion\": \"[variables('iacVersion')]\",\n      \"ConfigVersion\": \"[parameters('configVersion')]\"\n    },\n```\n\nAnd in the `resource` we can use the `union` function to merge the objects together\n\n```json\n  \"tags\": \"[union(parameters('tagValues'),variables('defaultTag'))]\",\n```\n\n### Final View\n\nThe final template will look as follows\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"configVersion\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"1.0.0.0\",\n      \"metadata\": {\n        \"description\": \"The version of the parameters file that holds the configuration.\"\n      }\n    },\n    \"tagValues\": {\n      \"type\": \"object\",\n      \"defaultValue\": {\n        \"Dept\": \"Undefined\",\n        \"Environment\": \"Development\"\n      }\n    },\n    \"purpose\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"diag\",\n        \"log\",\n        \"audit\",\n        \"data\"\n      ],\n      \"defaultValue\": \"data\",\n      \"metadata\": {\n        \"description\": \"The designated purpose of the storage account. 'diag' for diagnostics, 'log' for logging, 'audit' for auditing, 'data' for data storage\"\n      }\n    },\n    \"resilience\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Standard_LRS\",\n        \"Standard_ZRS\",\n        \"Standard_GRS\",\n        \"Standard_RAGRS\"\n      ],\n      \"defaultValue\": \"Standard_LRS\",\n      \"metadata\": {\n        \"description\": \"Choose a level of resilience and tier suitable for the purpose and region\"\n      }\n    },\n    \"tier\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Standard\",\n        \"Premium\"\n      ],\n      \"defaultValue\": \"Standard\",\n      \"metadata\": {\n        \"description\": \"Choose tier, Standard or Premium.\"\n      }\n    },\n    \"kind\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Storage\",\n        \"StorageV2\",\n        \"BlobStorage\",\n        \"FileStorage\",\n        \"BlockBlobStorage\"\n      ],\n      \"defaultValue\": \"StorageV2\",\n      \"metadata\": {\n        \"description\": \"Choose a kind of storage account\"\n      }\n    }\n  },\n  \"variables\": {\n    \"iacVersion\": \"undefined\",\n    \"defaultTag\": {\n      \"IaCVersion\": \"[variables('iacVersion')]\",\n      \"ConfigVersion\": \"[parameters('configVersion')]\"\n    },\n    \"rgName\": \"[resourceGroup().name]\",\n    \"rgLocation\": \"[resourceGroup().location]\",\n    \"uniqueString\": \"[uniqueString(subscription().id, resourceGroup().id)]\",\n    \"storageAccountAffix\": \"[concat(replace(variables('rgName'), '-', ''), parameters('purpose'))]\",\n    \"storageAccountName\": \"[toLower(substring(replace(concat(variables('storageAccountAffix'), variables('uniqueString')), '-', ''), 0, 23) )]\"\n  },\n  \"resources\": [\n    {\n      \"comments\": \"~~ Storage Account  ~~\",\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"apiVersion\": \"2018-07-01\",\n      \"name\": \"[variables('storageAccountName')]\",\n      \"sku\": {\n        \"name\": \"[parameters('resilience')]\",\n        \"tier\": \"[parameters('tier')]\"\n      },\n      \"kind\": \"[parameters('kind')]\",\n      \"location\": \"[variables('rgLocation')]\",\n      \"tags\": \"[union(parameters('tagValues'),variables('defaultTag'))]\",\n      \"scale\": null,\n      \"properties\": {\n        \"supportsHttpsTrafficOnly\": true,\n        \"encryption\": {\n          \"services\": {\n            \"file\": {\n              \"keyType\": \"Account\",\n              \"enabled\": true\n            },\n            \"blob\": {\n              \"keyType\": \"Account\",\n              \"enabled\": true\n            }\n          },\n          \"keySource\": \"Microsoft.Storage\"\n        },\n        \"accessTier\": \"[if(equals(parameters('kind'), 'Storage'), json('null'),'Hot')]\"\n      },\n      \"dependsOn\": [\n      ]\n    }\n  ],\n  \"outputs\": {\n    \"storageId\": {\n      \"type\": \"string\",\n      \"value\": \"[resourceId('Microsoft.Storage/storageAccounts', variables('storageAccountName'))]\"\n    },\n    \"storageAccountName\": {\n      \"type\": \"string\",\n      \"value\": \"[variables('storageAccountName')]\"\n    }\n  }\n}\n```\n\nNow, Feel the force, and see what you can create...",
    "lastmodified": "2023-04-13T22:40:03.124696259Z",
    "tags": [
      "ARM",
      "Azure",
      "Template",
      "Functions",
      "IaC"
    ]
  },
  "/posts/configuring-azure-web-application-firewall-powershell": {
    "title": "Configuring the Web Application Firewall with PowerShell.",
    "content": "\nMicrosoft Azure Application Gateway is a Layer 7 application delivery controller (ADC) offered as a service in Azure. It provides load balancing, SSL termination, end-to-end SSL, URL path-based routing, and basic web application firewall (WAF) functionality.\n\nWorking with the WAF, I usually build a basic configuration in the Portal before exporting the ARM JSON, which, then becomes my primary method to working on this service.\n\n\nOne of my biggest gripes with the Azure Firewall solutions currently is based on thier CRUD *(Create, Read, Update and Delete)* interface. It always  results in a workflow from hell, training along the lines of '*Edit, Save,* ***WAIT,***, *Edit, Save,* ***WAIT***' in a painful loop.  What should be a fast and straightforward configuration update, typically is a process that must be executed over many hours, preferably on a second screen.\n\n# Powershell\n\nWhile I spend a large amount of my working time sitting in VS code, with a terminal logged into Azure with both Powershell and Azure CLI, I do not every recall trying to work with the Application Gateway from this interface ever! \n\nI was asked to check the HTTP Timeout on one of the Firewalls I have access to, and send the details to a colleague; my first port of call was JSON, and then realized that this is a bit ugly for the request in hand.\n\nA quick PowerShell command should sort this out, which of course has even more Idiosyncrasy.\n\n\n## Working With Application Gateways in PowerShell\n\nWait for it (Remember my CRUD comment). Well, The PowerShell implementation is restricted by that same API limitations. The first step for updating any existing Gateway is to load the whole gateway configuration from Azure into a PowerShell object.\n\n```powershell\n$appGw = Get-AzApplicationGateway -Name p-ap1pub-waf01\n```\n\nFrom here, all the changes we make are to the PowerShell object `$appGw` until we are ready to commit the gateway back to Azure with the following command:\n\n```powershell\nSet-AzApplicationGateway -ApplicationGateway $appGw\n```\n\n### Making changes\n\nUnderstanding that all configurations are going to be applied to our in-memory object `$appGw`; we can use any of the available PowerShell commandlets to alter this object, and once ready to validate we must push these changes back to Azure with the `Set-AzApplicationGateway`\n\n\u003e Now you will understand why I ignore this rubbish and work with the ARM JSON!\n\nLet's take a swift tour of working with this object to establish an end-to-end SSL listener, which would be atypical of any implemented configuration.\n\n### Backend \n\n#### Backend Pool\n\nThe backend pool for our web servers can be created using FQDNs or IPs; I usually implement this initially using IP addresses.\n\n```powershell\nAdd-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\" -BackendIPAddresses \"192.168.####101\", \"192.168.1.102\"\n \n$appGwBackPool = Get-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\"\n```\n\n#### Backend SSL Encryption\n\nFrequently, we establish the connection as a genuine end-to-end encrypted connection; which implies that the Web Application Gateway should have an authentication certificate, so it only forward's traffic to a backend server if it has the expected SSL certificate. \n\nTo enable this, we must upload the public certificate used by the backend servers.\n\n```powershell\nAdd-AzApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\" -CertificateFile \".\\myAppPublicCertifcate.cer\"\n \n$appGwBackPoolCert = Get-AzureRmApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\"\n```\n\n#### Configure the Backend Service\n\nNow, we can configure the HTTPS Protocol and authentication certificate the WAF will use to validate the backend servers.\n\n```powershell\nAdd-AzApplicationGatewayBackendHttpSettings -ApplicationGateway $appGW -Name \"AppPoolHTTPS\" -Port 443 -Protocol Https -CookieBasedAffinity Enabled -AuthenticationCertificates $appGwBackPoolCert\n \n$appGwBackPoolHTTPS = Get-AzureRmApplicationGatewayBackendHttpSettings -ApplicationGateway $AppGW -Name \"AppPoolHTTPS\"\n```\n\n### Frontend\n\nNow we can switch focus on build the frontend configuration.\n\n#### A public SSL certificate.\n\nThe certificate should be in PFX format with a password. We add this to the gateway object as follows:\n\n```powershell\nAdd-AzApplicationGatewaySslCertificate -ApplicationGateway $appGw -Name \"FrontCert\" -CertificateFile \".\\myExternalFacingCertWithPrivateKey.pfx\" -Password \"myP@ssw0rd!\"\n \n$appGwFrontCert = Get-AzApplicationGatewaySslCertificate -ApplicationGateway $appGW -Name \"FrontCert\"\n```\n\n#### HTTPS Port (TCP 443)\n\nEnsure that we have TCP 443 allowed on our gateway.\n\n```powershell\nAdd-AzApplicationGatewayFrontendPort -ApplicationGateway $appGw -Name \"FrontHTTPS\" -Port “443”\n\n$appGwFrontPort = Get-AzApplicationGatewayFrontendPort -ApplicationGateway $appGw -Name \"FrontHTTPS\"\n```\n\n#### IP Configuration Object\n\nThe last part of this jigsaw is the Frontend IP Configuration.\n\n```powershell\n$appGwFrontIPConfig = Get-AzApplicationGatewayFrontendIPConfig -ApplicationGateway $AppGw -Name \"FrontIP\"\n```\n\n#### My Applications Listener\n\nNow, we can combine the parts above to create a listener object.\n\n```powershell\nAdd-AzApplicationGatewayHttpListener -ApplicationGateway $appGW -Name \"appListener\" -Protocol Https -FrontendIPConfiguration $appGwFrontIPConfig -FrontendPort $appGwFrontPort -HostName \"fqdn.myapp.site\" -RequireServerNameIndication true -SslCertificate $AppGwFrontCert\n \n$appGwListener = Get-AzureRmApplicationGatewayHttpListener -ApplicationGateway $appGW -Name \"appListener\"\n```\n\n### Routing Flow\n\nNow we glue these two concepts together and build the route from the listener to the server.\n\n```powershell\nAdd-AzApplicationGatewayRequestRoutingRule -ApplicationGateway $appGW -Name \"AppRule\" -RuleType basic -BackendHttpSettings $appGwBackPoolHTTPS -HttpListener $appGwListener -BackendAddressPool $appGwBackPool\n```\n\n### Publish\n\nThat's - Now we publish and test to see if we got it working, or messed up.\n\n```powershell\nSet-AzApplicationGateway -ApplicationGateway $appGw\n```\n\n## Reporting Configuration\n\nOk, Distraction aside, we started this post, as we needed to get a quick report on all the listeners configured, including the associated timeout each has currently defined.\n\nWith the knowledge of how to work with the App Gateway in PowerShell, this request is indeed trivial to complete\n\n```powershell\n\u003e $appGw.BackendHttpSettingsCollection | select name, port, protocol, requesttimeout\n\nName                             Port Protocol RequestTimeout\n----                             ---- -------- --------------\nhttp_t-www.mysite.com              80 Http                 20\nhttps_t-www.mysite.com            443 Https                20\n```",
    "lastmodified": "2023-04-13T22:40:03.116696162Z",
    "tags": [
      "Internet of Things"
    ]
  },
  "/posts/general_personal-reflection-2018": {
    "title": "Hitting Reset",
    "content": "\n\n10 Years, It is hard to believe that I have been posting thoughts here that long. And how so much has changed since I begun?\n\nI started this journey with the encouragement of some amazing people in Microsoft, as an opportunity to spread the news about *Hyper-V* and even more relevant at the time *System Center Virtual Machine Manager* which was still known by its code name!.\n\u003c!--more--\u003e\nMy daily experience with this application, Windows Server, and real-world enterprise issues; positioned me at one of the leading edges of Microsoft Technologies; and fully armed with a true business driver pushing forward. Wounds and pains exposed, I gained a lot of insight to the digital plumbing of these technologies and as a result of a lot of fantastic information to share - sometimes not good news; but never the less - reality.\n\nI have primarily worked in the mindset that when I find an issue to be addressed, before sharing, escalating or attacking - I need to stop and consider solutions; which normally result in a more constructive and progressive approach to unblocking my path. That ethos spans back to my days working as what would be considered today in 2018 as an *IoT* architect; but 20 years ago in a Rubber Molding plant, The Operations Manager always reminded me as I entered his office, \"If you don't have some suggestion for a solution before entering and presenting a problem, leave now, and come back when I am prepared (But don't spend all day - Problems cost money!).\"\n\nIn Hindsight, this approach challenged my limits every day; but I now also realise that he actually had no technical knowledge, and without my suggestions, we were heading the route of the Titanic!\n\n## System Center\n\nWhile the solution itself continues to live on, as clear from the very recent launch of *System Center 2019* at the Ignite Conference in Florida; My own passion and engagement with this technology has ultimately diminished to a point of history., despite co-authoring and technically reviewing a number of books, speaking at so many events, and investing 1000's of hours.\n\n## Personal Redevelopment\n\nAfter almost 20 years I changed Jobs, A decision which was extremely difficult to make; and honestly post that change point; I took at least 6 more months to adjust to the new world order.\n\nI found myself amidst a team of like-minded peers, left to find a niche which I could own. Despite working with fantastic scenarios, these new challenges were amazing; and I was learning new stuff again. But, yet I still felt uninspired.\n\nJust reflect on the number of blog posts I have published in the last 3 years.\n\nMy personal life also took a major change; and today after celebrating 21 years of marriage; I am a super proud father of two amazing girls, with my oldest just after celebrating her 4 birthday and the youngest just turned 2.\n\nWhen I reflect on these massive changes, it is a totally different world from when I stood just 10 years ago.\n\n## Inspiration\n\nLast week I participated in my first **Microsoft Ignite** event; and spent the vast majority of this opportunity meeting with so many old friends who have also evolved into completely new roles within their organizations.\n\nAs an example, Mr Taylor Brown; I had the honour of meeting Taylor for the first time almost 12 years ago. Back then we both were working on Test Scenarios for Hyper-V; in his role, he ran the labs for Microsoft's internal testing; and I was responsible for our internal *Technology Adoption Program (TAP)* Pilot testing. Today, Taylor owns the *Docker (Container)* features in Windows Server. An amazing achievement, from an inspiring person and a good friend. \n\nThere are so many amazing people, with just as amazing stories; and I was so proud to be able to stop, and say hello to these *icons*, and learn how their lives have also changed.\n\n## Governance\n\nAs the adoption, and practices of Cloud become centrally focused for so many organizations the focus shifts left, as Compliance, Control, and Culture changes ignite to enable a completely fresh view of the potential.\n\nWhen I combine past experience, with the foundational tooling which 3rd parties like Terraform offer, and native tooling which Microsoft adding to the core of their offerings, the next challenge is clear.\n\nNow, I see a clear path to assist organizations of any size to evolve from what might have been the chaos of Shadow IT, the central control of IT, or the old practices of Enterprise Architects; and guide them to a culture driven enablement of Cloud; supported with *Governance* and enabling that missing **trust** thought the use of *Safety Guard Rails*\n\n## Finding my Mojo\n\nLooking back at Ignite and the last 12 months of work; I now feel like I have a new rhythm. I have found a new passion - Namely that of enabling organizations on this transformation trough common sense, debate and technology.\n\nAddressing political issues, and provisioning the structures of support required to encourage trust and co-operation, all of which is based on logical technical foundations *(Yes Mr Spock!)*. \n\nEven more rewarding, I am actually observing directly the impact this has on people and processes as they evolve their life's with the culture changes required to start a new fabric of growth in cloud and DevOps practices; while also addressing their relationships with the business owners.\n\nI believe I have found my *Mojo*.\n\n## The Next Step...\n\nOver the last years, I have had the honour of presenting at so many fantastic conferences, delivering workshops, and engagement in meetups. When I reflect on the topics I have focused on these all have contributed to building strong foundational elements to this new way. \n\nTopics ranging from Containerization and Automation with Docker and Kubernetes, Serverless to AI, BOT Frameworks to Python, Git flow to Infrastructure as Code, DevOps to Event Handling; while all feel very disjointed, these technologies combined are core to understanding how the world is evolving, and therefore how the organizations can adopt.\n\nIts time to **Hit Refresh**, and join me on this next wave, as I share, present, document, and offer guidance; both here on the Blog, On stages in various locations and professionally, through a range of mediums from 'hands-on' demonstrations, Technical guides, papers, and talks.\n\nI have my *mojo* charged and ready; Have You?",
    "lastmodified": "2023-04-13T22:40:03.140696452Z",
    "tags": [
      "Career",
      "MVP",
      "Cisco Champion"
    ]
  },
  "/posts/iac_bicep_tags-as-parameters": {
    "title": "Bicep - Tags as Parameters",
    "content": "\nIn an earlier post, we look at using [ARM](ARM) to lookup the value of Tags' at both at the Subscription and Resource Level.\n\nWith Bicep this is much easier to understand. This is the same lab configuration as in the original post, but this time to code should be a lot more readable.\n\n```powershell\n// Sample to lookup tag values \n// Both Subscription and Resource Level\n\n@description('The resource ID of the resource we wish to look up a tag from.')\nparam sourceResourceId string = '/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001'\n\n\n// Variables to reference the Tags Resource Providers\n\n// Subscription Tags Resource Provider\nvar referenceSubscriptionTagsResourceId = '/subscriptions/${subscription().subscriptionId}/providers/Microsoft.Resources/tags/default'\n\n// Resource Tags Resource Provider\nvar referenceResourceTagsResourceId = '${sourceResourceId}/providers/Microsoft.Resources/tags/default'\nvar referenceTagsApi = '2020-06-01'\n\n\n// \n// Lookup the tags and return the value\n// \n\n// Subscription Tags\noutput subscriptionRecoveryVaultRGTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG\noutput subscriptionRecoveryVaultTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault\n\n// Resource Tags\noutput resourceRecoveryPolicyTag string = reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy\n\n\n// Concatanation of the outputs to build Resource Ids\n\noutput recoveryVaultId string = resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault)\noutput recoveryPolicyId string = '${resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault)}/backupPolicies/${reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy}'\n\n```",
    "lastmodified": "2023-04-13T22:40:03.132696355Z",
    "tags": [
      "ARM",
      "Azure",
      "Template",
      "Functions",
      "IaC"
    ]
  },
  "/posts/sw_active-directory_administrative-limits": {
    "title": "AD - Administrative Limits",
    "content": "\nI recently ran into an issue with a particular environment where [Active Directory](Active Directory) and [PKI](PKI) services were deployed. One of the service accounts which I was attempting to 'unlock' refused to co-operate and instead offered the most unhelpful message.\n \n`Administrative limit for this request was exceeded` - this was not my first time encountering the message, previously this haunted me while I was managing a Windows PKI infrastructure and with some quick searches, confirmed my initial suspicion. This is a symptom of an AD object quite simply being to large!\n\n## Explaination\n\nActive Directory is in the simplest form a Database, and each object can be considered as a row in this database. Like any database there are limits to what data can be stored in the table, the type of data, but also the amount. \n## Resolving the problem\n\nUsing a tool like **ADSI Edit** will allow you to open the database, and select the schema partition we would like to work with. Standard objects which we can see in the *Active Directory Users and Computers* extension are all hosted in the partition called the *Default Naming Context*; which is where we need to focus for this problem\n\n![ADSI Edit - Default Naming Context](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-10-55-18.png)\n\n### Identifying the Bloat\n\nThere is no super cool trick here, we just need a little investigation. Start by navigating the OU structure you are familiar with in Active Directory Users and Computers, to locate the account with issues.\n\n![AD Object Navigation in ADSI Edit](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-11-00-37.png)\n\nRight Click on the object, and select **Properties** where you now get to view the 100's of columns of data which AD stores for each object. \n\nClick on the **Filter** button and enable the option *Show only attributes that have values* - this will remove all the columns which are empty - and therefore not adding to our overside object!\n\n![ADSI Properties Filter](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-11-03-48.png)\n\nNow, you can check each of the remaining attributes (columns) and see what they contain, I'll save a few minutes, and in this case I expect my issue is to do with Certificates, so we will focus on the attribute called *userCertificate* which contains an collection of *[byte[]]*, each representing a Base64 encoded Certificate.\n\nBINGO! looking at this property I can see 100's of entries, and each one has the same *byte[]* string, which implies the same certificate is loaded into the object a lot of times.\n\nI am going to flush the lot of them out, as these are added by the system, and it will reload them as required; but for now, I am simply the garbage collector.\n\nUsing the UX, we can delete these 1 at a time, so we need a little automation, or the rest of the week will be wasted.\n\n## Powershell Helper\n\nUsing standard Active Directory module for Powershell, we can get a reference to the object which needs to be flushed\n\n```powershell\n$certUser = Get-Aduser \"svc_PKI_NDES\" -Properties certificates\n```\n\nNow, let's check what we got from AD\n\n```powershell\n$certuser.Certificates.Count\n1169\n```\n\nRight, time to fix this, we will use the following command to remove all the certificates from the object\n\n```powershell\nPS C:\\Users\\sysadmin.AD\u003e set-ADUser svc_PKI_NDES -Certificates @{}\n```\n\nAnd, let's confirm it worked\n```powershell\n$certuser = Get-ADUser svc_PKI_NDES -Properties Certificates\n$certuser.Certificates.count\n0\n```\n\nAnd now, just check that you can unlock the account, or complete the operation that sent you chasing this wild goose!",
    "lastmodified": "2023-04-13T22:40:03.124696259Z",
    "tags": [
      "AD"
    ]
  }
}