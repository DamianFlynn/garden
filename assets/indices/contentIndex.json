{
  "/posts/general_personal-reflection-2018": {
    "title": "Hitting Reset",
    "content": "\n\n10 Years, It is hard to believe that I have been posting thoughts here that long. And how so much has changed since I begun?\n\nI started this journey with the encouragement of some amazing people in Microsoft, as an opportunity to spread the news about *Hyper-V* and even more relevant at the time *System Center Virtual Machine Manager* which was still known by its code name!.\n\u003c!--more--\u003e\nMy daily experience with this application, Windows Server, and real-world enterprise issues; positioned me at one of the leading edges of Microsoft Technologies; and fully armed with a true business driver pushing forward. Wounds and pains exposed, I gained a lot of insight to the digital plumbing of these technologies and as a result of a lot of fantastic information to share - sometimes not good news; but never the less - reality.\n\nI have primarily worked in the mindset that when I find an issue to be addressed, before sharing, escalating or attacking - I need to stop and consider solutions; which normally result in a more constructive and progressive approach to unblocking my path. That ethos spans back to my days working as what would be considered today in 2018 as an *IoT* architect; but 20 years ago in a Rubber Molding plant, The Operations Manager always reminded me as I entered his office, \"If you don't have some suggestion for a solution before entering and presenting a problem, leave now, and come back when I am prepared (But don't spend all day - Problems cost money!).\"\n\nIn Hindsight, this approach challenged my limits every day; but I now also realise that he actually had no technical knowledge, and without my suggestions, we were heading the route of the Titanic!\n\n## System Center\n\nWhile the solution itself continues to live on, as clear from the very recent launch of *System Center 2019* at the Ignite Conference in Florida; My own passion and engagement with this technology has ultimately diminished to a point of history., despite co-authoring and technically reviewing a number of books, speaking at so many events, and investing 1000's of hours.\n\n## Personal Redevelopment\n\nAfter almost 20 years I changed Jobs, A decision which was extremely difficult to make; and honestly post that change point; I took at least 6 more months to adjust to the new world order.\n\nI found myself amidst a team of like-minded peers, left to find a niche which I could own. Despite working with fantastic scenarios, these new challenges were amazing; and I was learning new stuff again. But, yet I still felt uninspired.\n\nJust reflect on the number of blog posts I have published in the last 3 years.\n\nMy personal life also took a major change; and today after celebrating 21 years of marriage; I am a super proud father of two amazing girls, with my oldest just after celebrating her 4 birthday and the youngest just turned 2.\n\nWhen I reflect on these massive changes, it is a totally different world from when I stood just 10 years ago.\n\n## Inspiration\n\nLast week I participated in my first **Microsoft Ignite** event; and spent the vast majority of this opportunity meeting with so many old friends who have also evolved into completely new roles within their organizations.\n\nAs an example, Mr Taylor Brown; I had the honour of meeting Taylor for the first time almost 12 years ago. Back then we both were working on Test Scenarios for Hyper-V; in his role, he ran the labs for Microsoft's internal testing; and I was responsible for our internal *Technology Adoption Program (TAP)* Pilot testing. Today, Taylor owns the *Docker (Container)* features in Windows Server. An amazing achievement, from an inspiring person and a good friend. \n\nThere are so many amazing people, with just as amazing stories; and I was so proud to be able to stop, and say hello to these *icons*, and learn how their lives have also changed.\n\n## Governance\n\nAs the adoption, and practices of Cloud become centrally focused for so many organizations the focus shifts left, as Compliance, Control, and Culture changes ignite to enable a completely fresh view of the potential.\n\nWhen I combine past experience, with the foundational tooling which 3rd parties like Terraform offer, and native tooling which Microsoft adding to the core of their offerings, the next challenge is clear.\n\nNow, I see a clear path to assist organizations of any size to evolve from what might have been the chaos of Shadow IT, the central control of IT, or the old practices of Enterprise Architects; and guide them to a culture driven enablement of Cloud; supported with *Governance* and enabling that missing **trust** thought the use of *Safety Guard Rails*\n\n## Finding my Mojo\n\nLooking back at Ignite and the last 12 months of work; I now feel like I have a new rhythm. I have found a new passion - Namely that of enabling organizations on this transformation trough common sense, debate and technology.\n\nAddressing political issues, and provisioning the structures of support required to encourage trust and co-operation, all of which is based on logical technical foundations *(Yes Mr Spock!)*. \n\nEven more rewarding, I am actually observing directly the impact this has on people and processes as they evolve their life's with the culture changes required to start a new fabric of growth in cloud and DevOps practices; while also addressing their relationships with the business owners.\n\nI believe I have found my *Mojo*.\n\n## The Next Step...\n\nOver the last years, I have had the honour of presenting at so many fantastic conferences, delivering workshops, and engagement in meetups. When I reflect on the topics I have focused on these all have contributed to building strong foundational elements to this new way. \n\nTopics ranging from Containerization and Automation with Docker and Kubernetes, Serverless to AI, BOT Frameworks to Python, Git flow to Infrastructure as Code, DevOps to Event Handling; while all feel very disjointed, these technologies combined are core to understanding how the world is evolving, and therefore how the organizations can adopt.\n\nIts time to **Hit Refresh**, and join me on this next wave, as I share, present, document, and offer guidance; both here on the Blog, On stages in various locations and professionally, through a range of mediums from 'hands-on' demonstrations, Technical guides, papers, and talks.\n\nI have my *mojo* charged and ready; Have You?",
    "lastmodified": "2023-06-28T11:24:29.118792055Z",
    "tags": [
      "Career",
      "MVP",
      "Cisco Champion"
    ]
  },
  "/posts/iac_bicep_tags-as-parameters": {
    "title": "Bicep - Tags as Parameters",
    "content": "\nIn an earlier post, we look at using [ARM](ARM) to lookup the value of Tags' at both at the Subscription and Resource Level.\n\nWith Bicep this is much easier to understand. This is the same lab configuration as in the original post, but this time to code should be a lot more readable.\n\n```powershell\n// Sample to lookup tag values \n// Both Subscription and Resource Level\n\n@description('The resource ID of the resource we wish to look up a tag from.')\nparam sourceResourceId string = '/subscriptions/547d54ea-411b-459e-b6f8-b3cc5e84c535/resourceGroups/p-vm/providers/Microsoft.Compute/virtualMachines/p-vm001'\n\n\n// Variables to reference the Tags Resource Providers\n\n// Subscription Tags Resource Provider\nvar referenceSubscriptionTagsResourceId = '/subscriptions/${subscription().subscriptionId}/providers/Microsoft.Resources/tags/default'\n\n// Resource Tags Resource Provider\nvar referenceResourceTagsResourceId = '${sourceResourceId}/providers/Microsoft.Resources/tags/default'\nvar referenceTagsApi = '2020-06-01'\n\n\n// \n// Lookup the tags and return the value\n// \n\n// Subscription Tags\noutput subscriptionRecoveryVaultRGTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG\noutput subscriptionRecoveryVaultTag string = reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault\n\n// Resource Tags\noutput resourceRecoveryPolicyTag string = reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy\n\n\n// Concatanation of the outputs to build Resource Ids\n\noutput recoveryVaultId string = resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault)\noutput recoveryPolicyId string = '${resourceId(subscription().subscriptionId, reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVaultRG, 'Microsoft.RecoveryServices/vaults', reference(referenceSubscriptionTagsResourceId, referenceTagsApi).tags.recoveryVault)}/backupPolicies/${reference(referenceResourceTagsResourceId, referenceTagsApi).tags.recoveryPolicy}'\n\n```",
    "lastmodified": "2023-06-28T11:24:29.118792055Z",
    "tags": [
      "ARM",
      "Azure",
      "Template",
      "Functions",
      "IaC"
    ]
  },
  "/posts/iac_powershell_azure-web-application-firewall": {
    "title": "Configuring the Web Application Firewall with PowerShell",
    "content": "\nMicrosoft Azure Application Gateway is a Layer 7 application delivery controller (ADC) offered as a service in Azure. It provides load balancing, SSL termination, end-to-end SSL, URL path-based routing, and basic web application firewall (WAF) functionality.\n\nWorking with the WAF, I usually build a basic configuration in the Portal before exporting the ARM JSON, which, then becomes my primary method to working on this service.\n\n## Why JSON you may ask... \n\nOne of my biggest gripes with the Azure Firewall solutions currently is based on thier CRUD *(Create, Read, Update and Delete)* interface. It always  results in a workflow from hell, training along the lines of '*Edit, Save,* ***WAIT,***, *Edit, Save,* ***WAIT***' in a painful loop.  What should be a fast and straightforward configuration update, typically is a process that must be executed over many hours, preferably on a second screen.\n\n## Powershell\n\nWhile I spend a large amount of my working time sitting in VS code, with a terminal logged into Azure with both Powershell and Azure CLI, I do not every recall trying to work with the Application Gateway from this interface ever! \n\nI was asked to check the HTTP Timeout on one of the Firewalls I have access to, and send the details to a colleague; my first port of call was JSON, and then realized that this is a bit ugly for the request in hand.\n\nA quick PowerShell command should sort this out, which of course has even more Idiosyncrasy.\n\n\n## Working With Application Gateways in PowerShell\n\nWait for it (Remember my CRUD comment). Well, The PowerShell implementation is restricted by that same API limitations. The first step for updating any existing Gateway is to load the whole gateway configuration from Azure into a PowerShell object.\n\n```powershell\n$appGw = Get-AzApplicationGateway -Name p-ap1pub-waf01\n```\n\nFrom here, all the changes we make are to the PowerShell object `$appGw` until we are ready to commit the gateway back to Azure with the following command:\n\n```powershell\nSet-AzApplicationGateway -ApplicationGateway $appGw\n```\n\n### Making changes\n\nUnderstanding that all configurations are going to be applied to our in-memory object `$appGw`; we can use any of the available PowerShell commandlets to alter this object, and once ready to validate we must push these changes back to Azure with the `Set-AzApplicationGateway`\n\n\u003e Now you will understand why I ignore this rubbish and work with the ARM JSON!\n\nLet's take a swift tour of working with this object to establish an end-to-end SSL listener, which would be atypical of any implemented configuration.\n\n### Backend \n\n#### Backend Pool\n\nThe backend pool for our web servers can be created using FQDNs or IPs; I usually implement this initially using IP addresses.\n\n```powershell\nAdd-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\" -BackendIPAddresses \"192.168.####101\", \"192.168.1.102\"\n \n$appGwBackPool = Get-AzApplicationGatewayBackendAddressPool -ApplicationGateway $appGW -Name \"AppPool\"\n```\n\n#### Backend SSL Encryption\n\nFrequently, we establish the connection as a genuine end-to-end encrypted connection; which implies that the Web Application Gateway should have an authentication certificate, so it only forward's traffic to a backend server if it has the expected SSL certificate. \n\nTo enable this, we must upload the public certificate used by the backend servers.\n\n```powershell\nAdd-AzApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\" -CertificateFile \".\\myAppPublicCertifcate.cer\"\n \n$appGwBackPoolCert = Get-AzureRmApplicationGatewayAuthenticationCertificate -ApplicationGateway $appGW -Name \"AppPoolPublicCert\"\n```\n\n#### Configure the Backend Service\n\nNow, we can configure the HTTPS Protocol and authentication certificate the WAF will use to validate the backend servers.\n\n```powershell\nAdd-AzApplicationGatewayBackendHttpSettings -ApplicationGateway $appGW -Name \"AppPoolHTTPS\" -Port 443 -Protocol Https -CookieBasedAffinity Enabled -AuthenticationCertificates $appGwBackPoolCert\n \n$appGwBackPoolHTTPS = Get-AzureRmApplicationGatewayBackendHttpSettings -ApplicationGateway $AppGW -Name \"AppPoolHTTPS\"\n```\n\n### Frontend\n\nNow we can switch focus on build the frontend configuration.\n\n#### A public SSL certificate.\n\nThe certificate should be in PFX format with a password. We add this to the gateway object as follows:\n\n```powershell\nAdd-AzApplicationGatewaySslCertificate -ApplicationGateway $appGw -Name \"FrontCert\" -CertificateFile \".\\myExternalFacingCertWithPrivateKey.pfx\" -Password \"myP@ssw0rd!\"\n \n$appGwFrontCert = Get-AzApplicationGatewaySslCertificate -ApplicationGateway $appGW -Name \"FrontCert\"\n```\n\n#### HTTPS Port (TCP 443)\n\nEnsure that we have TCP 443 allowed on our gateway.\n\n```powershell\nAdd-AzApplicationGatewayFrontendPort -ApplicationGateway $appGw -Name \"FrontHTTPS\" -Port “443”\n\n$appGwFrontPort = Get-AzApplicationGatewayFrontendPort -ApplicationGateway $appGw -Name \"FrontHTTPS\"\n```\n\n#### IP Configuration Object\n\nThe last part of this jigsaw is the Frontend IP Configuration.\n\n```powershell\n$appGwFrontIPConfig = Get-AzApplicationGatewayFrontendIPConfig -ApplicationGateway $AppGw -Name \"FrontIP\"\n```\n\n#### My Applications Listener\n\nNow, we can combine the parts above to create a listener object.\n\n```powershell\nAdd-AzApplicationGatewayHttpListener -ApplicationGateway $appGW -Name \"appListener\" -Protocol Https -FrontendIPConfiguration $appGwFrontIPConfig -FrontendPort $appGwFrontPort -HostName \"fqdn.myapp.site\" -RequireServerNameIndication true -SslCertificate $AppGwFrontCert\n \n$appGwListener = Get-AzureRmApplicationGatewayHttpListener -ApplicationGateway $appGW -Name \"appListener\"\n```\n\n### Routing Flow\n\nNow we glue these two concepts together and build the route from the listener to the server.\n\n```powershell\nAdd-AzApplicationGatewayRequestRoutingRule -ApplicationGateway $appGW -Name \"AppRule\" -RuleType basic -BackendHttpSettings $appGwBackPoolHTTPS -HttpListener $appGwListener -BackendAddressPool $appGwBackPool\n```\n\n### Publish\n\nThat's - Now we publish and test to see if we got it working, or messed up.\n\n```powershell\nSet-AzApplicationGateway -ApplicationGateway $appGw\n```\n\n## Reporting Configuration\n\nOk, Distraction aside, we started this post, as we needed to get a quick report on all the listeners configured, including the associated timeout each has currently defined.\n\nWith the knowledge of how to work with the App Gateway in PowerShell, this request is indeed trivial to complete\n\n```powershell\n\u003e $appGw.BackendHttpSettingsCollection | select name, port, protocol, requesttimeout\n\nName                             Port Protocol RequestTimeout\n----                             ---- -------- --------------\nhttp_t-www.mysite.com              80 Http                 20\nhttps_t-www.mysite.com            443 Https                20\n```",
    "lastmodified": "2023-06-28T11:24:29.110792086Z",
    "tags": [
      "Powershell",
      "Azure Web Application Gateway"
    ]
  },
  "/posts/ssg_azure-static-sites-and-cdn": {
    "title": "Hosting Static Sites in Azure with CDN",
    "content": "\nHosting my site on Wordpress was not super complex; I leveraged the Azure PaaS Services for Web Apps, and orginally the 3rd party support for hosted MySQL database's. Once I was up and running I quickly realised that all media hosted on the site were landing on the webserver, so a plugin from its marketplace offered the ability to relocate the media to an Azure Blob; offloading some of the challanges.\n\nHosting this was not free, while I could have leveraged the Free Webserver option it did not take a lot of load for this to be causing some unacceptable performace issues; which when combined with a hosted MySQL service which also was not going to be super fast and was limited in the database size; which became event more of a problem when an attach on the site would result in 1000's of useless comments filling the database to capacity and taking the site down as a direct result.\n\n## Static Site Hosting\n\nNot a lot has to change as we move to the static side model; The web server is still needed to host the side; but the database component is no longer a concern with this approach.\n\nHowever, The cloud is a beautiful thing, and there are so many more ways to reach you goal, and while we are focused, we can complete a lot more for a lot less!\n\nFor this site I have chosen to run a lean cost model, while providing a super responsive experience to you, my readers and subscribers.\n\n### Blob Storage Foundations\n\nUsing the standard **Azure Blob Storage** offering, I simply copy over the generated HTML site to the account. Assuming the blob is exposed to the public its content is available as a HTTPS endpoint; which simply put is a website.\n\nBut, alone this is not enough; why? because how would I address requested for pages which do not exist for example; I would not want an ugly HTTP 404 error page to be rendered, but instead a nice response to offer a search, navigation, or other more professional experience.\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"#{GITVERSION_FullSemVer}#\",\n  \"parameters\": {\n    \"name\": {\n      \"type\": \"String\"\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"[resourceGroup().location]\"\n    },\n    \"contactEmail\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"info@damianflynn.com\"\n    },\n    \"projectName\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"Static Site Hosting\"\n    },\n    \"environment\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Production\",\n        \"Test\",\n        \"Developer\",\n        \"Proof of concept\"\n      ],\n      \"defaultValue\": \"Developer\",\n      \"metadata\": {\n        \"description\": \"Type of environment\"\n      }\n    }\n  },\n\n  \"variables\": {\n    \"storageName\": \"[tolower(take(concat('blob',parameters('name'),uniqueString(resourceGroup().id)),23))]\"\n  },\n\n  \"resources\": [\n\n    {\n      \"comments\": \"Storage Account for Static Site Content\",\n      \"name\": \"[variables('storageName')]\",\n      \"apiVersion\": \"2018-07-01\",\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"sku\": {\n        \"name\": \"Standard_LRS\",\n        \"tier\": \"Standard\"\n      },\n      \"kind\": \"StorageV2\",\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"properties\": {\n        \"networkAcls\": {\n          \"bypass\": \"AzureServices\",\n          \"virtualNetworkRules\": [],\n          \"ipRules\": [],\n          \"defaultAction\": \"Allow\"\n        },\n        \"supportsHttpsTrafficOnly\": true,\n        \"encryption\": {\n          \"services\": {\n            \"file\": {\n              \"enabled\": true\n            },\n            \"blob\": {\n              \"enabled\": true\n            }\n          },\n          \"keySource\": \"Microsoft.Storage\"\n        },\n        \"accessTier\": \"Hot\"\n      },\n      \"dependsOn\": []\n    }\n\n  ]\n}\n```\n\nTo achieve this I need some HTTP routing options\n\n### House Warming, Inviting Guests\n\nIt is not hard to find a list of potential solutions in Azure to fill this role; We could use a WebApp, API Managment, or event Azure Functions, or the Azure Functions proxy features. However, we do not actually need to be very creative; as Microsoft have listened to uservoice, MVPs and customers all calling for some better web publishing support for blob storage, especially given AWS have features in thier S3 offer that work simply and effectively.\n\nTo this end we will take a look at a feature which at the time of writing is still in preview called **Static Website** (You would have never guessed right!)\n\nAll we need do, is set this feature as *Enabled* and define where visitors should be routed to for the *Index Document* and the *Error Document*, and apply the changes. This simple feature makes our site behave just as we would like; and it is *free!*.\n\n\u003e Note: Due to Preview Status, we currently have no ARM Template settings for enabling and configuring this setting. Therefore for *Infrasructure as Code* we will fall back to *Azure CLI*\n\n```bash\naz extension add --name storage-preview\n\naz storage blob service-properties update '\n   --account-name \u003cACCOUNT_NAME\u003e\n   --static-website \n   --404-document \u003cERROR_DOCUMENT_NAME\u003e \n   --index-document \u003cINDEX_DOCUMENT_NAME\u003e\n\naz storage account show -n \u003cACCOUNT_NAME\u003e -g \u003cRESOURCE_GROUP\u003e --query \"primaryEndpoints.web\" --output tsv\n```\n\n## Foundation for Global Scale\n\nBut why stop here, we can go a little further; I want this site to be responsive for you, regardless of where you are located on the planet; afer all we are all embracing the cloud and need to learn and share; so how better to acomplish this, simply leverage another feature from the Azure arsnal of services.\n\nCalling out the *Azure Content Delivery Network* (Azure CDN) we can take advantage, or any one of three platforms \n\n* Verizon's Global Network\n* Akamai CDN Platform\n* Microsofts Azure's Footprint\n\nThe choice you make here really will come down to what you want in the list of features and the budget you have in mind; but for the purpose of this blog; I have for now chosen to run with the option of **Premium Verizon**!\n\nWhy? Well It is the most feature rich of the offerings currently, and also the most expensive; which still should not cost me more then 3 or 4 euro a month; which is a lot less than I was paying for my Wordpress site. Give me a month or two and I will share thee exact costs of this solution so you can appreciate the value offered by Azure PaaS offerings.\n\n\n```json\n{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"#{GITVERSION_FullSemVer}#\",\n  \"parameters\": {\n    \"name\": {\n      \"type\": \"String\"\n    },\n    \"cdnEndpointTargetUrl\": {\n      \"type\": \"String\"\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"[resourceGroup().location]\"\n    },\n    \"contactEmail\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"info@damianflynn.com\"\n    },\n    \"projectName\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"Static Site Hosting\"\n    },\n    \"environment\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"Production\",\n        \"Test\",\n        \"Developer\",\n        \"Proof of concept\"\n      ],\n      \"defaultValue\": \"Developer\",\n      \"metadata\": {\n        \"description\": \"Type of environment\"\n      }\n    }\n  },\n\n  \"variables\": {\n    \"cdnName\": \"[concat(parameters('name'),'-cdn-',parameters('environment'))]\",\n    \"cdnEndpoint\": \"[concat(parameters('name'),'-cde-',parameters('environment'))]\",\n    \"cdnEndpointName\": \"[concat(variables('cdnName'), '/',   variables('cdnEndpoint'))]\",\n    \"cdnEndpointOriginName\": \"[concat(variables('cdnEndPointName'), '/','Origin')]\"\n  },\n\n  \"resources\": [\n    \n    {\n      \"comments\": \"Azure CDN Service.\",\n      \"name\": \"[variables('cdnName')]\",\n      \"apiVersion\": \"2016-04-02\",\n      \"type\": \"Microsoft.Cdn/profiles\",\n      \"sku\": {\n        \"name\": \"Premium_Verizon\"\n      },\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"dependsOn\": []\n    },\n\n    {\n      \"comments\": \"Azure CDN Endpoint\",\n      \"name\": \"[variables('cdnEndpointName')]\",\n      \"apiVersion\": \"2016-04-02\",\n      \"type\": \"Microsoft.Cdn/profiles/endpoints\",\n      \"location\": \"[parameters('location')]\",\n      \"tags\": {\n        \"Contact\": \"[parameters('contactEmail')]\",\n        \"Project\": \"[parameters('projectName')]\",\n        \"Environment\": \"[parameters('environment')]\"\n      },\n      \"scale\": null,\n      \"properties\": {\n        \"originHostHeader\": \"[parameters('cdnEndpointTargetURL')]\",\n        \"isHttpAllowed\": true,\n        \"isHttpsAllowed\": true,\n        \"queryStringCachingBehavior\": \"NotSet\",\n        \"originPath\": null,\n        \"origins\": [\n          {\n            \"name\": \"[variables('cdnEndpoint')]\",\n            \"properties\": {\n              \"hostName\": \"[parameters('cdnEndpointTargetURL')]\",\n              \"httpPort\": 80,\n              \"httpsPort\": 443\n            }\n          }\n        ],\n        \"contentTypesToCompress\": [],\n        \"isCompressionEnabled\": false\n      },\n      \"dependsOn\": [\n        \"[resourceId('Microsoft.Cdn/profiles', variables('cdnName'))]\"\n      ]\n    }\n\n  ]\n}\n```\n\nThe template will setup out CDN environment, all we need to provide is a name for the CDN service, and the URI to the Static Site which we established in the previous stage, for example this may appear as `blobwebaddress.z00.web.core.windows.net` \n\n## Adding Content\n\nWith very little effort, and cost, we now have established a foundation which can serve content efficiently globally, Our next challage will be to deploy our site to this foundation in a easy manner",
    "lastmodified": "2023-06-28T11:24:29.114792071Z",
    "tags": [
      "ARM",
      "Azure CDN",
      "Azure Storage",
      "Azure Static Website"
    ]
  },
  "/posts/ssg_azure_devops_and_jekyll": {
    "title": "Publish Static Site Content using Azure DevOps and Jekyll",
    "content": "\nWith the heavy lifting done in creating the site building mechanics and a solid foundation to build and share upon; our final objective is to automate the process of connecting these two stages.\n\n## Release Pipeline\n\nTechnically the goal we are speaking about is the **Release Pipeline** which will take the artefact *(our site .ZIP file)* that we created in the **Build Pipeline** in our previous topic [Constructing a new Home with Jekyll and Azure DevOps](posts/ssg_building_using_jekyll); and publish this to our storage account.\n\nIn simple terms, we are going to Unzip the archive to the storage, which is configured to expose the content as a website, and to increase performance we are leveraging a content delivery network.\n\n### Copying Content\n\nAs with our *Build Pipeline* there are a few different options on how to accomplish the work. \n\n\u003e Currently Azure DevOps is not exposing the release pipelines in an *Infrastructure as Code* configuration so we will complete this in the portal.\n\nWe begin with a new *Release Pipeline*, add add the task **Azure File Copy**, and set the settings similar to the following:\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Azure Blob File Copy\n|Source                | $(System.DefaultWorkingDirectory)/_Jekyll Builder/_site\n|Azure Connection Type | Azure Resource Manager\n|Azure Subscription    | My Azure Subscription\n|Destination Type      | Azure Blob\n|Storage Account       | Name of the Storage Account you created, e.g. My Blog\n|Container Name        | $web\n\n\nDone!, Seriously; pretty easy right!\n\n### Replace Content\n\nOne small issue with the previous approach is that it merely overwrites the content in the blob with the latest version, but it failed to remove any content which might have been removed from the site.\n\nCurrently, there is no equivalent to the magic *XCOPY* command that will sync the blob removing the data that is no longer required; that's a project for another day.\n\nThe quick and dirty fix for this scenario is first to delete the existing content and then deploy the latest version. This would typically result in a potential outage as the content vanishes for a little time; however, we do not have that concern, because we have decided to use a Content Delivery Network which caches our site.\n\n#### Task 1\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Delete Old Files\n|Azure Subscription    | My Azure Subscription\n|Script Location       | Inline\u003cbr\u003e`az storage blob delete-batch --source $web --account-name $(storageAccount) --output table`\n\n#### Task 2\n\n|Setting               | Value |\n|---|---|\n|Display Name          | Upload Fresh Content\n|Azure Subscription    | My Azure Subscription\n|Script Location       | Inline\u003cbr\u003e`az storage blob upload-batch --source _site --destination $web --account-name $(storageAccount) --output table --no-progress`\n\n### Tiggers Ready\n\nThe last point to consider is *When* should this deployment happen, and that's also pretty obvious when we think about it.\n\nEvery time we have a good build of the site, we should check to see which branch just completed the process, and update the relevant site based on this information.\n\n![Release Pipeline Flow](ssg_azure_devops_and_jekyll/opps-missing-image.png)\n\n## Summary\n\nWhich flow you choose to implement are entirely at your discretion; As I am currently doing much work on the taxonomy of the site; I have implemented the second option of delete and redeploy; but I do in the plan to come back and optimize this by adding a sync process which will be far more efficient.",
    "lastmodified": "2023-06-28T11:24:29.118792055Z",
    "tags": [
      "Jekyll",
      "Azure DevOps",
      "Azure Static Website"
    ]
  },
  "/posts/ssg_building_using_jekyll": {
    "title": "Constructing a new Home with Jekyll and Azure DevOps",
    "content": "\nOne of the unspoken truths behind the lack of posts in recent history was due to a few bugs, which in the end resulted in an experience where from home it appeared that any new content was published and working; but outside this fortress in the real world, there was a large silence echoing.\n\nI really only discovered this issue in May of this year, and was, to say the least, a little agitated with the situation and decided then to change the approach to how I save my notes and share my thoughts.\n\n## Jekyll\nAfter a lot of hours hacking at CSS and JS, neither of which are my strongest points; combined with a whole lot of *liquid* scripting, which is based on the Python Jinja library; I chose to leverage the open source Jekyll project.\n\nThis is not to say, that I might not reconsider this again as I am pretty intrigued also with Hugo; but one point is for sure... My days struggling with *Wordpress* are history.\n\nDon't get me wrong, Wordpress is great, even fantastic, but when it breaks, or its hacked (and boy have I been hacked), or when the comments system becomes a spam target; then its a total nightmare to have to deal with.\n\nI want something that is easy to use, a lot less prone to hacking, and painless to host; so my choice was clear from the start - I was going to use a Static Site Generator\n\n## Building\nLeveraging GIT for my version control, I have a simple pipeline which rebuilds a new version of the site each time a new commit is made to the repository. I do like to tweak and have actually no less than two approaches to the effort\n\n\u003cdiv class=\"mermaid\" style=\"text-align:center\"\u003e\n\ngraph LR\n\nA(Blog Repository)\n\nB(Build Pipeline)\n\nC(Docker Based Build)\n\nD(Native Build)\n\nE(Publish Built Site)\n\n  \n\nA -.-\u003e|Git Push Trigger| B\n\nB --\u003e C\n\nB --\u003e D\n\nC --\u003e E\n\nD --\u003e E\n\n\u003c/div\u003e\n\n  \nAs I spend the majority of my time focused on Microsoft Technology stack, I am leveraging Azure DevOps to run my build process; however, if you prefer other tools, for example, Jenkins, CircleCI, etc then the concepts should be easily transportable, as there is nothing truly complex happening at this point.\n\n### Docker Build Pipeline\n\nThis version of the pipeline is my favourite, as I can use the same commands on my workstation to run a local web server to watch in realtime what my edits are going to look like when I finally commit, with 100% confidence that there will be no drift, as I use the exact same container for both roles, development and deployment\n\nThe pipeline I am sharing is in YAML format, which we are going to see a whole lot most of over time, and by sharing this you can easily recreate your own build pipeline with nothing more than a good paste!\n\nThe build is running on a hosted Ubuntu 1604 instance, but this could be easily replaced with a dedicated build node; however for the amount of time I will use for the building, I should fall well inside the free monthly allocation offered in Azure DevOps; so, for now, this is perfect.\n\nThe pipeline has only 3 steps\n\n\u003cdiv class=\"mermaid\" style=\"text-align:center\"\u003e\n\ngraph TD\n\nA[Retrieve the relevant commit from Git Repo]\n\nB(Run Docker Image to Build Site)\n\nC(Move Generated HTML Site to Staging Area)\n\nD(Publish Built Site)\n\n  \n\nA -.-\u003e B\n\nB --\u003e C\n\nC --\u003e D\n\n\u003c/div\u003e\n\n  \nThe *YAML* representation of the flow is as follows; you can also choose to add the steps in the UX and provide the data below into the relevant fields, as there is a 1:1 relationship between the UX and the YAML Infrastructure as Code\n\n```yaml\n\nresources:\n\n- repo: self\n\nqueue:\n\nname: Hosted Ubuntu 1604\n\nsteps:\n\n- task: Docker@1\n\ndisplayName: 'Run an image'\n\ninputs:\n\ncontainerregistrytype: 'Container Registry'\n\n  \n\ncommand: 'Run an image'\n\n  \n\nimageName: 'jekyll/builder:latest'\n\n  \n\nqualifyImageName: false\n\n  \n\nvolumes: |\n\n$(Build.SourcesDirectory):/srv/jekyll\n\n$(Build.BinariesDirectory):/srv/jekyll/_site\n\n  \n\nworkingDirectory: '$(Build.SourcesDirectory):/srv/jekyll'\n\n  \n\ncontainerCommand: 'jekyll build --future'\n\n  \n\nrunInBackground: false\n\n  \n\n- task: CopyFiles@2\n\ndisplayName: 'Copy Files to: $(Build.ArtifactStagingDirectory)'\n\ninputs:\n\nSourceFolder: '$(Build.BinariesDirectory)'\n\n  \n\nTargetFolder: '$(Build.ArtifactStagingDirectory)'\n\n  \n  \n\n- task: PublishBuildArtifacts@1\n\ndisplayName: 'Publish Artifact: _site'\n\ninputs:\n\nArtifactName: '_site'\n\n```\n\n  \n\n### Native Build Pipeline\n\nThe Native approach does not offer a whole lot of immediate advantages over the docker version of the pipeline; I honestly created this to prove to myself that I could.\n\nHowever, after creating this, I do see an advantage. If I should choose to create a dedicated Build Server; I would be able to have the *Ruby bundler* and all the *Jekyll gems* pre-staged on the node; which would remove almost 3 minutes from the build pipeline, as these steps would not need to be repeated every time I executed a new build.\n\nNow, I would have expected the Docker approach to have this as an advantage with the Gems pre-installed in the container, but that's not the case with the official container I have used in the other pipeline; As a result, both pipelines take 3.5 minutes to prepare, build and publish my site artefacts currently. Clearly, I have a lot of room to make this better.\n\nThis pipeline is a little more verbose with 5 steps currently\n\nThe pipeline has only 3 steps\n\n\u003cdiv class=\"mermaid\" style=\"text-align:center\"\u003e\n\ngraph TD\n\nA[Retrieve the relevant commit from Git Repo]\n\nB(Use a Current Release of Ruby)\n\nC(Install the Ruby Bundler toolchain)\n\nD(Use Bundler to install the Jekyll dependencies)\n\nE(Build the Jekyll site to the Staging Area)\n\nF[Publish Built Site]\n\n  \n\nA -.-\u003e B\n\nB --\u003e C\n\nC --\u003e D\n\nD --\u003e E\n\nE --\u003e F\n\n\u003c/div\u003e\n\n  \n\nThe *YAML* representation of the flow is very similar to the previous sample, this time however you are going to really just been looking at some shell commands, which run essentially on any platform we can host ruby on.\n\n```yaml\n\nresources:\n\n- repo: self\n\nqueue:\n\nname: Hosted Ubuntu 1604\n\nsteps:\n\n  \n\n- task: UseRubyVersion@0\n\ndisplayName: 'Use Ruby \u003e= 2.4'\n\n  \n  \n\n- script: 'gem install bundler'\n\ndisplayName: 'Install bundler'\n\n  \n\n- script: 'bundle install'\n\ndisplayName: 'Install Jekyll and Dependencies'\n\n  \n\n- script: 'bundle exec jekyll build -d $(Build.ArtifactStagingDirectory)'\n\ndisplayName: 'Build Jekyll Static Site'\n\n  \n\n- task: PublishBuildArtifacts@1\n\ndisplayName: 'Publish Artifact: _site'\n\ninputs:\n\nArtifactName: '_site'\n\n```\n\n## Next Steps\n\nWith a built site, published; what we really have is a *.ZIP* file which contains all the generated HTML which we can drop onto our web server to publish to the world.\n\nThere are many choices on the web hosting platform to use, keep tuned, and I will share with you the solution I have elected to use for this site",
    "lastmodified": "2023-06-28T11:24:29.118792055Z",
    "tags": [
      "Azure"
    ]
  },
  "/posts/sw_active-directory_administrative-limits": {
    "title": "AD - Administrative Limits",
    "content": "\nI recently ran into an issue with a particular environment where [Active Directory](Active Directory) and [PKI](PKI) services were deployed. One of the service accounts which I was attempting to 'unlock' refused to co-operate and instead offered the most unhelpful message.\n \n`Administrative limit for this request was exceeded` - this was not my first time encountering the message, previously this haunted me while I was managing a Windows PKI infrastructure and with some quick searches, confirmed my initial suspicion. This is a symptom of an AD object quite simply being to large!\n\n## Explaination\n\nActive Directory is in the simplest form a Database, and each object can be considered as a row in this database. Like any database there are limits to what data can be stored in the table, the type of data, but also the amount. \n## Resolving the problem\n\nUsing a tool like **ADSI Edit** will allow you to open the database, and select the schema partition we would like to work with. Standard objects which we can see in the *Active Directory Users and Computers* extension are all hosted in the partition called the *Default Naming Context*; which is where we need to focus for this problem\n\n![ADSI Edit - Default Naming Context](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-10-55-18.png)\n\n### Identifying the Bloat\n\nThere is no super cool trick here, we just need a little investigation. Start by navigating the OU structure you are familiar with in Active Directory Users and Computers, to locate the account with issues.\n\n![AD Object Navigation in ADSI Edit](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-11-00-37.png)\n\nRight Click on the object, and select **Properties** where you now get to view the 100's of columns of data which AD stores for each object. \n\nClick on the **Filter** button and enable the option *Show only attributes that have values* - this will remove all the columns which are empty - and therefore not adding to our overside object!\n\n![ADSI Properties Filter](sw_active-directory_administrative-limits/sw_active-directory_administrative-limits_2021-07-27-11-03-48.png)\n\nNow, you can check each of the remaining attributes (columns) and see what they contain, I'll save a few minutes, and in this case I expect my issue is to do with Certificates, so we will focus on the attribute called *userCertificate* which contains an collection of *[byte[]]*, each representing a Base64 encoded Certificate.\n\nBINGO! looking at this property I can see 100's of entries, and each one has the same *byte[]* string, which implies the same certificate is loaded into the object a lot of times.\n\nI am going to flush the lot of them out, as these are added by the system, and it will reload them as required; but for now, I am simply the garbage collector.\n\nUsing the UX, we can delete these 1 at a time, so we need a little automation, or the rest of the week will be wasted.\n\n## Powershell Helper\n\nUsing standard Active Directory module for Powershell, we can get a reference to the object which needs to be flushed\n\n```powershell\n$certUser = Get-Aduser \"svc_PKI_NDES\" -Properties certificates\n```\n\nNow, let's check what we got from AD\n\n```powershell\n$certuser.Certificates.Count\n1169\n```\n\nRight, time to fix this, we will use the following command to remove all the certificates from the object\n\n```powershell\nPS C:\\Users\\sysadmin.AD\u003e set-ADUser svc_PKI_NDES -Certificates @{}\n```\n\nAnd, let's confirm it worked\n```powershell\n$certuser = Get-ADUser svc_PKI_NDES -Properties Certificates\n$certuser.Certificates.count\n0\n```\n\nAnd now, just check that you can unlock the account, or complete the operation that sent you chasing this wild goose!",
    "lastmodified": "2023-06-28T11:24:29.114792071Z",
    "tags": [
      "AD"
    ]
  }
}